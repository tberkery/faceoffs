{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.decomposition import PCA\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from joblib import dump, load\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data read\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('C:/Users/Tad/Documents/faceoffs/data_imputed_with_event_zone_2016_to_2018.csv')\n",
    "print(\"data read\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pca fitting\n",
      "[[-3.87663838e+03  7.64215754e+02  2.83555365e+03 ...  5.30867811e+00\n",
      "  -8.54660376e+00 -1.25299812e+01]\n",
      " [-3.98792117e+03  8.42796593e+02  2.70425309e+03 ... -1.06508736e+01\n",
      "  -1.79328258e+00 -1.65613936e+01]\n",
      " [-4.99695840e+03  6.96332881e+02  2.24469185e+03 ... -9.97324552e+00\n",
      "  -8.93760115e-01 -1.67742471e+01]\n",
      " ...\n",
      " [-4.53634080e+02 -3.25884430e+02 -8.32333959e+02 ... -2.54542553e+01\n",
      "   1.22326075e+01 -5.19838461e+00]\n",
      " [-1.28605756e+03 -1.97785370e+03  1.73194605e+03 ...  1.30259586e+01\n",
      "   2.55679535e+01 -3.15952177e+01]\n",
      " [ 4.09892570e+01  2.81688319e+02 -8.37661043e+02 ...  1.87607242e+01\n",
      "  -2.22754125e-01 -3.36240524e+00]]\n"
     ]
    }
   ],
   "source": [
    "# Principal Component Analysis\n",
    "data_all = data.dropna()\n",
    "#data_all = data_all[data_all['event_zone'] == 'Off']\n",
    "data = data_all.select_dtypes(['number'])\n",
    "x = data.loc[:, data.columns != 'faceoff_losing_team_xG_since_faceoff']\n",
    "x = x.loc[:, x.columns != 'faceoff_winning_team_xG_since_faceoff']\n",
    "x = x.loc[:, x.columns != 'event_zone'] # We don't want event_zone in the principal components\n",
    "y = data['faceoff_winning_team_xG_since_faceoff']\n",
    "pca = PCA(n_components = 100)\n",
    "print(\"pca fitting\")\n",
    "principal_components = pca.fit_transform(x)\n",
    "print(principal_components)\n",
    "principal_components_df = pd.DataFrame(principal_components)\n",
    "principal_components_df.to_csv(\"principal_components_newest.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed train-test split\n"
     ]
    }
   ],
   "source": [
    "# Prep Train and Test Data\n",
    "objectives = data['faceoff_winning_team_xG_since_faceoff']\n",
    "principal_components_df['faceoff_winning_team_xG_since_faceoff'] = objectives\n",
    "data_no_na = principal_components_df.dropna() # FLAG\n",
    "X = data_no_na.loc[:, data_no_na.columns != 'faceoff_losing_team_xG_since_faceoff']\n",
    "X = X.loc[:, X.columns != 'faceoff_winning_team_xG_since_faceoff']\n",
    "X = OneHotEncoder(categories = 'auto').fit_transform(X)\n",
    "y = data_no_na['faceoff_winning_team_xG_since_faceoff']\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "print(\"completed train-test split\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-07 03:32:28.865340\n"
     ]
    }
   ],
   "source": [
    "# Build initial, untuned random forest model\n",
    "print(datetime.now())\n",
    "# rf_initial = RandomForestRegressor().fit(x_train, y_train)\n",
    "# print(\"completed RandomForestRegressor initial fitting\")\n",
    "# prediction_initial = rf_initial.predict(x_test)\n",
    "# mse_initial = mean_squared_error(y_test, prediction_initial)\n",
    "# rmse_initial = mse_initial ** .5\n",
    "# print(mse_initial)\n",
    "# print(rmse_initial)\n",
    "# print(datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Initial Big Picture Tuning using RandomizedSearchCV\n",
    "print(datetime.now())\n",
    "\n",
    "# random_grid = { # FLAG\n",
    "#     'bootstrap': [True],\n",
    "#     'max_depth': [10, 25, 50, 100, 500, 1000],\n",
    "#     'max_features': ['sqrt', 'log2'],\n",
    "#     'min_samples_leaf': [5, 10],\n",
    "#     'min_samples_split': [5, 10, 25, 50],\n",
    "#     'n_estimators': [100, 250, 500, 1000],\n",
    "# }\n",
    "random_grid = { # FLAG\n",
    "    'bootstrap': [True, False],\n",
    "    'max_depth': [10, 25, 50, 100, 500, 1000],\n",
    "    'max_features': ['sqrt', 'log2'],\n",
    "    'min_samples_leaf': [5, 10, 25, 50],\n",
    "    'min_samples_split': [5, 10, 25, 50, 100],\n",
    "    'n_estimators': [100, 200, 400, 600, 800, 1000, 2000, 5000],\n",
    "}\n",
    "\n",
    "# SOLELY FOR TESTING WHETHER CODE COMPILES (FLAG)\n",
    "# random_grid = {\n",
    "#     'bootstrap': [True],\n",
    "#     'max_depth': [5],\n",
    "#     'max_features': ['auto'],\n",
    "#     'min_samples_leaf': [2],\n",
    "#     'min_samples_split': [2],\n",
    "#     'n_estimators': [15],\n",
    "# }\n",
    "\n",
    "rf_tuning = RandomForestRegressor()\n",
    "rf_random = RandomizedSearchCV(estimator = rf_tuning, param_distributions = random_grid, n_iter = 10, cv = 3, verbose = 2, random_state = 42, n_jobs = -1) # FLAG (n_iter)\n",
    "rf_fit_output = rf_random.fit(x_train, y_train)\n",
    "print(\"writing joblib\")\n",
    "dump(rf_fit_output, 'rf_randomized_search_cv_fit_off_win.joblib') #FLAG... change suffix\n",
    "print(datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# def evaluate(model, test_features, test_labels):\n",
    "#     predictions = model.predict(test_features)\n",
    "#     errors = abs(predictions - test_labels)\n",
    "#     mape = 100 * np.mean(errors / test_labels)\n",
    "#     accuracy = 100 - mape\n",
    "#     print('Model Performance')\n",
    "#     print('Average Error: {:0.4f} degrees.'.format(np.mean(errors)))\n",
    "#     print('Accuracy = {:0.2f}%.'.format(accuracy))\n",
    "#\n",
    "#     return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "best_random = rf_random.best_estimator_\n",
    "# random_accuracy = evaluate(best_random, x_test, y_test)\n",
    "\n",
    "predictions = rf_fit_output.predict(x_test)\n",
    "errors = abs(predictions - y_test)\n",
    "# from sklearn.metrics import r2_score\n",
    "# r_sq = r2_score(y_test, predictions)\n",
    "# mse_grid = mean_squared_error(y_test, predictions)\n",
    "# print(mse_grid)\n",
    "# rmse_grid = mse_grid ** .5\n",
    "# print(rmse_grid)\n",
    "#\n",
    "# # Display the performance metrics\n",
    "# print('Mean Absolute Error:', round(np.mean(errors), 2), 'degrees.')\n",
    "# mape = np.mean(100 * (errors / y_test))\n",
    "# accuracy = 100 - np.mean(mape[np.isfinite(mape)])\n",
    "# print('Accuracy:', round(accuracy, 2), '%.')\n",
    "# See https://stackoverflow.com/questions/58067438/mape-mean-absolute-percentage-error-measurement-in-python-result-in-error#:~:text=mape%20%3D%20100%20*%20(errors%20%2F,')\n",
    "\n",
    "best_bootstrap = best_random.bootstrap\n",
    "best_max_depth = best_random.max_depth\n",
    "best_max_features = best_random.max_features\n",
    "best_min_samples_leaf = best_random.min_samples_leaf\n",
    "best_min_samples_split = best_random.min_samples_split\n",
    "best_n_estimators = best_random.n_estimators\n",
    "\n",
    "best_parameters = rf_fit_output.best_params_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "rf_best_param = RandomForestRegressor(bootstrap = best_bootstrap,\n",
    "                                      max_depth = best_max_depth,\n",
    "                                      max_features = best_max_features,\n",
    "                                      min_samples_leaf = best_min_samples_leaf,\n",
    "                                      min_samples_split = best_min_samples_split,\n",
    "                                      n_estimators = best_n_estimators)\n",
    "rf_best_param_fit = rf_best_param.fit(x_train, y_train)\n",
    "\n",
    "dump(rf_best_param_fit, 'rf_best_parameters_fit_off_win.joblib') # FLAG... change suffix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# var_imps = rf_best_param_fit.feature_importances_\n",
    "#\n",
    "# def plot_feature_importance(importance,names,model_type):\n",
    "#     #Create arrays from feature importance and feature names\n",
    "#     feature_importance = np.array(importance)\n",
    "#     feature_names = np.array(names)\n",
    "#\n",
    "#     #Create a DataFrame using a Dictionary\n",
    "#     df ={'feature_names':feature_names,'feature_importance':feature_importance}\n",
    "#     fi_df = pd.DataFrame(df)\n",
    "#\n",
    "#     #Sort the DataFrame in order decreasing feature importance\n",
    "#     fi_df.sort_values(by=['feature_importance'], ascending=False,inplace=True)\n",
    "#\n",
    "#     #Define size of bar plot\n",
    "#     plt.figure(figsize=(10,8))\n",
    "#     #Plot Searborn bar chart\n",
    "#     sns.barplot(x=fi_df['feature_importance'], y=fi_df['feature_names'])\n",
    "#     #Add chart labels\n",
    "#     plt.title(model_type + 'FEATURE IMPORTANCE')\n",
    "#     plt.xlabel('FEATURE IMPORTANCE')\n",
    "#     plt.ylabel('FEATURE NAMES')\n",
    "# plot_feature_importance(rf_best_param_fit.feature_importances_,x_train.columns,'RANDOM FOREST')\n",
    "#\n",
    "# feat_importances_series = pd.Series(var_imps, index=x_train.columns)\n",
    "# feat_importances_series.nlargest(20).plot(kind='barh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
