{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as functional\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from skimage import io\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3569, 694)\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"/Users/Tad/Documents/faceoffs\"\n",
    "data_initial = pd.read_csv(\"final_dataset.csv\")\n",
    "print(data_initial.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3569, 691)\n",
      "[0, 48, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 623, 624, 625, 626, 627, 628, 629, 630, 631, 632, 633, 634, 635, 636, 637, 638, 639, 640, 641, 642, 643, 644, 645, 646, 647, 648, 649, 650, 651, 652, 653, 654, 655, 656, 657, 658, 659, 660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 670, 671, 672, 673, 674, 675, 676, 677, 678, 679, 680, 681, 682, 683, 684, 685, 686, 687, 688, 689, 690]\n",
      "310\n"
     ]
    }
   ],
   "source": [
    "data_no_na = data_initial.dropna(axis = 1) # data should already have no NAs due to numerical imputation\n",
    "print(data_no_na.shape)\n",
    "i = 0\n",
    "#for col in data_no_na.columns:\n",
    "#    print(\"\" + str(i) + \": \" + col)\n",
    "#    i = i + 1\n",
    "\n",
    "include_col_indices = [0, 48]  # Start with individual indices\n",
    "\n",
    "# Add ranges using loops\n",
    "include_col_indices.extend(range(63, 139))\n",
    "include_col_indices.extend(range(159, 279))\n",
    "include_col_indices.extend(range(299, 343))\n",
    "include_col_indices.extend(range(623, 691))\n",
    "\n",
    "print(include_col_indices)\n",
    "print(len(include_col_indices))\n",
    "\n",
    "# Subset columns from the NumPy array\n",
    "subset_data = data_no_na.iloc[:, include_col_indices]\n",
    "\n",
    "# Convert the subsetted array to a DataFrame\n",
    "df_subset = pd.DataFrame(subset_data)\n",
    "\n",
    "# Select columns of number data types\n",
    "data = df_subset.select_dtypes(include=['number'])\n",
    "\n",
    "y = data['FA_zone_time']\n",
    "x = data.loc[:, data.columns != 'FA_zone_time']\n",
    "x = x.loc[:, x.columns != 'FA_zone_time']\n",
    "X_train, X_intermediate, y_train, y_intermediate = train_test_split(x, y, train_size = 0.6, test_size=0.4, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_intermediate, y_intermediate, train_size = 0.5, test_size=0.5, random_state=45)\n",
    "\n",
    "# Above two lines in unison accomplish a 60-20-20 split for train-validation-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device_selection = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device = torch.device(device_selection)\n",
    "print(device_selection)\n",
    "print(device)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "train_batch_size = 1000\n",
    "val_batch_size = 1000\n",
    "test_batch_size = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = torch.tensor(self.df.iloc[idx, :-1].values, dtype=torch.float32)\n",
    "        y = torch.tensor(self.df.iloc[idx, -1:].values, dtype=torch.float32)\n",
    "        return x, y\n",
    "\n",
    "train_df =  pd.concat([X_train, y_train], axis=1)  # pandas DataFrame containing training data\n",
    "valid_df = pd.concat([X_val, y_val], axis=1) # pandas DataFrame containing validation data\n",
    "test_df = pd.concat([X_test, y_test], axis=1)  # pandas DataFrame containing test data\n",
    "train_dataset = CustomDataset(train_df)\n",
    "valid_dataset = CustomDataset(valid_df)\n",
    "test_dataset = CustomDataset(test_df)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=train_batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=val_batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=test_batch_size, shuffle=False)\n",
    "\n",
    "# Credit to ChatGPT for generating this cell of code in response to the following prompt: design a dataloader that accepts the output of a pandas train-validation-test split. Basically write code for a PyTorch dataloader that accepts pandas dataframes as arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VanillaNeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, dropout_rate):\n",
    "        super(VanillaNeuralNet, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.dropout_rate = dropout_rate\n",
    "        \n",
    "        self.fc1 = nn.Linear(self.input_size, 1000)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=self.dropout_rate)\n",
    "        self.batch_norm_1000 = nn.BatchNorm1d(1000)\n",
    "        self.batch_norm_500 = nn.BatchNorm1d(500)\n",
    "        self.batch_norm_400 = nn.BatchNorm1d(400)\n",
    "        self.batch_norm_300 = nn.BatchNorm1d(300)\n",
    "        self.batch_norm_200 = nn.BatchNorm1d(200)\n",
    "        self.batch_norm_100 = nn.BatchNorm1d(100)\n",
    "\n",
    "        self.fc2 = nn.Linear(1000, 1000)\n",
    "\n",
    "        self.fc3 = nn.Linear(1000, 1000)\n",
    "\n",
    "        self.fc4 = nn.Linear(1000, 500)\n",
    "\n",
    "        self.fc5 = nn.Linear(500, 500)\n",
    "\n",
    "        self.fc6 = nn.Linear(500, 400)\n",
    "\n",
    "        self.fc7 = nn.Linear(400, 300)\n",
    "\n",
    "        self.fc8 = nn.Linear(300, 200)\n",
    "\n",
    "        self.fc9 = nn.Linear(200, 100)\n",
    "\n",
    "        self.fc10 = nn.Linear(100, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #print(x.shape)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.batch_norm_1000(x)\n",
    "\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.batch_norm_1000(x)\n",
    "\n",
    "        x = self.fc3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.batch_norm_1000(x)\n",
    "\n",
    "        x = self.fc4(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.batch_norm_500(x)\n",
    "\n",
    "        x = self.fc5(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.batch_norm_500(x)\n",
    "\n",
    "        x = self.fc6(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.batch_norm_400(x)\n",
    "\n",
    "        x = self.fc7(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.batch_norm_300(x)\n",
    "\n",
    "        x = self.fc8(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.batch_norm_200(x)\n",
    "\n",
    "        x = self.fc9(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.batch_norm_100(x)\n",
    "\n",
    "        x = self.fc10(x)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 50\n",
    "trial_count = 30\n",
    "\n",
    "learning_rate = 0.01 # in last optimization, best lr ended up being 0.0016\n",
    "loss_func = nn.MSELoss()\n",
    "\n",
    "def train(model, train_loader, valid_loader, loss_func, optimizer, scheduler, num_epochs, device):\n",
    "    training_losses = np.array([])\n",
    "    validation_losses = np.array([])\n",
    "    for epoch in range(num_epochs):\n",
    "        print(\"\\nEPOCH \" +str(epoch+1)+\" of \"+str(num_epochs)+\"\\n\")\n",
    "        ########################### Training #####################################\n",
    "        model.train(True)\n",
    "        train_loss = 0\n",
    "        count = 0\n",
    "        for batch, (X, y) in enumerate(train_loader):\n",
    "            X = X.float().to(device)\n",
    "            y = y.float().to(device)\n",
    "            optimizer.zero_grad()\n",
    "            prediction = model(X).to(device)\n",
    "            loss = loss_func(prediction, y.view(-1, 1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            train_loss += loss.float().item()\n",
    "            count += train_batch_size\n",
    "        training_losses = np.append(training_losses, train_loss / count)\n",
    "        print(\"Training loss:\", train_loss / count)\n",
    "        #model.train(False)\n",
    "\n",
    "        ########################### Validation #####################################\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        count = 0\n",
    "        with torch.no_grad():\n",
    "            for (X, y) in valid_loader:\n",
    "                X = X.to(device)\n",
    "                y = y.to(device)\n",
    "                prediction = model(X).to(device)\n",
    "                loss = loss_func(prediction, y.view(-1, 1))\n",
    "                val_loss += loss.float().item()\n",
    "                count += val_batch_size\n",
    "        validation_losses = np.append(validation_losses, val_loss / count)\n",
    "        print(\"Validation loss:\", val_loss / count)\n",
    "    return validation_losses[-1]\n",
    "\n",
    "def objective(trial):\n",
    "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-1, log=True)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-6, 1e-3, log=True)\n",
    "    hidden_size = trial.suggest_int(\"hidden_size\", 300, 600)\n",
    "    dropout_rate = trial.suggest_float(\"dropout_rate\", 0.2, 0.5)\n",
    "    print(\"learning rate:\", lr)\n",
    "    print(\"weight_decay:\", weight_decay)\n",
    "    print(\"hidden_size:\", hidden_size)\n",
    "    print(\"dropout_rate:\", dropout_rate)\n",
    "\n",
    "    model = VanillaNeuralNet(input_dim, hidden_size, dropout_rate).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "    return train(model, train_loader, valid_loader, loss_func, optimizer, scheduler, num_epochs, device)\n",
    "\n",
    "input_dim = 309\n",
    "output_dim = 1\n",
    "hidden_dim = 300\n",
    "dropout = 0.1\n",
    "model = VanillaNeuralNet(input_dim, hidden_dim, dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Training loss: 731.6875813802084\n",
      "Epoch 2 Training loss: 630.9138590494791\n",
      "Epoch 3 Training loss: 743.2991536458334\n",
      "Epoch 4 Training loss: 680.441162109375\n",
      "Epoch 5 Training loss: 620.7219441731771\n",
      "Epoch 6 Training loss: 658.5355021158854\n",
      "Epoch 7 Training loss: 548.1417439778646\n",
      "Epoch 8 Training loss: 535.0118001302084\n",
      "Epoch 9 Training loss: 550.2408650716146\n",
      "Epoch 10 Training loss: 502.1156514485677\n",
      "Epoch 11 Training loss: 528.8517557779948\n",
      "Epoch 12 Training loss: 501.62054443359375\n",
      "Epoch 13 Training loss: 505.22580973307294\n",
      "Epoch 14 Training loss: 498.5453796386719\n",
      "Epoch 15 Training loss: 481.10064697265625\n",
      "Epoch 16 Training loss: 441.4634704589844\n",
      "Epoch 17 Training loss: 469.9709981282552\n",
      "Epoch 18 Training loss: 468.3215637207031\n",
      "Epoch 19 Training loss: 510.55030314127606\n",
      "Epoch 20 Training loss: 434.3927001953125\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 500\n",
    "dropout_rate = 0.25\n",
    "weight_decay = 0.000001\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Above inspired by last run best params, which were:\n",
    "# {'lr': 0.0016359428701992723, 'weight_decay': 3.4162832861696797e-06, 'hidden_size': 493, 'dropout_rate': 0.4524812840729844}\n",
    "model = VanillaNeuralNet(input_dim, hidden_size, dropout_rate).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "loss_func = nn.MSELoss()\n",
    "\n",
    "\n",
    "# # Train the model with the best hyperparameters on the entire dataset\n",
    "# for epoch in range(num_epochs):\n",
    "#     ########################### Training #####################################\n",
    "#     model.train(True)\n",
    "#     train_loss = 0\n",
    "#     count = 0\n",
    "#     for batch, (X, y) in enumerate(train_loader):\n",
    "#         X = X.float().to(device)\n",
    "#         y = y.float().to(device)\n",
    "#         prediction = model(X).to(device)\n",
    "#         optimizer.zero_grad()\n",
    "#         loss = loss_func(prediction, y.view(-1, 1).float().to(device))\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         train_loss += loss.float().item()\n",
    "#         count += 1\n",
    "#     scheduler.step()\n",
    "#     print(\"Epoch\", epoch+1, \"Training loss:\", train_loss / count)\n",
    "\n",
    "# # Save the best model\n",
    "# torch.save(model.state_dict(), \"faceoffs_off_off_state_dict2.pt\")\n",
    "# # save the trained model\n",
    "# torch.save(model, \"faceoffs_off_off2.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tad\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "\u001b[32m[I 2023-08-29 20:41:02,483]\u001b[0m A new study created in memory with name: no-name-3875ebc9-65eb-4b04-a830-52e88ed037b1\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning rate: 0.0016201586298246254\n",
      "weight_decay: 4.3394834205798547e-05\n",
      "hidden_size: 458\n",
      "dropout_rate: 0.2331684137024599\n",
      "\n",
      "EPOCH 1 of 20\n",
      "\n",
      "Training loss: 0.6598216756184896\n",
      "Validation loss: 0.6832701416015625\n",
      "\n",
      "EPOCH 2 of 20\n",
      "\n",
      "Training loss: 0.6471678059895833\n",
      "Validation loss: 4.25683740234375\n",
      "\n",
      "EPOCH 3 of 20\n",
      "\n",
      "Training loss: 0.6635911661783854\n",
      "Validation loss: 32.357580078125\n",
      "\n",
      "EPOCH 4 of 20\n",
      "\n",
      "Training loss: 0.6965250651041667\n",
      "Validation loss: 1.6983831787109376\n",
      "\n",
      "EPOCH 5 of 20\n",
      "\n",
      "Training loss: 0.684968505859375\n",
      "Validation loss: 0.546871826171875\n",
      "\n",
      "EPOCH 6 of 20\n",
      "\n",
      "Training loss: 0.6980617879231771\n",
      "Validation loss: 1.6001920166015624\n",
      "\n",
      "EPOCH 7 of 20\n",
      "\n",
      "Training loss: 0.6926809895833334\n",
      "Validation loss: 0.5414559936523438\n",
      "\n",
      "EPOCH 8 of 20\n",
      "\n",
      "Training loss: 0.6412587280273437\n",
      "Validation loss: 0.5402754516601562\n",
      "\n",
      "EPOCH 9 of 20\n",
      "\n",
      "Training loss: 0.6226188863118489\n",
      "Validation loss: 0.5386800537109375\n",
      "\n",
      "EPOCH 10 of 20\n",
      "\n",
      "Training loss: 0.7013902180989583\n",
      "Validation loss: 0.5781532592773437\n",
      "\n",
      "EPOCH 11 of 20\n",
      "\n",
      "Training loss: 0.7711354777018229\n",
      "Validation loss: 0.543358154296875\n",
      "\n",
      "EPOCH 12 of 20\n",
      "\n",
      "Training loss: 0.6730181070963541\n",
      "Validation loss: 0.5443802490234375\n",
      "\n",
      "EPOCH 13 of 20\n",
      "\n",
      "Training loss: 0.6528330688476562\n",
      "Validation loss: 0.5471050415039063\n",
      "\n",
      "EPOCH 14 of 20\n",
      "\n",
      "Training loss: 0.6719668579101562\n",
      "Validation loss: 0.5460153198242188\n",
      "\n",
      "EPOCH 15 of 20\n",
      "\n",
      "Training loss: 0.6568997599283855\n",
      "Validation loss: 0.546731689453125\n",
      "\n",
      "EPOCH 16 of 20\n",
      "\n",
      "Training loss: 0.6391324462890625\n",
      "Validation loss: 0.5458475952148437\n",
      "\n",
      "EPOCH 17 of 20\n",
      "\n",
      "Training loss: 0.6408325805664062\n",
      "Validation loss: 0.5470657958984375\n",
      "\n",
      "EPOCH 18 of 20\n",
      "\n",
      "Training loss: 0.6257618204752604\n",
      "Validation loss: 0.5466260986328125\n",
      "\n",
      "EPOCH 19 of 20\n",
      "\n",
      "Training loss: 0.5958354390462239\n",
      "Validation loss: 0.5492273559570312\n",
      "\n",
      "EPOCH 20 of 20\n",
      "\n",
      "Training loss: 0.6545688883463542\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-08-29 20:42:47,108]\u001b[0m Trial 0 finished with value: 0.5496725463867187 and parameters: {'lr': 0.0016201586298246254, 'weight_decay': 4.3394834205798547e-05, 'hidden_size': 458, 'dropout_rate': 0.2331684137024599}. Best is trial 0 with value: 0.5496725463867187.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.5496725463867187\n",
      "{'lr': 0.0016201586298246254, 'weight_decay': 4.3394834205798547e-05, 'hidden_size': 458, 'dropout_rate': 0.2331684137024599}\n"
     ]
    }
   ],
   "source": [
    "# Optuna Hyperparameter Tuning\n",
    "import optuna\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=trial_count)\n",
    "print(study.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lr': 0.0016201586298246254, 'weight_decay': 4.3394834205798547e-05, 'hidden_size': 458, 'dropout_rate': 0.2331684137024599}\n",
      "\n",
      "EPOCH 1 of 5\n",
      "\n",
      "Training loss: 0.6297891845703125\n",
      "Validation loss: 0.7173461303710937\n",
      "\n",
      "EPOCH 2 of 5\n",
      "\n",
      "Training loss: 0.6995457763671875\n",
      "Validation loss: 6.4935859375\n",
      "\n",
      "EPOCH 3 of 5\n",
      "\n",
      "Training loss: 0.5985120646158855\n",
      "Validation loss: 172.67725\n",
      "\n",
      "EPOCH 4 of 5\n",
      "\n",
      "Training loss: 0.7675731811523437\n",
      "Validation loss: 0.553473876953125\n",
      "\n",
      "EPOCH 5 of 5\n",
      "\n",
      "Training loss: 0.7130353800455729\n",
      "Validation loss: 1.2157974853515625\n"
     ]
    }
   ],
   "source": [
    "best_params = study.best_trial.params\n",
    "print(best_params)\n",
    "model = VanillaNeuralNet(input_dim, best_params[\"hidden_size\"], best_params[\"dropout_rate\"]).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=best_params[\"lr\"], weight_decay=best_params[\"weight_decay\"])\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "loss_func = nn.MSELoss()\n",
    "\n",
    "num_epochs = 5 # training for more epochs on best model (which will be saved)\n",
    "\n",
    "train(model, train_loader, valid_loader, loss_func, optimizer, scheduler, num_epochs, device)\n",
    "\n",
    "# Train the model with the best hyperparameters on the entire dataset\n",
    "# for epoch in range(num_epochs):\n",
    "#     ########################### Training #####################################\n",
    "#     model.train(True)\n",
    "#     train_loss = 0\n",
    "#     count = 0\n",
    "#     for batch, (X, y) in enumerate(train_loader):\n",
    "#         X = X.float().to(device)\n",
    "#         y = y.float().to(device)\n",
    "#         prediction = model(X).to(device)\n",
    "#         optimizer.zero_grad()\n",
    "#         loss = loss_func(prediction, y.view(-1, 1).float().to(device))\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         train_loss += loss.float().item()\n",
    "#         count += 1\n",
    "#     print(\"Epoch\", epoch+1, \"Training loss:\", train_loss / count)\n",
    "#     scheduler.step()\n",
    "#     model.eval()\n",
    "#     val_loss = 0\n",
    "#     count = 0\n",
    "#     with torch.no_grad():\n",
    "#         for (X, y) in valid_loader:\n",
    "#             X = X.to(device)\n",
    "#             y = y.to(device)\n",
    "#             prediction = model(X).to(device)\n",
    "#             loss = loss_func(prediction, y.view(-1, 1))\n",
    "#             val_loss += loss.float().item()\n",
    "#             count += val_batch_size\n",
    "#         print(\"Validation loss:\", val_loss / count)\n",
    "\n",
    "# Save the best model\n",
    "torch.save(model.state_dict(), \"faceoff_mod_state_dict.pt\")\n",
    "# save the trained model\n",
    "torch.save(model, \"faceoff_mod.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1319.6432\n",
      "True labels: [11.0, 41.0, 28.0, 1.0, 8.0, 10.0, 21.0, 9.0, 11.0, 1.0, 23.0, 15.0, 11.0, 6.0, 7.0, 8.0, 5.0, 14.0, 1.0, 6.0, 1.0, 2.0, 4.0, 7.0, 8.0, 7.0, 1.0, 1.0, 3.0, 6.0, 22.0, 21.0, 48.0, 7.0, 8.0, 13.0, 27.0, 78.0, 11.0, 9.0, 21.0, 10.0, 9.0, 28.0, 94.0, 3.0, 6.0, 14.0, 8.0, 13.0, 20.0, 7.0, 6.0, 1.0, 9.0, 16.0, 12.0, 14.0, 12.0, 1.0, 19.0, 48.0, 4.0, 3.0, 4.0, 13.0, 8.0, 43.0, 16.0, 5.0, 59.0, 6.0, 15.0, 19.0, 24.0, 1.0, 22.0, 13.0, 5.0, 16.0, 1.0, 10.0, 14.0, 6.0, 8.0, 44.0, 4.0, 16.0, 3.0, 1.0, 8.0, 45.0, 4.0, 9.0, 9.0, 1.0, 6.0, 14.0, 48.0, 17.0, 48.0, 18.0, 3.0, 7.0, 3.0, 33.0, 10.0, 5.0, 37.0, 4.0, 5.0, 6.0, 5.0, 15.0, 60.0, 10.0, 18.0, 1.0, 5.0, 2.0, 10.0, 49.0, 9.0, 18.0, 8.0, 27.0, 82.0, 18.0, 10.0, 2.0, 28.0, 3.0, 7.0, 4.0, 8.0, 10.0, 16.0, 16.0, 80.0, 14.0, 12.0, 25.0, 4.0, 145.0, 9.0, 8.0, 30.0, 1.0, 77.0, 4.0, 106.0, 9.0, 3.0, 2.0, 25.0, 11.0, 5.0, 20.0, 9.0, 7.0, 1.0, 69.0, 8.0, 122.0, 1.0, 26.0, 12.0, 4.0, 21.0, 20.0, 6.0, 29.0, 3.0, 44.0, 6.0, 4.0, 31.0, 31.0, 5.0, 5.0, 13.0, 16.0, 5.0, 12.0, 32.0, 12.0, 19.0, 62.0, 24.0, 19.0, 60.0, 16.0, 3.0, 9.0, 12.0, 1.0, 41.0, 32.0, 23.0, 2.0, 6.0, 68.0, 11.0, 3.0, 4.0, 11.0, 2.0, 4.0, 12.0, 59.0, 9.0, 5.0, 1.0, 25.0, 5.0, 2.0, 73.0, 58.0, 5.0, 23.0, 11.0, 6.0, 2.0, 74.0, 12.0, 17.0, 7.0, 40.0, 4.0, 2.0, 3.0, 36.0, 14.0, 10.0, 18.0, 19.0, 11.0, 29.0, 7.0, 41.0, 1.0, 1.0, 11.0, 5.0, 23.0, 12.0, 3.0, 1.0, 9.0, 9.0, 2.0, 7.0, 19.0, 11.0, 63.0, 9.0, 4.0, 39.0, 52.0, 1.0, 23.0, 23.0, 10.0, 5.0, 22.0, 5.0, 29.0, 1.0, 24.0, 36.0, 17.0, 4.0, 8.0, 29.0, 12.0, 6.0, 10.0, 8.0, 28.0, 4.0, 4.0, 26.0, 63.0, 5.0, 11.0, 5.0, 16.0, 6.0, 5.0, 1.0, 7.0, 1.0, 16.0, 69.0, 58.0, 9.0, 28.0, 23.0, 21.0, 71.0, 2.0, 7.0, 1.0, 3.0, 4.0, 47.0, 15.0, 9.0, 2.0, 35.0, 7.0, 6.0, 7.0, 5.0, 8.0, 6.0, 13.0, 15.0, 10.0, 8.0, 8.0, 9.0, 55.0, 4.0, 5.0, 4.0, 2.0, 11.0, 2.0, 5.0, 9.0, 5.0, 6.0, 11.0, 5.0, 5.0, 44.0, 10.0, 33.0, 12.0, 16.0, 29.0, 12.0, 4.0, 19.0, 4.0, 1.0, 8.0, 14.0, 15.0, 15.0, 28.0, 23.0, 9.0, 4.0, 23.0, 2.0, 13.0, 45.0, 10.0, 29.0, 59.0, 5.0, 17.0, 16.0, 20.0, 18.0, 25.0, 13.0, 6.0, 10.0, 50.0, 21.0, 12.0, 2.0, 4.0, 29.0, 25.0, 3.0, 3.0, 19.0, 6.0, 31.0, 1.0, 4.0, 37.0, 12.0, 5.0, 17.0, 1.0, 38.0, 4.0, 14.0, 15.0, 7.0, 14.0, 16.0, 20.0, 9.0, 19.0, 10.0, 11.0, 2.0, 11.0, 26.0, 10.0, 22.0, 4.0, 11.0, 8.0, 25.0, 24.0, 3.0, 19.0, 33.0, 1.0, 17.0, 7.0, 14.0, 7.0, 23.0, 40.0, 11.0, 68.0, 16.0, 1.0, 7.0, 2.0, 2.0, 23.0, 33.0, 18.0, 2.0, 2.0, 17.0, 30.0, 3.0, 12.0, 6.0, 9.0, 32.0, 3.0, 26.0, 15.0, 17.0, 21.0, 9.0, 15.0, 15.0, 4.0, 20.0, 60.0, 11.0, 8.0, 19.0, 33.0, 17.0, 39.0, 2.0, 12.0, 35.0, 19.0, 18.0, 9.0, 41.0, 82.0, 12.0, 5.0, 6.0, 17.0, 3.0, 6.0, 3.0, 20.0, 10.0, 3.0, 6.0, 1.0, 5.0, 16.0, 34.0, 8.0, 6.0, 1.0, 20.0, 13.0, 1.0, 12.0, 11.0, 3.0, 10.0, 14.0, 20.0, 7.0, 4.0, 10.0, 4.0, 5.0, 2.0, 18.0, 3.0, 1.0, 18.0, 3.0, 28.0, 1.0, 7.0, 9.0, 11.0, 1.0, 28.0, 23.0, 51.0, 6.0, 10.0, 19.0, 5.0, 44.0, 28.0, 9.0, 9.0, 6.0, 9.0, 1.0, 14.0, 65.0, 4.0, 34.0, 4.0, 2.0, 16.0, 3.0, 8.0, 25.0, 4.0, 2.0, 23.0, 1.0, 21.0, 11.0, 55.0, 35.0, 66.0, 11.0, 2.0, 10.0, 71.0, 7.0, 62.0, 34.0, 50.0, 4.0, 5.0, 11.0, 6.0, 39.0, 14.0, 5.0, 17.0, 12.0, 2.0, 6.0, 56.0, 26.0, 3.0, 14.0, 3.0, 3.0, 23.0, 14.0, 1.0, 11.0, 11.0, 7.0, 86.0, 16.0, 4.0, 1.0, 1.0, 7.0, 18.0, 13.0, 10.0, 42.0, 10.0, 2.0, 13.0, 16.0, 42.0, 26.0, 126.0, 5.0, 5.0, 19.0, 3.0, 2.0, 11.0, 27.0, 21.0, 1.0, 14.0, 8.0, 5.0, 62.0, 3.0, 59.0, 24.0, 20.0, 2.0, 17.0, 39.0, 14.0, 8.0, 10.0, 20.0, 13.0, 12.0, 16.0, 4.0, 7.0, 10.0, 2.0, 8.0, 20.0, 34.0, 3.0, 16.0, 13.0, 4.0, 13.0, 5.0, 49.0, 24.0, 8.0, 6.0, 19.0, 11.0, 5.0, 16.0, 43.0, 8.0, 60.0, 20.0, 6.0, 72.0, 5.0, 60.0, 2.0, 15.0, 15.0, 10.0, 11.0, 4.0, 28.0, 32.0, 99.0, 5.0, 17.0, 10.0, 14.0, 10.0, 7.0, 1.0, 21.0, 11.0, 6.0, 88.0, 16.0, 10.0, 66.0, 33.0, 8.0, 8.0, 21.0, 6.0, 52.0, 6.0, 97.0, 8.0, 6.0, 4.0, 3.0, 25.0, 5.0, 14.0, 9.0, 4.0, 2.0, 20.0, 7.0, 6.0, 26.0, 19.0, 27.0, 7.0, 3.0, 16.0, 2.0, 5.0, 24.0, 5.0, 24.0, 11.0, 5.0, 9.0, 17.0, 11.0, 5.0, 5.0, 7.0, 7.0, 12.0, 8.0, 16.0]\n",
      "Predictions: [-14.209665, -14.244699, -14.209529, -14.213006, -14.217102, -14.205595, -14.218972, -14.20646, -14.223532, -14.205232, -14.208507, -14.231748, -14.211666, -14.209869, -14.224941, -14.206889, -14.198946, -14.208612, -14.223798, -14.19986, -14.199603, -14.226505, -14.223213, -14.22137, -14.210588, -14.2185, -14.228154, -14.227115, -14.206571, -14.260499, -14.219137, -14.195253, -14.218748, -14.231499, -14.253033, -14.228273, -14.218853, -14.207315, -14.224125, -14.206434, -14.203809, -14.2117, -14.203888, -14.262697, -14.218882, -14.226369, -14.2193365, -14.214678, -14.209713, -14.213946, -14.191511, -14.221307, -14.226013, -14.213308, -14.205185, -14.209582, -14.255314, -14.212284, -14.200212, -14.199045, -14.215408, -14.213328, -14.217209, -14.218502, -14.215122, -14.203023, -14.216961, -14.21545, -14.219933, -14.230798, -14.202757, -14.21225, -14.23271, -14.210815, -14.214467, -14.212749, -14.223863, -14.237718, -14.210608, -14.204868, -14.202492, -14.216797, -14.222885, -14.215555, -14.212757, -14.255458, -14.205692, -14.216439, -14.192967, -14.21867, -14.221312, -14.212144, -14.19592, -14.222571, -14.210697, -14.213223, -14.197418, -14.20365, -14.215676, -14.213507, -14.206335, -14.208371, -14.216801, -14.208077, -14.225244, -14.226027, -14.2198515, -14.205996, -14.215694, -14.215897, -14.229042, -14.222643, -14.204664, -14.179973, -14.217968, -14.221657, -14.226621, -14.20077, -14.199177, -14.213367, -14.209089, -14.199887, -14.219183, -14.214113, -14.22085, -14.203269, -14.214203, -14.20882, -14.201896, -14.222376, -14.21413, -14.2530365, -14.209023, -14.2159395, -14.203611, -14.212538, -14.21118, -14.193165, -14.216746, -14.211389, -14.220652, -14.200581, -14.211595, -14.227512, -14.24737, -14.213251, -14.220419, -14.224068, -14.219008, -14.208257, -14.20797, -14.212618, -14.224546, -14.215379, -14.217193, -14.226458, -14.219007, -14.218551, -14.229677, -14.22279, -14.202551, -14.219173, -14.21406, -14.216871, -14.225218, -14.247555, -14.220202, -14.212618, -14.228553, -14.193252, -14.222359, -14.217576, -14.204585, -14.230962, -14.220961, -14.207397, -14.244955, -14.211102, -14.244387, -14.213751, -14.227733, -14.217846, -14.213134, -14.215432, -14.208471, -14.224715, -14.21959, -14.217907, -14.214946, -14.231511, -14.206587, -14.211023, -14.222721, -14.220106, -14.211993, -14.219737, -14.21744, -14.221429, -14.21256, -14.213127, -14.205307, -14.208412, -14.208794, -14.222738, -14.216285, -14.215752, -14.21483, -14.211459, -14.252137, -14.201688, -14.21278, -14.218848, -14.207079, -14.234775, -14.208466, -14.200401, -14.200087, -14.2196665, -14.210599, -14.215666, -14.215113, -14.230551, -14.205576, -14.215077, -14.256357, -14.192294, -14.212538, -14.207051, -14.207176, -14.2207985, -14.210024, -14.221506, -14.208018, -14.220301, -14.212193, -14.226741, -14.217058, -14.182548, -14.219118, -14.252047, -14.218948, -14.2197075, -14.246296, -14.214897, -14.222446, -14.2109375, -14.225815, -14.217177, -14.194092, -14.20005, -14.2077, -14.25005, -14.210232, -14.215121, -14.231025, -14.222391, -14.223969, -14.222832, -14.232655, -14.212259, -14.2241535, -14.219525, -14.265903, -14.204683, -14.19952, -14.23778, -14.217782, -14.209622, -14.2234335, -14.205819, -14.211274, -14.219471, -14.207002, -14.211517, -14.220062, -14.231365, -14.208697, -14.225358, -14.213856, -14.215908, -14.206966, -14.225164, -14.212924, -14.236036, -14.219667, -14.220217, -14.216684, -14.205971, -14.239249, -14.194753, -14.209751, -14.211658, -14.226861, -14.203731, -14.224572, -14.21865, -14.201887, -14.2001295, -14.228534, -14.213478, -14.215795, -14.225494, -14.189928, -14.258489, -14.2030525, -14.243451, -14.210953, -14.213856, -14.203038, -14.216034, -14.220829, -14.199486, -14.204868, -14.209448, -14.211112, -14.206963, -14.221389, -14.220313, -14.221491, -14.219456, -14.204392, -14.268871, -14.23922, -14.211172, -14.212352, -14.21017, -14.201765, -14.217664, -14.209328, -14.222611, -14.216452, -14.260965, -14.20806, -14.223319, -14.201085, -14.214869, -14.265438, -14.208241, -14.204291, -14.234444, -14.214736, -14.211599, -14.211967, -14.210276, -14.218749, -14.224328, -14.209114, -14.212245, -14.21195, -14.218051, -14.220663, -14.221427, -14.211398, -14.198212, -14.19272, -14.223036, -14.214258, -14.202134, -14.215166, -14.2199955, -14.215518, -14.230176, -14.20965, -14.249142, -14.217366, -14.260236, -14.208569, -14.179781, -14.217794, -14.213921, -14.223623, -14.200356, -14.253711, -14.215057, -14.225793, -14.219328, -14.20569, -14.213432, -14.224062, -14.210831, -14.212082, -14.2194605, -14.193931, -14.229679, -14.217623, -14.2208805, -14.222016, -14.213055, -14.205202, -14.215023, -14.214165, -14.221003, -14.219661, -14.222163, -14.21337, -14.217421, -14.216072, -14.207919, -14.222514, -14.208393, -14.222773, -14.211334, -14.223509, -14.229387, -14.206413, -14.216037, -14.20237, -14.205022, -14.251211, -14.206588, -14.224948, -14.224842, -14.213772, -14.220803, -14.208311, -14.220187, -14.217529, -14.209108, -14.211007, -14.224045, -14.214736, -14.214358, -14.209024, -14.231387, -14.21335, -14.228696, -14.207176, -14.209231, -14.200407, -14.228016, -14.213723, -14.2048, -14.219002, -14.218477, -14.208688, -14.202413, -14.216992, -14.223455, -14.212959, -14.209274, -14.258106, -14.247091, -14.209501, -14.214674, -14.212579, -14.191744, -14.195028, -14.209965, -14.212117, -14.215474, -14.210274, -14.227604, -14.210363, -14.200215, -14.2081585, -14.214294, -14.208461, -14.227949, -14.203001, -14.22374, -14.2138195, -14.215975, -14.201932, -14.219167, -14.203551, -14.195634, -14.207918, -14.253159, -14.211076, -14.210579, -14.2216, -14.2295065, -14.218215, -14.207039, -14.218313, -14.194741, -14.211214, -14.196615, -14.205761, -14.2137785, -14.200905, -14.211274, -14.21336, -14.210945, -14.201532, -14.207735, -14.219954, -14.209414, -14.228659, -14.230236, -14.192163, -14.247579, -14.200344, -14.212664, -14.1957855, -14.221862, -14.209472, -14.218955, -14.210522, -14.219867, -14.220512, -14.220098, -14.223457, -14.206839, -14.207666, -14.206398, -14.217065, -14.217764, -14.209403, -14.2242, -14.214697, -14.211767, -14.201907, -14.198808, -14.216326, -14.214537, -14.2024555, -14.207947, -14.226093, -14.207213, -14.228181, -14.22822, -14.201804, -14.204676, -14.225766, -14.2186365, -14.220312, -14.222188, -14.200821, -14.2142315, -14.220243, -14.214275, -14.216102, -14.209749, -14.224033, -14.224399, -14.201918, -14.197427, -14.196765, -14.225914, -14.227845, -14.205559, -14.19949, -14.217586, -14.232254, -14.212357, -14.193056, -14.199045, -14.21706, -14.219505, -14.199795, -14.199642, -14.210084, -14.214303, -14.252746, -14.2009945, -14.210006, -14.224496, -14.214516, -14.251274, -14.206231, -14.231531, -14.222925, -14.198628, -14.212134, -14.213196, -14.2028265, -14.238359, -14.209755, -14.218565, -14.210773, -14.2144375, -14.214816, -14.218614, -14.215934, -14.211181, -14.213596, -14.214576, -14.22979, -14.221126, -14.256517, -14.219581, -14.209497, -14.225365, -14.211769, -14.227486, -14.21897, -14.212826, -14.203715, -14.214456, -14.208262, -14.20299, -14.231442, -14.2246475, -14.2177105, -14.2109995, -14.20445, -14.271657, -14.200781, -14.208889, -14.253414, -14.213155, -14.193383, -14.209012, -14.215294, -14.220622, -14.201672, -14.245585, -14.206589, -14.225977, -14.207351, -14.212629, -14.205261, -14.21674, -14.199461, -14.212896, -14.254538, -14.213438, -14.194415, -14.223295, -14.203632, -14.217056, -14.2266655, -14.205794, -14.199596, -14.22822, -14.225711, -14.207979, -14.207873, -14.201256, -14.224184, -14.224281, -14.194548, -14.218176, -14.251406, -14.234711, -14.20468, -14.199896, -14.20653, -14.2200985, -14.209651, -14.213272, -14.206045, -14.208786, -14.208777, -14.213449, -14.205125, -14.212777, -14.239288, -14.224161, -14.223291, -14.201517, -14.209009, -14.214967, -14.215258, -14.216262, -14.218048, -14.21886, -14.260092, -14.204094, -14.210822, -14.210846, -14.222129, -14.208899, -14.223267, -14.2175, -14.215263, -14.214371, -14.212566, -14.209804, -14.221323, -14.203863, -14.220445, -14.211826, -14.219334, -14.218262, -14.210148, -14.216171, -14.2171755, -14.225731, -14.202193, -14.216927, -14.21477, -14.208463, -14.203484, -14.260056, -14.257128, -14.207935, -14.248723, -14.2514515, -14.231894, -14.196064, -14.195498, -14.202936, -14.226231, -14.210275, -14.222374, -14.23538, -14.222337, -14.198564, -14.193123, -14.213946, -14.207849, -14.232129, -14.218032, -14.222607, -14.207451, -14.203059, -14.201014, -14.2054825, -14.20982, -14.212086, -14.201626, -14.219786]\n",
      "Loss: 1319.6431884765625\n",
      "R-squared Coefficient: -2.6905\n"
     ]
    }
   ],
   "source": [
    "test_loss = 0\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for X, y in test_loader:\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "        prediction = model(X).to(device)\n",
    "        # Remove extra dimensions from y\n",
    "        y = y.unsqueeze(1).squeeze()\n",
    "        loss = loss_func(prediction.squeeze(), y)\n",
    "        test_loss += loss.float().item()\n",
    "        y_true.extend(y.cpu().numpy())\n",
    "        y_pred.extend(prediction.cpu().numpy().squeeze())\n",
    "\n",
    "test_loss /= len(test_loader)\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "r2 = r2_score(y_true, y_pred)\n",
    "print(\"True labels:\", y_true)\n",
    "print(\"Predictions:\", y_pred)\n",
    "print(\"Loss:\", test_loss)\n",
    "print(f\"R-squared Coefficient: {r2:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
