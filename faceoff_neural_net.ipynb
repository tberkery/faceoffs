{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as functional\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "#from skimage import io\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3569, 694)\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"/Users/Tad/Documents/faceoffs\"\n",
    "data_initial = pd.read_csv(\"final_dataset.csv\")\n",
    "print(data_initial.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3569, 691)\n",
      "0: season\n",
      "1: game_id\n",
      "2: game_date\n",
      "3: session\n",
      "4: event_index\n",
      "5: game_period\n",
      "6: game_seconds\n",
      "7: clock_time\n",
      "8: event_type\n",
      "9: event_description\n",
      "10: event_zone\n",
      "11: event_team\n",
      "12: event_player_1\n",
      "13: event_player_2\n",
      "14: event_length\n",
      "15: coords_x\n",
      "16: coords_y\n",
      "17: home_goalie\n",
      "18: away_goalie\n",
      "19: home_team\n",
      "20: away_team\n",
      "21: home_score\n",
      "22: away_score\n",
      "23: game_score_state\n",
      "24: game_strength_state\n",
      "25: home_zone\n",
      "26: event_distance\n",
      "27: event_angle\n",
      "28: home_zonestart\n",
      "29: face_index\n",
      "30: pen_index\n",
      "31: shift_index\n",
      "32: pbp\n",
      "33: teams\n",
      "34: last_faceoff_winner\n",
      "35: last_faceoff_winner_home\n",
      "36: last_faceoff_winner_zone\n",
      "37: event_team_relative_to_faceoff\n",
      "38: event_zone_relative_to_faceoff_winner\n",
      "39: last_faceoff_winner_faceoff_zone\n",
      "40: event_team_positioning_last_faceoff\n",
      "41: last_faceoff_loser_faceoff_zone\n",
      "42: start_FA\n",
      "43: next_stoppage\n",
      "44: next_neutral_zone_event\n",
      "45: next_zone_exit\n",
      "46: is_FA\n",
      "47: end_FA\n",
      "48: FA_zone_time\n",
      "49: Pos_home_on_3\n",
      "50: Pos_home_on_4\n",
      "51: Pos_home_on_5\n",
      "52: Pos_away_on_1\n",
      "53: Pos_away_on_2\n",
      "54: Pos_away_on_3\n",
      "55: Pos_away_on_4\n",
      "56: Pos_away_on_5\n",
      "57: Pos_away_on_6\n",
      "58: Win_Players\n",
      "59: Lose_Players\n",
      "60: net_xg\n",
      "61: count\n",
      "62: attributable_possession\n",
      "63: Draft_Ov_Win_F\n",
      "64: Draft_Ov_Win_D\n",
      "65: Draft_Ov_Lose_F\n",
      "66: Draft_Ov_Lose_D\n",
      "67: GP_Win_F\n",
      "68: GP_Win_D\n",
      "69: GP_Lose_F\n",
      "70: GP_Lose_D\n",
      "71: TOI_Win_F\n",
      "72: TOI_Win_D\n",
      "73: TOI_Lose_F\n",
      "74: TOI_Lose_D\n",
      "75: G_Win_F\n",
      "76: G_Win_D\n",
      "77: G_Lose_F\n",
      "78: G_Lose_D\n",
      "79: A1_Win_F\n",
      "80: A1_Win_D\n",
      "81: A1_Lose_F\n",
      "82: A1_Lose_D\n",
      "83: A2_Win_F\n",
      "84: A2_Win_D\n",
      "85: A2_Lose_F\n",
      "86: A2_Lose_D\n",
      "87: Points_Win_F\n",
      "88: Points_Win_D\n",
      "89: Points_Lose_F\n",
      "90: Points_Lose_D\n",
      "91: iSF_Win_F\n",
      "92: iSF_Win_D\n",
      "93: iSF_Lose_F\n",
      "94: iSF_Lose_D\n",
      "95: iFF_Win_F\n",
      "96: iFF_Win_D\n",
      "97: iFF_Lose_F\n",
      "98: iFF_Lose_D\n",
      "99: iCF_Win_F\n",
      "100: iCF_Win_D\n",
      "101: iCF_Lose_F\n",
      "102: iCF_Lose_D\n",
      "103: ixG_Win_F\n",
      "104: ixG_Win_D\n",
      "105: ixG_Lose_F\n",
      "106: ixG_Lose_D\n",
      "107: Sh%_Win_F\n",
      "108: Sh%_Win_D\n",
      "109: Sh%_Lose_F\n",
      "110: Sh%_Lose_D\n",
      "111: FSh_Percent_Win_F\n",
      "112: FSh_Percent_Win_D\n",
      "113: FSh_Percent_Lose_F\n",
      "114: FSh_Percent_Lose_D\n",
      "115: xFSh_Percent_Win_F\n",
      "116: xFSh_Percent_Win_D\n",
      "117: xFSh_Percent_Lose_F\n",
      "118: xFSh_Percent_Lose_D\n",
      "119: iBLK_Win_F\n",
      "120: iBLK_Win_D\n",
      "121: iBLK_Lose_F\n",
      "122: iBLK_Lose_D\n",
      "123: GIVE_Win_F\n",
      "124: GIVE_Win_D\n",
      "125: GIVE_Lose_F\n",
      "126: GIVE_Lose_D\n",
      "127: TAKE_Win_F\n",
      "128: TAKE_Win_D\n",
      "129: TAKE_Lose_F\n",
      "130: TAKE_Lose_D\n",
      "131: iHF_Win_F\n",
      "132: iHF_Win_D\n",
      "133: iHF_Lose_F\n",
      "134: iHF_Lose_D\n",
      "135: iHA_Win_F\n",
      "136: iHA_Win_D\n",
      "137: iHA_Lose_F\n",
      "138: iHA_Lose_D\n",
      "139: iPENT2_Win_F\n",
      "140: iPENT2_Win_D\n",
      "141: iPENT2_Lose_F\n",
      "142: iPENT2_Lose_D\n",
      "143: iPEND2_Win_F\n",
      "144: iPEND2_Win_D\n",
      "145: iPEND2_Lose_F\n",
      "146: iPEND2_Lose_D\n",
      "147: iPENT5_Win_F\n",
      "148: iPENT5_Win_D\n",
      "149: iPENT5_Lose_F\n",
      "150: iPENT5_Lose_D\n",
      "151: iPEND5_Win_F\n",
      "152: iPEND5_Win_D\n",
      "153: iPEND5_Lose_F\n",
      "154: iPEND5_Lose_D\n",
      "155: iPEN_Plus_Minus_Win_F\n",
      "156: iPEN_Plus_Minus_Win_D\n",
      "157: iPEN_Plus_Minus_Lose_F\n",
      "158: iPEN_Plus_Minus_Lose_D\n",
      "159: RelTM_G_Plus_Minus_per_60_Win_F\n",
      "160: RelTM_G_Plus_Minus_per_60_Win_D\n",
      "161: RelTM_G_Plus_Minus_per_60_Lose_F\n",
      "162: RelTM_G_Plus_Minus_per_60_Lose_D\n",
      "163: RelTM_S_Plus_Minus_per_60_Win_F\n",
      "164: RelTM_S_Plus_Minus_per_60_Win_D\n",
      "165: RelTM_S_Plus_Minus_per_60_Lose_F\n",
      "166: RelTM_S_Plus_Minus_per_60_Lose_D\n",
      "167: RelTM_F_Plus_Minus_per_60_Win_F\n",
      "168: RelTM_F_Plus_Minus_per_60_Win_D\n",
      "169: RelTM_F_Plus_Minus_per_60_Lose_F\n",
      "170: RelTM_F_Plus_Minus_per_60_Lose_D\n",
      "171: RelTM_C_Plus_Minus_per_60_Win_F\n",
      "172: RelTM_C_Plus_Minus_per_60_Win_D\n",
      "173: RelTM_C_Plus_Minus_per_60_Lose_F\n",
      "174: RelTM_C_Plus_Minus_per_60_Lose_D\n",
      "175: RelTM_xG_Plus_Minus_per_60_Win_F\n",
      "176: RelTM_xG_Plus_Minus_per_60_Win_D\n",
      "177: RelTM_xG_Plus_Minus_per_60_Lose_F\n",
      "178: RelTM_xG_Plus_Minus_per_60_Lose_D\n",
      "179: RelTM_GF_per_60_Win_F\n",
      "180: RelTM_GF_per_60_Win_D\n",
      "181: RelTM_GF_per_60_Lose_F\n",
      "182: RelTM_GF_per_60_Lose_D\n",
      "183: RelTM_GA_per_60_Win_F\n",
      "184: RelTM_GA_per_60_Win_D\n",
      "185: RelTM_GA_per_60_Lose_F\n",
      "186: RelTM_GA_per_60_Lose_D\n",
      "187: RelTM_SF_per_60_Win_F\n",
      "188: RelTM_SF_per_60_Win_D\n",
      "189: RelTM_SF_per_60_Lose_F\n",
      "190: RelTM_SF_per_60_Lose_D\n",
      "191: RelTM_SA_per_60_Win_F\n",
      "192: RelTM_SA_per_60_Win_D\n",
      "193: RelTM_SA_per_60_Lose_F\n",
      "194: RelTM_SA_per_60_Lose_D\n",
      "195: RelTM_FF_per_60_Win_F\n",
      "196: RelTM_FF_per_60_Win_D\n",
      "197: RelTM_FF_per_60_Lose_F\n",
      "198: RelTM_FF_per_60_Lose_D\n",
      "199: RelTM_FA_per_60_Win_F\n",
      "200: RelTM_FA_per_60_Win_D\n",
      "201: RelTM_FA_per_60_Lose_F\n",
      "202: RelTM_FA_per_60_Lose_D\n",
      "203: RelTM_CF_per_60_Win_F\n",
      "204: RelTM_CF_per_60_Win_D\n",
      "205: RelTM_CF_per_60_Lose_F\n",
      "206: RelTM_CF_per_60_Lose_D\n",
      "207: RelTM_CA_per_60_Win_F\n",
      "208: RelTM_CA_per_60_Win_D\n",
      "209: RelTM_CA_per_60_Lose_F\n",
      "210: RelTM_CA_per_60_Lose_D\n",
      "211: RelTM_xGF_per_60_Win_F\n",
      "212: RelTM_xGF_per_60_Win_D\n",
      "213: RelTM_xGF_per_60_Lose_F\n",
      "214: RelTM_xGF_per_60_Lose_D\n",
      "215: RelTM_xGA_per_60_Win_F\n",
      "216: RelTM_xGA_per_60_Win_D\n",
      "217: RelTM_xGA_per_60_Lose_F\n",
      "218: RelTM_xGA_per_60_Lose_D\n",
      "219: GF_Percent_on_ice_Win_F\n",
      "220: GF_Percent_on_ice_Win_D\n",
      "221: GF_Percent_on_ice_Lose_F\n",
      "222: GF_Percent_on_ice_Lose_D\n",
      "223: SF_Percent_on_ice_Win_F\n",
      "224: SF_Percent_on_ice_Win_D\n",
      "225: SF_Percent_on_ice_Lose_F\n",
      "226: SF_Percent_on_ice_Lose_D\n",
      "227: FF_Percent_on_ice_Win_F\n",
      "228: FF_Percent_on_ice_Win_D\n",
      "229: FF_Percent_on_ice_Lose_F\n",
      "230: FF_Percent_on_ice_Lose_D\n",
      "231: CF_Percent_on_ice_Win_F\n",
      "232: CF_Percent_on_ice_Win_D\n",
      "233: CF_Percent_on_ice_Lose_F\n",
      "234: CF_Percent_on_ice_Lose_D\n",
      "235: xGF_Percent_on_ice_Win_F\n",
      "236: xGF_Percent_on_ice_Win_D\n",
      "237: xGF_Percent_on_ice_Lose_F\n",
      "238: xGF_Percent_on_ice_Lose_D\n",
      "239: GF_per_60_on_ice_Win_F\n",
      "240: GF_per_60_on_ice_Win_D\n",
      "241: GF_per_60_on_ice_Lose_F\n",
      "242: GF_per_60_on_ice_Lose_D\n",
      "243: GA_per_60_on_ice_Win_F\n",
      "244: GA_per_60_on_ice_Win_D\n",
      "245: GA_per_60_on_ice_Lose_F\n",
      "246: GA_per_60_on_ice_Lose_D\n",
      "247: SF_per_60_on_ice_Win_F\n",
      "248: SF_per_60_on_ice_Win_D\n",
      "249: SF_per_60_on_ice_Lose_F\n",
      "250: SF_per_60_on_ice_Lose_D\n",
      "251: SA_per_60_on_ice_Win_F\n",
      "252: SA_per_60_on_ice_Win_D\n",
      "253: SA_per_60_on_ice_Lose_F\n",
      "254: SA_per_60_on_ice_Lose_D\n",
      "255: FF_per_60_on_ice_Win_F\n",
      "256: FF_per_60_on_ice_Win_D\n",
      "257: FF_per_60_on_ice_Lose_F\n",
      "258: FF_per_60_on_ice_Lose_D\n",
      "259: FA_per_60_on_ice_Win_F\n",
      "260: FA_per_60_on_ice_Win_D\n",
      "261: FA_per_60_on_ice_Lose_F\n",
      "262: FA_per_60_on_ice_Lose_D\n",
      "263: CF_per_60_on_ice_Win_F\n",
      "264: CF_per_60_on_ice_Win_D\n",
      "265: CF_per_60_on_ice_Lose_F\n",
      "266: CF_per_60_on_ice_Lose_D\n",
      "267: CA_per_60_on_ice_Win_F\n",
      "268: CA_per_60_on_ice_Win_D\n",
      "269: CA_per_60_on_ice_Lose_F\n",
      "270: CA_per_60_on_ice_Lose_D\n",
      "271: xGF_per_60_on_ice_Win_F\n",
      "272: xGF_per_60_on_ice_Win_D\n",
      "273: xGF_per_60_on_ice_Lose_F\n",
      "274: xGF_per_60_on_ice_Lose_D\n",
      "275: xGA_per_60_on_ice_Win_F\n",
      "276: xGA_per_60_on_ice_Win_D\n",
      "277: xGA_per_60_on_ice_Lose_F\n",
      "278: xGA_per_60_on_ice_Lose_D\n",
      "279: G_Plus_Minus_per_60_on_ice_Win_F\n",
      "280: G_Plus_Minus_per_60_on_ice_Win_D\n",
      "281: G_Plus_Minus_per_60_on_ice_Lose_F\n",
      "282: G_Plus_Minus_per_60_on_ice_Lose_D\n",
      "283: S_Plus_Minus_per_60_on_ice_Win_F\n",
      "284: S_Plus_Minus_per_60_on_ice_Win_D\n",
      "285: S_Plus_Minus_per_60_on_ice_Lose_F\n",
      "286: S_Plus_Minus_per_60_on_ice_Lose_D\n",
      "287: F_Plus_Minus_per_60_on_ice_Win_F\n",
      "288: F_Plus_Minus_per_60_on_ice_Win_D\n",
      "289: F_Plus_Minus_per_60_on_ice_Lose_F\n",
      "290: F_Plus_Minus_per_60_on_ice_Lose_D\n",
      "291: C_Plus_Minus_per_60_on_ice_Win_F\n",
      "292: C_Plus_Minus_per_60_on_ice_Win_D\n",
      "293: C_Plus_Minus_per_60_on_ice_Lose_F\n",
      "294: C_Plus_Minus_per_60_on_ice_Lose_D\n",
      "295: xG_Plus_Minus_per_60_on_ice_Win_F\n",
      "296: xG_Plus_Minus_per_60_on_ice_Win_D\n",
      "297: xG_Plus_Minus_per_60_on_ice_Lose_F\n",
      "298: xG_Plus_Minus_per_60_on_ice_Lose_D\n",
      "299: Sh_Percent_on_ice_Win_F\n",
      "300: Sh_Percent_on_ice_Win_D\n",
      "301: Sh_Percent_on_ice_Lose_F\n",
      "302: Sh_Percent_on_ice_Lose_D\n",
      "303: Sv_Percent_on_ice_Win_F\n",
      "304: Sv_Percent_on_ice_Win_D\n",
      "305: Sv_Percent_on_ice_Lose_F\n",
      "306: Sv_Percent_on_ice_Lose_D\n",
      "307: TOI_per_GP_Win_F\n",
      "308: TOI_per_GP_Win_D\n",
      "309: TOI_per_GP_Lose_F\n",
      "310: TOI_per_GP_Lose_D\n",
      "311: TOI_Percent_Win_F\n",
      "312: TOI_Percent_Win_D\n",
      "313: TOI_Percent_Lose_F\n",
      "314: TOI_Percent_Lose_D\n",
      "315: OZS_Percent_Win_F\n",
      "316: OZS_Percent_Win_D\n",
      "317: OZS_Percent_Lose_F\n",
      "318: OZS_Percent_Lose_D\n",
      "319: NZS_Percent_Win_F\n",
      "320: NZS_Percent_Win_D\n",
      "321: NZS_Percent_Lose_F\n",
      "322: NZS_Percent_Lose_D\n",
      "323: DZS_Percent_Win_F\n",
      "324: DZS_Percent_Win_D\n",
      "325: DZS_Percent_Lose_F\n",
      "326: DZS_Percent_Lose_D\n",
      "327: OTF_Percent_Win_F\n",
      "328: OTF_Percent_Win_D\n",
      "329: OTF_Percent_Lose_F\n",
      "330: OTF_Percent_Lose_D\n",
      "331: OZF_Percent_Win_F\n",
      "332: OZF_Percent_Win_D\n",
      "333: OZF_Percent_Lose_F\n",
      "334: OZF_Percent_Lose_D\n",
      "335: NZF_Percent_Win_F\n",
      "336: NZF_Percent_Win_D\n",
      "337: NZF_Percent_Lose_F\n",
      "338: NZF_Percent_Lose_D\n",
      "339: DZF_Percent_Win_F\n",
      "340: DZF_Percent_Win_D\n",
      "341: DZF_Percent_Lose_F\n",
      "342: DZF_Percent_Lose_D\n",
      "343: Ice_F_Win_F\n",
      "344: Ice_F_Win_D\n",
      "345: Ice_F_Lose_F\n",
      "346: Ice_F_Lose_D\n",
      "347: Ice_A_Win_F\n",
      "348: Ice_A_Win_D\n",
      "349: Ice_A_Lose_F\n",
      "350: Ice_A_Lose_D\n",
      "351: Ice_Percent_Win_F\n",
      "352: Ice_Percent_Win_D\n",
      "353: Ice_Percent_Lose_F\n",
      "354: Ice_Percent_Lose_D\n",
      "355: TOI_All_Win_F\n",
      "356: TOI_All_Win_D\n",
      "357: TOI_All_Lose_F\n",
      "358: TOI_All_Lose_D\n",
      "359: EVO_GAR_Win_F\n",
      "360: EVO_GAR_Win_D\n",
      "361: EVO_GAR_Lose_F\n",
      "362: EVO_GAR_Lose_D\n",
      "363: EVD_GAR_Win_F\n",
      "364: EVD_GAR_Win_D\n",
      "365: EVD_GAR_Lose_F\n",
      "366: EVD_GAR_Lose_D\n",
      "367: PPO_GAR_Win_F\n",
      "368: PPO_GAR_Win_D\n",
      "369: PPO_GAR_Lose_F\n",
      "370: PPO_GAR_Lose_D\n",
      "371: SHD_GAR_Win_F\n",
      "372: SHD_GAR_Win_D\n",
      "373: SHD_GAR_Lose_F\n",
      "374: SHD_GAR_Lose_D\n",
      "375: Take_GAR_Win_F\n",
      "376: Take_GAR_Win_D\n",
      "377: Take_GAR_Lose_F\n",
      "378: Take_GAR_Lose_D\n",
      "379: Draw_GAR_Win_F\n",
      "380: Draw_GAR_Win_D\n",
      "381: Draw_GAR_Lose_F\n",
      "382: Draw_GAR_Lose_D\n",
      "383: Off_GAR_Win_F\n",
      "384: Off_GAR_Win_D\n",
      "385: Off_GAR_Lose_F\n",
      "386: Off_GAR_Lose_D\n",
      "387: Def_GAR_Win_F\n",
      "388: Def_GAR_Win_D\n",
      "389: Def_GAR_Lose_F\n",
      "390: Def_GAR_Lose_D\n",
      "391: Pens_GAR_Win_F\n",
      "392: Pens_GAR_Win_D\n",
      "393: Pens_GAR_Lose_F\n",
      "394: Pens_GAR_Lose_D\n",
      "395: GAR_Win_F\n",
      "396: GAR_Win_D\n",
      "397: GAR_Lose_F\n",
      "398: GAR_Lose_D\n",
      "399: WAR_Win_F\n",
      "400: WAR_Win_D\n",
      "401: WAR_Lose_F\n",
      "402: WAR_Lose_D\n",
      "403: SPAR_Win_F\n",
      "404: SPAR_Win_D\n",
      "405: SPAR_Lose_F\n",
      "406: SPAR_Lose_D\n",
      "407: Age_goaltending_Win_F\n",
      "408: Age_goaltending_Win_D\n",
      "409: Age_goaltending_Lose_F\n",
      "410: Age_goaltending_Lose_D\n",
      "411: GA_Win_F\n",
      "412: GA_Win_D\n",
      "413: GA_Lose_F\n",
      "414: GA_Lose_D\n",
      "415: SA_Win_F\n",
      "416: SA_Win_D\n",
      "417: SA_Lose_F\n",
      "418: SA_Lose_D\n",
      "419: FA_Win_F\n",
      "420: FA_Win_D\n",
      "421: FA_Lose_F\n",
      "422: FA_Lose_D\n",
      "423: xGA_Win_F\n",
      "424: xGA_Win_D\n",
      "425: xGA_Lose_F\n",
      "426: xGA_Lose_D\n",
      "427: Sv_Percent_goaltending_Win_F\n",
      "428: Sv_Percent_goaltending_Win_D\n",
      "429: Sv_Percent_goaltending_Lose_F\n",
      "430: Sv_Percent_goaltending_Lose_D\n",
      "431: FSv_Percent_Win_F\n",
      "432: FSv_Percent_Win_D\n",
      "433: FSv_Percent_Lose_F\n",
      "434: FSv_Percent_Lose_D\n",
      "435: xFSv_Percent_Win_F\n",
      "436: xFSv_Percent_Win_D\n",
      "437: xFSv_Percent_Lose_F\n",
      "438: xFSv_Percent_Lose_D\n",
      "439: dFSv_Percent_Win_F\n",
      "440: dFSv_Percent_Win_D\n",
      "441: dFSv_Percent_Lose_F\n",
      "442: dFSv_Percent_Lose_D\n",
      "443: GSAA_Win_F\n",
      "444: GSAA_Win_D\n",
      "445: GSAA_Lose_F\n",
      "446: GSAA_Lose_D\n",
      "447: GSAx_Win_F\n",
      "448: GSAx_Win_D\n",
      "449: GSAx_Lose_F\n",
      "450: GSAx_Lose_D\n",
      "451: TOI_EV_Win_F\n",
      "452: TOI_EV_Win_D\n",
      "453: TOI_EV_Lose_F\n",
      "454: TOI_EV_Lose_D\n",
      "455: TOI_SH_Win_F\n",
      "456: TOI_SH_Win_D\n",
      "457: TOI_SH_Lose_F\n",
      "458: TOI_SH_Lose_D\n",
      "459: FA_EV_Win_F\n",
      "460: FA_EV_Win_D\n",
      "461: FA_EV_Lose_F\n",
      "462: FA_EV_Lose_D\n",
      "463: FA_SH_Win_F\n",
      "464: FA_SH_Win_D\n",
      "465: FA_SH_Lose_F\n",
      "466: FA_SH_Lose_D\n",
      "467: EVD_GAR_goaltending_Win_F\n",
      "468: EVD_GAR_goaltending_Win_D\n",
      "469: EVD_GAR_goaltending_Lose_F\n",
      "470: EVD_GAR_goaltending_Lose_D\n",
      "471: SHD_GAR_goaltending_Win_F\n",
      "472: SHD_GAR_goaltending_Win_D\n",
      "473: SHD_GAR_goaltending_Lose_F\n",
      "474: SHD_GAR_goaltending_Lose_D\n",
      "475: Take_GAR_goaltending_Win_F\n",
      "476: Take_GAR_goaltending_Win_D\n",
      "477: Take_GAR_goaltending_Lose_F\n",
      "478: Take_GAR_goaltending_Lose_D\n",
      "479: Draw_GAR_goaltending_Win_F\n",
      "480: Draw_GAR_goaltending_Win_D\n",
      "481: Draw_GAR_goaltending_Lose_F\n",
      "482: Draw_GAR_goaltending_Lose_D\n",
      "483: GAR_goaltending_Win_F\n",
      "484: GAR_goaltending_Win_D\n",
      "485: GAR_goaltending_Lose_F\n",
      "486: GAR_goaltending_Lose_D\n",
      "487: WAR_goaltending_Win_F\n",
      "488: WAR_goaltending_Win_D\n",
      "489: WAR_goaltending_Lose_F\n",
      "490: WAR_goaltending_Lose_D\n",
      "491: SPAR_goaltending_Win_F\n",
      "492: SPAR_goaltending_Win_D\n",
      "493: SPAR_goaltending_Lose_F\n",
      "494: SPAR_goaltending_Lose_D\n",
      "495: GF%_team_Win_F\n",
      "496: GF%_team_Win_D\n",
      "497: GF%_team_Lose_F\n",
      "498: GF%_team_Lose_D\n",
      "499: SF%_team_Win_F\n",
      "500: SF%_team_Win_D\n",
      "501: SF%_team_Lose_F\n",
      "502: SF%_team_Lose_D\n",
      "503: FF%_team_Win_F\n",
      "504: FF%_team_Win_D\n",
      "505: FF%_team_Lose_F\n",
      "506: FF%_team_Lose_D\n",
      "507: CF%_team_Win_F\n",
      "508: CF%_team_Win_D\n",
      "509: CF%_team_Lose_F\n",
      "510: CF%_team_Lose_D\n",
      "511: xGF_Percent_team_Win_F\n",
      "512: xGF_Percent_team_Win_D\n",
      "513: xGF_Percent_team_Lose_F\n",
      "514: xGF_Percent_team_Lose_D\n",
      "515: GF_per_60_team_Win_F\n",
      "516: GF_per_60_team_Win_D\n",
      "517: GF_per_60_team_Lose_F\n",
      "518: GF_per_60_team_Lose_D\n",
      "519: GA_per_60_team_Win_F\n",
      "520: GA_per_60_team_Win_D\n",
      "521: GA_per_60_team_Lose_F\n",
      "522: GA_per_60_team_Lose_D\n",
      "523: SF_per_60_team_Win_F\n",
      "524: SF_per_60_team_Win_D\n",
      "525: SF_per_60_team_Lose_F\n",
      "526: SF_per_60_team_Lose_D\n",
      "527: SA_per_60_team_Win_F\n",
      "528: SA_per_60_team_Win_D\n",
      "529: SA_per_60_team_Lose_F\n",
      "530: SA_per_60_team_Lose_D\n",
      "531: FF_per_60_team_Win_F\n",
      "532: FF_per_60_team_Win_D\n",
      "533: FF_per_60_team_Lose_F\n",
      "534: FF_per_60_team_Lose_D\n",
      "535: FA_per_60_team_Win_F\n",
      "536: FA_per_60_team_Win_D\n",
      "537: FA_per_60_team_Lose_F\n",
      "538: FA_per_60_team_Lose_D\n",
      "539: CF_per_60_team_Win_F\n",
      "540: CF_per_60_team_Win_D\n",
      "541: CF_per_60_team_Lose_F\n",
      "542: CF_per_60_team_Lose_D\n",
      "543: CA_per_60_team_Win_F\n",
      "544: CA_per_60_team_Win_D\n",
      "545: CA_per_60_team_Lose_F\n",
      "546: CA_per_60_team_Lose_D\n",
      "547: xGF_per_60_team_Win_F\n",
      "548: xGF_per_60_team_Win_D\n",
      "549: xGF_per_60_team_Lose_F\n",
      "550: xGF_per_60_team_Lose_D\n",
      "551: xGA_per_60_team_Win_F\n",
      "552: xGA_per_60_team_Win_D\n",
      "553: xGA_per_60_team_Lose_F\n",
      "554: xGA_per_60_team_Lose_D\n",
      "555: G_Plus_Minus_per_60_team_Win_F\n",
      "556: G_Plus_Minus_per_60_team_Win_D\n",
      "557: G_Plus_Minus_per_60_team_Lose_F\n",
      "558: G_Plus_Minus_per_60_team_Lose_D\n",
      "559: S_Plus_Minus_per_60_team_Win_F\n",
      "560: S_Plus_Minus_per_60_team_Win_D\n",
      "561: S_Plus_Minus_per_60_team_Lose_F\n",
      "562: S_Plus_Minus_per_60_team_Lose_D\n",
      "563: F_Plus_Minus_per_60_team_Win_F\n",
      "564: F_Plus_Minus_per_60_team_Win_D\n",
      "565: F_Plus_Minus_per_60_team_Lose_F\n",
      "566: F_Plus_Minus_per_60_team_Lose_D\n",
      "567: C_Plus_Minus_per_60_team_Win_F\n",
      "568: C_Plus_Minus_per_60_team_Win_D\n",
      "569: C_Plus_Minus_per_60_team_Lose_F\n",
      "570: C_Plus_Minus_per_60_team_Lose_D\n",
      "571: xG_Plus_Minus_per_60_team_Win_F\n",
      "572: xG_Plus_Minus_per_60_team_Win_D\n",
      "573: xG_Plus_Minus_per_60_team_Lose_F\n",
      "574: xG_Plus_Minus_per_60_team_Lose_D\n",
      "575: Sh_Percent_Adjusted_team_Win_F\n",
      "576: Sh_Percent_Adjusted_team_Win_D\n",
      "577: Sh_Percent_Adjusted_team_Lose_F\n",
      "578: Sh_Percent_Adjusted_team_Lose_D\n",
      "579: Sv_Percent_Adjusted_team_Win_F\n",
      "580: Sv_Percent_Adjusted_team_Win_D\n",
      "581: Sv_Percent_Adjusted_team_Lose_F\n",
      "582: Sv_Percent_Adjusted_team_Lose_D\n",
      "583: W_team_Win_F\n",
      "584: W_team_Win_D\n",
      "585: W_team_Lose_F\n",
      "586: W_team_Lose_D\n",
      "587: L_team_Win_F\n",
      "588: L_team_Win_D\n",
      "589: L_team_Lose_F\n",
      "590: L_team_Lose_D\n",
      "591: OL_team_Win_F\n",
      "592: OL_team_Win_D\n",
      "593: OL_team_Lose_F\n",
      "594: OL_team_Lose_D\n",
      "595: Points_team_Win_F\n",
      "596: Points_team_Win_D\n",
      "597: Points_team_Lose_F\n",
      "598: Points_team_Lose_D\n",
      "599: Points_Percent_team_Win_F\n",
      "600: Points_Percent_team_Win_D\n",
      "601: Points_Percent_team_Lose_F\n",
      "602: Points_Percent_team_Lose_D\n",
      "603: GF_team_Win_F\n",
      "604: GF_team_Win_D\n",
      "605: GF_team_Lose_F\n",
      "606: GF_team_Lose_D\n",
      "607: GA_team_Win_F\n",
      "608: GA_team_Win_D\n",
      "609: GA_team_Lose_F\n",
      "610: GA_team_Lose_D\n",
      "611: G_Plus_Minus_team_Win_F\n",
      "612: G_Plus_Minus_team_Win_D\n",
      "613: G_Plus_Minus_team_Lose_F\n",
      "614: G_Plus_Minus_team_Lose_D\n",
      "615: Sh_Percent_team_Win_F\n",
      "616: Sh_Percent_team_Win_D\n",
      "617: Sh_Percent_team_Lose_F\n",
      "618: Sh_Percent_team_Lose_D\n",
      "619: Sv_Percent_team_Win_F\n",
      "620: Sv_Percent_team_Win_D\n",
      "621: Sv_Percent_team_Lose_F\n",
      "622: Sv_Percent_team_Lose_D\n",
      "623: Entries_Win_F\n",
      "624: Entries_Win_D\n",
      "625: Entries_Lose_F\n",
      "626: Entries_Lose_D\n",
      "627: Carries_Win_F\n",
      "628: Carries_Win_D\n",
      "629: Carries_Lose_F\n",
      "630: Carries_Lose_D\n",
      "631: Carry-in%_Win_F\n",
      "632: Carry-in%_Win_D\n",
      "633: Carry-in%_Lose_F\n",
      "634: Carry-in%_Lose_D\n",
      "635: Passes_Win_F\n",
      "636: Passes_Win_D\n",
      "637: Passes_Lose_F\n",
      "638: Passes_Lose_D\n",
      "639: Pass%_Win_F\n",
      "640: Pass%_Win_D\n",
      "641: Pass%_Lose_F\n",
      "642: Pass%_Lose_D\n",
      "643: Recovered.Dump-ins_Win_F\n",
      "644: Recovered.Dump-ins_Win_D\n",
      "645: Recovered.Dump-ins_Lose_F\n",
      "646: Recovered.Dump-ins_Lose_D\n",
      "647: Exits_Win_F\n",
      "648: Exits_Win_D\n",
      "649: Exits_Lose_F\n",
      "650: Exits_Lose_D\n",
      "651: Exit%_Win_F\n",
      "652: Exit%_Win_D\n",
      "653: Exit%_Lose_F\n",
      "654: Exit%_Lose_D\n",
      "655: Fail%_Win_F\n",
      "656: Fail%_Win_D\n",
      "657: Fail%_Lose_F\n",
      "658: Fail%_Lose_D\n",
      "659: Clear_Win_F\n",
      "660: Clear_Win_D\n",
      "661: Clear_Lose_F\n",
      "662: Clear_Lose_D\n",
      "663: NZ_Win_F\n",
      "664: NZ_Win_D\n",
      "665: NZ_Lose_F\n",
      "666: NZ_Lose_D\n",
      "667: DZ_Win_F\n",
      "668: DZ_Win_D\n",
      "669: DZ_Lose_F\n",
      "670: DZ_Lose_D\n",
      "671: Low-to-High_Win_F\n",
      "672: Low-to-High_Win_D\n",
      "673: Low-to-High_Lose_F\n",
      "674: Low-to-High_Lose_D\n",
      "675: Behind.Net_Win_F\n",
      "676: Behind.Net_Win_D\n",
      "677: Behind.Net_Lose_F\n",
      "678: Behind.Net_Lose_D\n",
      "679: High.Danger_Win_F\n",
      "680: High.Danger_Win_D\n",
      "681: High.Danger_Lose_F\n",
      "682: High.Danger_Lose_D\n",
      "683: One-timer_Win_F\n",
      "684: One-timer_Win_D\n",
      "685: One-timer_Lose_F\n",
      "686: One-timer_Lose_D\n",
      "687: Deflection_Win_F\n",
      "688: Deflection_Win_D\n",
      "689: Deflection_Lose_F\n",
      "690: Deflection_Lose_D\n",
      "[0, 48, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 623, 624, 625, 626, 627, 628, 629, 630, 631, 632, 633, 634, 635, 636, 637, 638, 639, 640, 641, 642, 643, 644, 645, 646, 647, 648, 649, 650, 651, 652, 653, 654, 655, 656, 657, 658, 659, 660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 670, 671, 672, 673, 674, 675, 676, 677, 678, 679, 680, 681, 682, 683, 684, 685, 686, 687, 688, 689, 690]\n",
      "310\n",
      "(204, 310)\n",
      "Index(['season', 'FA_zone_time', 'Draft_Ov_Win_F', 'Draft_Ov_Win_D',\n",
      "       'Draft_Ov_Lose_F', 'Draft_Ov_Lose_D', 'GP_Win_F', 'GP_Win_D',\n",
      "       'GP_Lose_F', 'GP_Lose_D',\n",
      "       ...\n",
      "       'High.Danger_Lose_F', 'High.Danger_Lose_D', 'One-timer_Win_F',\n",
      "       'One-timer_Win_D', 'One-timer_Lose_F', 'One-timer_Lose_D',\n",
      "       'Deflection_Win_F', 'Deflection_Win_D', 'Deflection_Lose_F',\n",
      "       'Deflection_Lose_D'],\n",
      "      dtype='object', length=310)\n",
      "1425    20172018\n",
      "3557    20212022\n",
      "907     20172018\n",
      "1846    20172018\n",
      "2417    20172018\n",
      "          ...   \n",
      "3244    20172018\n",
      "162     20172018\n",
      "57      20172018\n",
      "184     20172018\n",
      "2793    20172018\n",
      "Name: season, Length: 714, dtype: int64\n",
      "1425    ALEKSANDER.BARKOV\n",
      "3557        ALEX.WENNBERG\n",
      "907         RYAN.O'REILLY\n",
      "1846       TOMAS.PLEKANEC\n",
      "2417          SEAN.KURALY\n",
      "              ...        \n",
      "3244         JARED.MCCANN\n",
      "162         CHARLIE.COYLE\n",
      "57            NICK.BONINO\n",
      "184          TYLER.SEGUIN\n",
      "2793        EVGENI.MALKIN\n",
      "Name: event_player_1, Length: 714, dtype: object\n",
      "1425         RYAN.O'REILLY\n",
      "3557           TYLER.BOZAK\n",
      "907           NICK.FOLIGNO\n",
      "1846    FREDERICK.GAUDREAU\n",
      "2417          DYLAN.LARKIN\n",
      "               ...        \n",
      "3244          MARK.LETESTU\n",
      "162           JORDAN.STAAL\n",
      "57            DAVID.KREJCI\n",
      "184           PAUL.STASTNY\n",
      "2793     HENRIK.ZETTERBERG\n",
      "Name: event_player_2, Length: 714, dtype: object\n"
     ]
    }
   ],
   "source": [
    "data_no_na = data_initial.dropna(axis = 1) # data should already have no NAs due to numerical imputation\n",
    "print(data_no_na.shape)\n",
    "i = 0\n",
    "for col in data_no_na.columns:\n",
    "   print(\"\" + str(i) + \": \" + col)\n",
    "   i = i + 1\n",
    "\n",
    "include_col_indices = [0, 48]  # Start with individual indices\n",
    "\n",
    "# Add ranges using loops\n",
    "include_col_indices.extend(range(63, 139))\n",
    "include_col_indices.extend(range(159, 279))\n",
    "include_col_indices.extend(range(299, 343))\n",
    "include_col_indices.extend(range(623, 691))\n",
    "\n",
    "print(include_col_indices)\n",
    "print(len(include_col_indices))\n",
    "\n",
    "# Subset columns from the NumPy array\n",
    "subset_data = data_no_na.iloc[:, include_col_indices]\n",
    "\n",
    "# Convert the subsetted array to a DataFrame\n",
    "df_subset = pd.DataFrame(subset_data)\n",
    "\n",
    "# Select columns of number data types\n",
    "data = df_subset.select_dtypes(include=['number'])\n",
    "data_removed = data[data['season'] == 20212022]\n",
    "print(data_removed.shape)\n",
    "data = data[data['season'] != 20212022]\n",
    "data_unnorm = data\n",
    "\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "vals = data.values #returns a numpy array\n",
    "scaler = preprocessing.StandardScaler()\n",
    "scaler_fit= scaler.fit(vals)\n",
    "scaler_transform = scaler_fit.transform(vals)\n",
    "data = pd.DataFrame(vals_scaled, columns = data.columns)\n",
    "#print(data)\n",
    "print(data.columns)\n",
    "\n",
    "y = data['FA_zone_time']\n",
    "x = data.loc[:, data.columns != 'FA_zone_time']\n",
    "x = x.loc[:, x.columns != 'FA_zone_time']\n",
    "X_train, X_intermediate, y_train, y_intermediate = train_test_split(x, y, train_size = 0.6, test_size=0.4, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_intermediate, y_intermediate, train_size = 0.5, test_size=0.5, random_state=45)\n",
    "\n",
    "# Above two lines in unison accomplish a 60-20-20 split for train-validation-test\n",
    "\n",
    "seasons_train = data_initial.iloc[X_train.index]['season']\n",
    "seasons_val = data_initial.iloc[X_val.index]['season']\n",
    "seasons_test = data_initial.iloc[X_test.index]['season']\n",
    "\n",
    "print(seasons_test)\n",
    "\n",
    "player_ids_train = data_initial.iloc[X_train.index]['event_player_1']\n",
    "player_ids_val = data_initial.iloc[X_val.index]['event_player_1']\n",
    "player_ids_test = data_initial.iloc[X_test.index]['event_player_1']\n",
    "print(player_ids_test)\n",
    "\n",
    "opposing_player_ids_train = data_initial.iloc[X_train.index]['event_player_2']\n",
    "opposing_player_ids_val = data_initial.iloc[X_val.index]['event_player_2']\n",
    "opposing_player_ids_test = data_initial.iloc[X_test.index]['event_player_2']\n",
    "print(opposing_player_ids_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device_selection = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device = torch.device(device_selection)\n",
    "print(device_selection)\n",
    "print(device)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "train_batch_size = 256\n",
    "val_batch_size = 256\n",
    "test_batch_size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = torch.tensor(self.df.iloc[idx, :-1].values, dtype=torch.float32)\n",
    "        y = torch.tensor(self.df.iloc[idx, -1:].values, dtype=torch.float32)\n",
    "        return x, y\n",
    "\n",
    "train_df =  pd.concat([X_train, y_train], axis=1)  # pandas DataFrame containing training data\n",
    "valid_df = pd.concat([X_val, y_val], axis=1) # pandas DataFrame containing validation data\n",
    "test_df = pd.concat([X_test, y_test], axis=1)  # pandas DataFrame containing test data\n",
    "train_dataset = CustomDataset(train_df)\n",
    "valid_dataset = CustomDataset(valid_df)\n",
    "test_dataset = CustomDataset(test_df)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=train_batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=val_batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=test_batch_size, shuffle=False)\n",
    "\n",
    "# Credit to ChatGPT for generating this cell of code in response to the following prompt: design a dataloader that accepts the output of a pandas train-validation-test split. Basically write code for a PyTorch dataloader that accepts pandas dataframes as arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VanillaNeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, dropout_rate):\n",
    "        super(VanillaNeuralNet, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.dropout_rate = dropout_rate\n",
    "        \n",
    "        self.fc1 = nn.Linear(self.input_size, 1000)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=self.dropout_rate)\n",
    "        self.batch_norm_1000 = nn.BatchNorm1d(1000)\n",
    "        self.batch_norm_500 = nn.BatchNorm1d(500)\n",
    "        self.batch_norm_400 = nn.BatchNorm1d(400)\n",
    "        self.batch_norm_300 = nn.BatchNorm1d(300)\n",
    "        self.batch_norm_200 = nn.BatchNorm1d(200)\n",
    "        self.batch_norm_100 = nn.BatchNorm1d(100)\n",
    "\n",
    "        self.fc2 = nn.Linear(1000, 1000)\n",
    "\n",
    "        self.fc3 = nn.Linear(1000, 1000)\n",
    "\n",
    "        self.fc4 = nn.Linear(1000, 500)\n",
    "\n",
    "        self.fc5 = nn.Linear(500, 500)\n",
    "\n",
    "        self.fc6 = nn.Linear(500, 400)\n",
    "\n",
    "        self.fc7 = nn.Linear(400, 300)\n",
    "\n",
    "        self.fc8 = nn.Linear(300, 200)\n",
    "\n",
    "        self.fc9 = nn.Linear(200, 100)\n",
    "\n",
    "        self.fc10 = nn.Linear(100, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #print(x.shape)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.batch_norm_1000(x)\n",
    "\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.batch_norm_1000(x)\n",
    "\n",
    "        x = self.fc3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.batch_norm_1000(x)\n",
    "\n",
    "        x = self.fc4(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.batch_norm_500(x)\n",
    "\n",
    "        x = self.fc5(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.batch_norm_500(x)\n",
    "\n",
    "        x = self.fc6(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.batch_norm_400(x)\n",
    "\n",
    "        x = self.fc7(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.batch_norm_300(x)\n",
    "\n",
    "        x = self.fc8(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.batch_norm_200(x)\n",
    "\n",
    "        x = self.fc9(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.batch_norm_100(x)\n",
    "\n",
    "        x = self.fc10(x)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 50\n",
    "trial_count = 50\n",
    "\n",
    "learning_rate = 0.01 # in last optimization, best lr ended up being 0.0016\n",
    "loss_func = nn.MSELoss()\n",
    "\n",
    "def train(model, train_loader, valid_loader, loss_func, optimizer, scheduler, num_epochs, device):\n",
    "    training_losses = np.array([])\n",
    "    validation_losses = np.array([])\n",
    "    for epoch in range(num_epochs):\n",
    "        print(\"\\nEPOCH \" +str(epoch+1)+\" of \"+str(num_epochs)+\"\\n\")\n",
    "        ########################### Training #####################################\n",
    "        model.train(True)\n",
    "        train_loss = 0\n",
    "        count = 0\n",
    "        for batch, (X, y) in enumerate(train_loader):\n",
    "            X = X.float().to(device)\n",
    "            y = y.float().to(device)\n",
    "            optimizer.zero_grad()\n",
    "            prediction = model(X).to(device)\n",
    "            loss = loss_func(prediction, y.view(-1, 1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            train_loss += loss.float().item()\n",
    "            count += train_batch_size\n",
    "        training_losses = np.append(training_losses, train_loss / count)\n",
    "        print(\"Training loss:\", train_loss / count)\n",
    "        #model.train(False)\n",
    "\n",
    "        ########################### Validation #####################################\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        count = 0\n",
    "        with torch.no_grad():\n",
    "            for batch, (X, y) in enumerate(valid_loader):\n",
    "                X = X.to(device)\n",
    "                y = y.to(device)\n",
    "                prediction = model(X).to(device)\n",
    "                loss = loss_func(prediction, y.view(-1, 1))\n",
    "                val_loss += loss.float().item()\n",
    "                count += val_batch_size\n",
    "        validation_losses = np.append(validation_losses, val_loss / count)\n",
    "        print(\"Validation loss:\", val_loss / count)\n",
    "    return validation_losses[-1]\n",
    "\n",
    "def objective(trial):\n",
    "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-1, log=True)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-6, 1e-3, log=True)\n",
    "    hidden_size = trial.suggest_int(\"hidden_size\", 300, 600)\n",
    "    dropout_rate = trial.suggest_float(\"dropout_rate\", 0.2, 0.5)\n",
    "    print(\"learning rate:\", lr)\n",
    "    print(\"weight_decay:\", weight_decay)\n",
    "    print(\"hidden_size:\", hidden_size)\n",
    "    print(\"dropout_rate:\", dropout_rate)\n",
    "\n",
    "    model = VanillaNeuralNet(input_dim, hidden_size, dropout_rate).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "    return train(model, train_loader, valid_loader, loss_func, optimizer, scheduler, num_epochs, device)\n",
    "\n",
    "input_dim = 309\n",
    "output_dim = 1\n",
    "hidden_dim = 300\n",
    "dropout = 0.1\n",
    "model = VanillaNeuralNet(input_dim, hidden_dim, dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 500\n",
    "dropout_rate = 0.25\n",
    "weight_decay = 0.000001\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Above inspired by last run best params, which were:\n",
    "# {'lr': 0.0016359428701992723, 'weight_decay': 3.4162832861696797e-06, 'hidden_size': 493, 'dropout_rate': 0.4524812840729844}\n",
    "model = VanillaNeuralNet(input_dim, hidden_size, dropout_rate).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "loss_func = nn.MSELoss()\n",
    "\n",
    "\n",
    "# # Train the model with the best hyperparameters on the entire dataset\n",
    "# for epoch in range(num_epochs):\n",
    "#     ########################### Training #####################################\n",
    "#     model.train(True)\n",
    "#     train_loss = 0\n",
    "#     count = 0\n",
    "#     for batch, (X, y) in enumerate(train_loader):\n",
    "#         X = X.float().to(device)\n",
    "#         y = y.float().to(device)\n",
    "#         prediction = model(X).to(device)\n",
    "#         optimizer.zero_grad()\n",
    "#         loss = loss_func(prediction, y.view(-1, 1).float().to(device))\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         train_loss += loss.float().item()\n",
    "#         count += 1\n",
    "#     scheduler.step()\n",
    "#     print(\"Epoch\", epoch+1, \"Training loss:\", train_loss / count)\n",
    "\n",
    "# # Save the best model\n",
    "# torch.save(model.state_dict(), \"faceoffs_off_off_state_dict2.pt\")\n",
    "# # save the trained model\n",
    "# torch.save(model, \"faceoffs_off_off2.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-30 03:27:04,729] A new study created in memory with name: no-name-5d20ea02-d1ef-4a71-8234-71f3e799414d\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning rate: 6.809338136745198e-05\n",
      "weight_decay: 1.7035177750901175e-05\n",
      "hidden_size: 523\n",
      "dropout_rate: 0.22678383656094042\n",
      "\n",
      "EPOCH 1 of 50\n",
      "\n",
      "Training loss: 0.005393321418927776\n",
      "Validation loss: 0.0032931099024911723\n",
      "\n",
      "EPOCH 2 of 50\n",
      "\n",
      "Training loss: 0.005124916581230031\n",
      "Validation loss: 0.003293322787309686\n",
      "\n",
      "EPOCH 3 of 50\n",
      "\n",
      "Training loss: 0.005166857037693262\n",
      "Validation loss: 0.003307464144503077\n",
      "\n",
      "EPOCH 4 of 50\n",
      "\n",
      "Training loss: 0.0055441259820428156\n",
      "Validation loss: 0.0033193297373751798\n",
      "\n",
      "EPOCH 5 of 50\n",
      "\n",
      "Training loss: 0.00522795661042134\n",
      "Validation loss: 0.003344061862056454\n",
      "\n",
      "EPOCH 6 of 50\n",
      "\n",
      "Training loss: 0.005567361393736469\n",
      "Validation loss: 0.0033532873882601657\n",
      "\n",
      "EPOCH 7 of 50\n",
      "\n",
      "Training loss: 0.005369110705537928\n",
      "Validation loss: 0.003356541351725658\n",
      "\n",
      "EPOCH 8 of 50\n",
      "\n",
      "Training loss: 0.0052971851028915905\n",
      "Validation loss: 0.00335526194733878\n",
      "\n",
      "EPOCH 9 of 50\n",
      "\n",
      "Training loss: 0.005238724820729759\n",
      "Validation loss: 0.003342749628548821\n",
      "\n",
      "EPOCH 10 of 50\n",
      "\n",
      "Training loss: 0.0055675700099931825\n",
      "Validation loss: 0.003347522889574369\n",
      "\n",
      "EPOCH 11 of 50\n",
      "\n",
      "Training loss: 0.0050966644452677835\n",
      "Validation loss: 0.003352172595138351\n",
      "\n",
      "EPOCH 12 of 50\n",
      "\n",
      "Training loss: 0.005070236252827777\n",
      "Validation loss: 0.003352405736222863\n",
      "\n",
      "EPOCH 13 of 50\n",
      "\n",
      "Training loss: 0.005329345766868856\n",
      "Validation loss: 0.0033566219887385764\n",
      "\n",
      "EPOCH 14 of 50\n",
      "\n",
      "Training loss: 0.0051521628888116944\n",
      "Validation loss: 0.0033558428597946963\n",
      "\n",
      "EPOCH 15 of 50\n",
      "\n",
      "Training loss: 0.005495304267646538\n",
      "Validation loss: 0.003342761735742291\n",
      "\n",
      "EPOCH 16 of 50\n",
      "\n",
      "Training loss: 0.005389905482944515\n",
      "Validation loss: 0.003348515989879767\n",
      "\n",
      "EPOCH 17 of 50\n",
      "\n",
      "Training loss: 0.005289835389703512\n",
      "Validation loss: 0.003336147638037801\n",
      "\n",
      "EPOCH 18 of 50\n",
      "\n",
      "Training loss: 0.00545272388909426\n",
      "Validation loss: 0.003350544177616636\n",
      "\n",
      "EPOCH 19 of 50\n",
      "\n",
      "Training loss: 0.005393779448543985\n",
      "Validation loss: 0.0033405300540228686\n",
      "\n",
      "EPOCH 20 of 50\n",
      "\n",
      "Training loss: 0.0053753975468377275\n",
      "Validation loss: 0.0033343523585548005\n",
      "\n",
      "EPOCH 21 of 50\n",
      "\n",
      "Training loss: 0.00518144682670633\n",
      "Validation loss: 0.003360909642651677\n",
      "\n",
      "EPOCH 22 of 50\n",
      "\n",
      "Training loss: 0.0050303656866567\n",
      "Validation loss: 0.003359852513919274\n",
      "\n",
      "EPOCH 23 of 50\n",
      "\n",
      "Training loss: 0.005020608452873098\n",
      "Validation loss: 0.0033639377603928247\n",
      "\n",
      "EPOCH 24 of 50\n",
      "\n",
      "Training loss: 0.005203564030428727\n",
      "Validation loss: 0.0033486338797956705\n",
      "\n",
      "EPOCH 25 of 50\n",
      "\n",
      "Training loss: 0.005292928156753381\n",
      "Validation loss: 0.003360837853203217\n",
      "\n",
      "EPOCH 26 of 50\n",
      "\n",
      "Training loss: 0.005422490881755948\n",
      "Validation loss: 0.003348062358175715\n",
      "\n",
      "EPOCH 27 of 50\n",
      "\n",
      "Training loss: 0.005633511813357472\n",
      "Validation loss: 0.0033506672674169144\n",
      "\n",
      "EPOCH 28 of 50\n",
      "\n",
      "Training loss: 0.005136357542748253\n",
      "Validation loss: 0.003348474157974124\n",
      "\n",
      "EPOCH 29 of 50\n",
      "\n",
      "Training loss: 0.0053186781652685665\n",
      "Validation loss: 0.0033544166944921017\n",
      "\n",
      "EPOCH 30 of 50\n",
      "\n",
      "Training loss: 0.005585817775378625\n",
      "Validation loss: 0.0033437275948623815\n",
      "\n",
      "EPOCH 31 of 50\n",
      "\n",
      "Training loss: 0.005224710149276588\n",
      "Validation loss: 0.003360364974165956\n",
      "\n",
      "EPOCH 32 of 50\n",
      "\n",
      "Training loss: 0.0054073024075478315\n",
      "Validation loss: 0.0033415444971372685\n",
      "\n",
      "EPOCH 33 of 50\n",
      "\n",
      "Training loss: 0.005197030880178015\n",
      "Validation loss: 0.0033536830451339483\n",
      "\n",
      "EPOCH 34 of 50\n",
      "\n",
      "Training loss: 0.00501482468098402\n",
      "Validation loss: 0.003361636229480306\n",
      "\n",
      "EPOCH 35 of 50\n",
      "\n",
      "Training loss: 0.0052116793683833545\n",
      "Validation loss: 0.003361747212087115\n",
      "\n",
      "EPOCH 36 of 50\n",
      "\n",
      "Training loss: 0.005269156148036321\n",
      "Validation loss: 0.0033530270836005607\n",
      "\n",
      "EPOCH 37 of 50\n",
      "\n",
      "Training loss: 0.0054225351971884566\n",
      "Validation loss: 0.0033525990632673106\n",
      "\n",
      "EPOCH 38 of 50\n",
      "\n",
      "Training loss: 0.0051505481565578114\n",
      "Validation loss: 0.0033474607237925134\n",
      "\n",
      "EPOCH 39 of 50\n",
      "\n",
      "Training loss: 0.005427965454550253\n",
      "Validation loss: 0.003347711560005943\n",
      "\n",
      "EPOCH 40 of 50\n",
      "\n",
      "Training loss: 0.005067394968743126\n",
      "Validation loss: 0.003345696721225977\n",
      "\n",
      "EPOCH 41 of 50\n",
      "\n",
      "Training loss: 0.005326819243944353\n",
      "Validation loss: 0.0033369711600244045\n",
      "\n",
      "EPOCH 42 of 50\n",
      "\n",
      "Training loss: 0.005232479863075746\n",
      "Validation loss: 0.0033467848940442004\n",
      "\n",
      "EPOCH 43 of 50\n",
      "\n",
      "Training loss: 0.005315695009711716\n",
      "Validation loss: 0.003339350844422976\n",
      "\n",
      "EPOCH 44 of 50\n",
      "\n",
      "Training loss: 0.005158561514690518\n",
      "Validation loss: 0.0033380302290121713\n",
      "\n",
      "EPOCH 45 of 50\n",
      "\n",
      "Training loss: 0.0053260922949347235\n",
      "Validation loss: 0.0033495958584050336\n",
      "\n",
      "EPOCH 46 of 50\n",
      "\n",
      "Training loss: 0.005563492110619943\n",
      "Validation loss: 0.0033523994497954845\n",
      "\n",
      "EPOCH 47 of 50\n",
      "\n",
      "Training loss: 0.005363512629022201\n",
      "Validation loss: 0.003352235226581494\n",
      "\n",
      "EPOCH 48 of 50\n",
      "\n",
      "Training loss: 0.00533670000731945\n",
      "Validation loss: 0.0033513364226867757\n",
      "\n",
      "EPOCH 49 of 50\n",
      "\n",
      "Training loss: 0.005288831165267361\n",
      "Validation loss: 0.003353090723976493\n",
      "\n",
      "EPOCH 50 of 50\n",
      "\n",
      "Training loss: 0.00528474351287716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-30 03:30:57,631] Trial 0 finished with value: 0.003348852740600705 and parameters: {'lr': 6.809338136745198e-05, 'weight_decay': 1.7035177750901175e-05, 'hidden_size': 523, 'dropout_rate': 0.22678383656094042}. Best is trial 0 with value: 0.003348852740600705.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.003348852740600705\n",
      "learning rate: 5.037295587384018e-05\n",
      "weight_decay: 6.286699943283631e-06\n",
      "hidden_size: 523\n",
      "dropout_rate: 0.22708097259014226\n",
      "\n",
      "EPOCH 1 of 50\n",
      "\n",
      "Training loss: 0.0052463581992520224\n",
      "Validation loss: 0.0033474373631179333\n",
      "\n",
      "EPOCH 2 of 50\n",
      "\n",
      "Training loss: 0.00541510350174374\n",
      "Validation loss: 0.0033584447422375283\n",
      "\n",
      "EPOCH 3 of 50\n",
      "\n",
      "Training loss: 0.005220594401988719\n",
      "Validation loss: 0.0033995783111701408\n",
      "\n",
      "EPOCH 4 of 50\n",
      "\n",
      "Training loss: 0.005152616054854459\n",
      "Validation loss: 0.003449534997344017\n",
      "\n",
      "EPOCH 5 of 50\n",
      "\n",
      "Training loss: 0.00513926986604929\n",
      "Validation loss: 0.003478188688556353\n",
      "\n",
      "EPOCH 6 of 50\n",
      "\n",
      "Training loss: 0.005177199504234725\n",
      "Validation loss: 0.003495599919309219\n",
      "\n",
      "EPOCH 7 of 50\n",
      "\n",
      "Training loss: 0.005042081052023504\n",
      "Validation loss: 0.003501219985385736\n",
      "\n",
      "EPOCH 8 of 50\n",
      "\n",
      "Training loss: 0.005585541483014822\n",
      "Validation loss: 0.0034787846573938928\n",
      "\n",
      "EPOCH 9 of 50\n",
      "\n",
      "Training loss: 0.005247405523227321\n",
      "Validation loss: 0.003491507920747002\n",
      "\n",
      "EPOCH 10 of 50\n",
      "\n",
      "Training loss: 0.005129698819170396\n",
      "Validation loss: 0.0035014088110377393\n",
      "\n",
      "EPOCH 11 of 50\n",
      "\n",
      "Training loss: 0.004915362586163812\n",
      "Validation loss: 0.003487187127272288\n",
      "\n",
      "EPOCH 12 of 50\n",
      "\n",
      "Training loss: 0.0051786243242936\n",
      "Validation loss: 0.003488937004779776\n",
      "\n",
      "EPOCH 13 of 50\n",
      "\n",
      "Training loss: 0.005021113229708539\n",
      "Validation loss: 0.0034842471747348704\n",
      "\n",
      "EPOCH 14 of 50\n",
      "\n",
      "Training loss: 0.005118532002800041\n",
      "Validation loss: 0.0035008353491624198\n",
      "\n",
      "EPOCH 15 of 50\n",
      "\n",
      "Training loss: 0.005505282276620467\n",
      "Validation loss: 0.0035049276581654945\n",
      "\n",
      "EPOCH 16 of 50\n",
      "\n",
      "Training loss: 0.004921707867955168\n",
      "Validation loss: 0.003505956536779801\n",
      "\n",
      "EPOCH 17 of 50\n",
      "\n",
      "Training loss: 0.005285266993774308\n",
      "Validation loss: 0.0035104559113581977\n",
      "\n",
      "EPOCH 18 of 50\n",
      "\n",
      "Training loss: 0.005090446263137791\n",
      "Validation loss: 0.0035095353766034045\n",
      "\n",
      "EPOCH 19 of 50\n",
      "\n",
      "Training loss: 0.005141453869226906\n",
      "Validation loss: 0.0034972581391533217\n",
      "\n",
      "EPOCH 20 of 50\n",
      "\n",
      "Training loss: 0.005369934977756606\n",
      "Validation loss: 0.0034948796965181828\n",
      "\n",
      "EPOCH 21 of 50\n",
      "\n",
      "Training loss: 0.005261841592275434\n",
      "Validation loss: 0.0035034435180326304\n",
      "\n",
      "EPOCH 22 of 50\n",
      "\n",
      "Training loss: 0.005068896183123191\n",
      "Validation loss: 0.0035082740553965173\n",
      "\n",
      "EPOCH 23 of 50\n",
      "\n",
      "Training loss: 0.004861131543293595\n",
      "Validation loss: 0.003502629386881987\n",
      "\n",
      "EPOCH 24 of 50\n",
      "\n",
      "Training loss: 0.005129874373475711\n",
      "Validation loss: 0.003499575269718965\n",
      "\n",
      "EPOCH 25 of 50\n",
      "\n",
      "Training loss: 0.005115744218023287\n",
      "Validation loss: 0.0034937295131385326\n",
      "\n",
      "EPOCH 26 of 50\n",
      "\n",
      "Training loss: 0.005213102843198512\n",
      "Validation loss: 0.003500541284059485\n",
      "\n",
      "EPOCH 27 of 50\n",
      "\n",
      "Training loss: 0.005390379940056139\n",
      "Validation loss: 0.0034987207812567553\n",
      "\n",
      "EPOCH 28 of 50\n",
      "\n",
      "Training loss: 0.005257273972448375\n",
      "Validation loss: 0.0034905095429470143\n",
      "\n",
      "EPOCH 29 of 50\n",
      "\n",
      "Training loss: 0.005107376362300581\n",
      "Validation loss: 0.0034952016236881414\n",
      "\n",
      "EPOCH 30 of 50\n",
      "\n",
      "Training loss: 0.004965423813296689\n",
      "Validation loss: 0.0034973178214083114\n",
      "\n",
      "EPOCH 31 of 50\n",
      "\n",
      "Training loss: 0.0054126133521397906\n",
      "Validation loss: 0.003510694873208801\n",
      "\n",
      "EPOCH 32 of 50\n",
      "\n",
      "Training loss: 0.004997105673990316\n",
      "Validation loss: 0.00349017643990616\n",
      "\n",
      "EPOCH 33 of 50\n",
      "\n",
      "Training loss: 0.005150464130565524\n",
      "Validation loss: 0.0035105705416450896\n",
      "\n",
      "EPOCH 34 of 50\n",
      "\n",
      "Training loss: 0.005311593620313538\n",
      "Validation loss: 0.003501597869520386\n",
      "\n",
      "EPOCH 35 of 50\n",
      "\n",
      "Training loss: 0.005520790194471677\n",
      "Validation loss: 0.0035014440460751453\n",
      "\n",
      "EPOCH 36 of 50\n",
      "\n",
      "Training loss: 0.005377048160880804\n",
      "Validation loss: 0.003489171930899223\n",
      "\n",
      "EPOCH 37 of 50\n",
      "\n",
      "Training loss: 0.004921110734964411\n",
      "Validation loss: 0.003506958018988371\n",
      "\n",
      "EPOCH 38 of 50\n",
      "\n",
      "Training loss: 0.005061090096003479\n",
      "Validation loss: 0.00348486906538407\n",
      "\n",
      "EPOCH 39 of 50\n",
      "\n",
      "Training loss: 0.0053985997413595515\n",
      "Validation loss: 0.0034920955852915845\n",
      "\n",
      "EPOCH 40 of 50\n",
      "\n",
      "Training loss: 0.005260780065630873\n",
      "Validation loss: 0.0034970242995768785\n",
      "\n",
      "EPOCH 41 of 50\n",
      "\n",
      "Training loss: 0.005292106368061569\n",
      "Validation loss: 0.0035037193447351456\n",
      "\n",
      "EPOCH 42 of 50\n",
      "\n",
      "Training loss: 0.005417979684554868\n",
      "Validation loss: 0.003493442122514049\n",
      "\n",
      "EPOCH 43 of 50\n",
      "\n",
      "Training loss: 0.0050123452415896785\n",
      "Validation loss: 0.0034971493296325207\n",
      "\n",
      "EPOCH 44 of 50\n",
      "\n",
      "Training loss: 0.005344240408804681\n",
      "Validation loss: 0.003477753217642506\n",
      "\n",
      "EPOCH 45 of 50\n",
      "\n",
      "Training loss: 0.005214614069296254\n",
      "Validation loss: 0.0034881356793145337\n",
      "\n",
      "EPOCH 46 of 50\n",
      "\n",
      "Training loss: 0.005346345534134243\n",
      "Validation loss: 0.0034853197478999696\n",
      "\n",
      "EPOCH 47 of 50\n",
      "\n",
      "Training loss: 0.0051063066348433495\n",
      "Validation loss: 0.003498873362938563\n",
      "\n",
      "EPOCH 48 of 50\n",
      "\n",
      "Training loss: 0.005074578751292493\n",
      "Validation loss: 0.003489072279383739\n",
      "\n",
      "EPOCH 49 of 50\n",
      "\n",
      "Training loss: 0.005437066054178609\n",
      "Validation loss: 0.003483194625005126\n",
      "\n",
      "EPOCH 50 of 50\n",
      "\n",
      "Training loss: 0.005245161915404929\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-30 03:35:00,448] Trial 1 finished with value: 0.0034989824052900076 and parameters: {'lr': 5.037295587384018e-05, 'weight_decay': 6.286699943283631e-06, 'hidden_size': 523, 'dropout_rate': 0.22708097259014226}. Best is trial 0 with value: 0.003348852740600705.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.0034989824052900076\n",
      "learning rate: 8.431054282248576e-05\n",
      "weight_decay: 3.9126979062041685e-06\n",
      "hidden_size: 460\n",
      "dropout_rate: 0.43699941576079554\n",
      "\n",
      "EPOCH 1 of 50\n",
      "\n",
      "Training loss: 0.005427899097816812\n",
      "Validation loss: 0.003334345684076349\n",
      "\n",
      "EPOCH 2 of 50\n",
      "\n",
      "Training loss: 0.005433072180797656\n",
      "Validation loss: 0.0033841216936707497\n",
      "\n",
      "EPOCH 3 of 50\n",
      "\n",
      "Training loss: 0.005105191634760963\n",
      "Validation loss: 0.0033981025529404483\n",
      "\n",
      "EPOCH 4 of 50\n",
      "\n",
      "Training loss: 0.0055388822220265865\n",
      "Validation loss: 0.0034165295461813607\n",
      "\n",
      "EPOCH 5 of 50\n",
      "\n",
      "Training loss: 0.005574665704949034\n",
      "Validation loss: 0.0034201041950533786\n",
      "\n",
      "EPOCH 6 of 50\n",
      "\n",
      "Training loss: 0.005563568220370346\n",
      "Validation loss: 0.0034425201204915843\n",
      "\n",
      "EPOCH 7 of 50\n",
      "\n",
      "Training loss: 0.005176089238375425\n",
      "Validation loss: 0.0034433751522252956\n",
      "\n",
      "EPOCH 8 of 50\n",
      "\n",
      "Training loss: 0.005477472518881162\n",
      "Validation loss: 0.003448817025249203\n",
      "\n",
      "EPOCH 9 of 50\n",
      "\n",
      "Training loss: 0.005347420047554705\n",
      "Validation loss: 0.0034455806793024144\n",
      "\n",
      "EPOCH 10 of 50\n",
      "\n",
      "Training loss: 0.005619094137930208\n",
      "Validation loss: 0.0034398111359526715\n",
      "\n",
      "EPOCH 11 of 50\n",
      "\n",
      "Training loss: 0.005678799675984515\n",
      "Validation loss: 0.00343684502877295\n",
      "\n",
      "EPOCH 12 of 50\n",
      "\n",
      "Training loss: 0.005476969035549296\n",
      "Validation loss: 0.0033947490931799016\n",
      "\n",
      "EPOCH 13 of 50\n",
      "\n",
      "Training loss: 0.005692065331257052\n",
      "Validation loss: 0.0034283024724572897\n",
      "\n",
      "EPOCH 14 of 50\n",
      "\n",
      "Training loss: 0.005500083892709679\n",
      "Validation loss: 0.0034398483112454414\n",
      "\n",
      "EPOCH 15 of 50\n",
      "\n",
      "Training loss: 0.005487683384368817\n",
      "Validation loss: 0.0034632652532309294\n",
      "\n",
      "EPOCH 16 of 50\n",
      "\n",
      "Training loss: 0.0054714076686650515\n",
      "Validation loss: 0.0034480366545418897\n",
      "\n",
      "EPOCH 17 of 50\n",
      "\n",
      "Training loss: 0.005390882854246431\n",
      "Validation loss: 0.003443280622983972\n",
      "\n",
      "EPOCH 18 of 50\n",
      "\n",
      "Training loss: 0.005473861807129449\n",
      "Validation loss: 0.0034388179580370584\n",
      "\n",
      "EPOCH 19 of 50\n",
      "\n",
      "Training loss: 0.005550446971837018\n",
      "Validation loss: 0.003445735356460015\n",
      "\n",
      "EPOCH 20 of 50\n",
      "\n",
      "Training loss: 0.0054131125410397845\n",
      "Validation loss: 0.0034355490934103727\n",
      "\n",
      "EPOCH 21 of 50\n",
      "\n",
      "Training loss: 0.005493701176924838\n",
      "Validation loss: 0.003435867295290033\n",
      "\n",
      "EPOCH 22 of 50\n",
      "\n",
      "Training loss: 0.005471605497101943\n",
      "Validation loss: 0.003431175990651051\n",
      "\n",
      "EPOCH 23 of 50\n",
      "\n",
      "Training loss: 0.0052758016002674895\n",
      "Validation loss: 0.0034314613634099564\n",
      "\n",
      "EPOCH 24 of 50\n",
      "\n",
      "Training loss: 0.005555782932788134\n",
      "Validation loss: 0.0034682198893278837\n",
      "\n",
      "EPOCH 25 of 50\n",
      "\n",
      "Training loss: 0.005491701575616996\n",
      "Validation loss: 0.003457671186576287\n",
      "\n",
      "EPOCH 26 of 50\n",
      "\n",
      "Training loss: 0.005308256251737475\n",
      "Validation loss: 0.0034490150089065232\n",
      "\n",
      "EPOCH 27 of 50\n",
      "\n",
      "Training loss: 0.005440636486228969\n",
      "Validation loss: 0.003448057066028317\n",
      "\n",
      "EPOCH 28 of 50\n",
      "\n",
      "Training loss: 0.006109551319645511\n",
      "Validation loss: 0.003452056087553501\n",
      "\n",
      "EPOCH 29 of 50\n",
      "\n",
      "Training loss: 0.00541236675861809\n",
      "Validation loss: 0.003457540568585197\n",
      "\n",
      "EPOCH 30 of 50\n",
      "\n",
      "Training loss: 0.005179689938409461\n",
      "Validation loss: 0.003426898426065842\n",
      "\n",
      "EPOCH 31 of 50\n",
      "\n",
      "Training loss: 0.005447065483571755\n",
      "Validation loss: 0.0034278055342535176\n",
      "\n",
      "EPOCH 32 of 50\n",
      "\n",
      "Training loss: 0.005462762175334824\n",
      "Validation loss: 0.0034359749406576157\n",
      "\n",
      "EPOCH 33 of 50\n",
      "\n",
      "Training loss: 0.005434893459702532\n",
      "Validation loss: 0.0034723880235105753\n",
      "\n",
      "EPOCH 34 of 50\n",
      "\n",
      "Training loss: 0.005454397605111201\n",
      "Validation loss: 0.003448567818850279\n",
      "\n",
      "EPOCH 35 of 50\n",
      "\n",
      "Training loss: 0.00531812793471747\n",
      "Validation loss: 0.0034409082339455685\n",
      "\n",
      "EPOCH 36 of 50\n",
      "\n",
      "Training loss: 0.005577039542711443\n",
      "Validation loss: 0.003446214599534869\n",
      "\n",
      "EPOCH 37 of 50\n",
      "\n",
      "Training loss: 0.005786972896506389\n",
      "Validation loss: 0.003434765695904692\n",
      "\n",
      "EPOCH 38 of 50\n",
      "\n",
      "Training loss: 0.0056412854852775736\n",
      "Validation loss: 0.0034424768139918647\n",
      "\n",
      "EPOCH 39 of 50\n",
      "\n",
      "Training loss: 0.0053781744920545155\n",
      "Validation loss: 0.0034139398485422134\n",
      "\n",
      "EPOCH 40 of 50\n",
      "\n",
      "Training loss: 0.005219176334018509\n",
      "Validation loss: 0.0034167316431800523\n",
      "\n",
      "EPOCH 41 of 50\n",
      "\n",
      "Training loss: 0.00564358323915965\n",
      "Validation loss: 0.0034351684929182134\n",
      "\n",
      "EPOCH 42 of 50\n",
      "\n",
      "Training loss: 0.0053730364371505045\n",
      "Validation loss: 0.0034386528811107078\n",
      "\n",
      "EPOCH 43 of 50\n",
      "\n",
      "Training loss: 0.005546242541943987\n",
      "Validation loss: 0.0034432195437451205\n",
      "\n",
      "EPOCH 44 of 50\n",
      "\n",
      "Training loss: 0.005571761530720525\n",
      "Validation loss: 0.0034497800127913556\n",
      "\n",
      "EPOCH 45 of 50\n",
      "\n",
      "Training loss: 0.005480848252773285\n",
      "Validation loss: 0.0034415931440889835\n",
      "\n",
      "EPOCH 46 of 50\n",
      "\n",
      "Training loss: 0.005497494143330389\n",
      "Validation loss: 0.0034442878483484187\n",
      "\n",
      "EPOCH 47 of 50\n",
      "\n",
      "Training loss: 0.005437381772531403\n",
      "Validation loss: 0.0034235865653802953\n",
      "\n",
      "EPOCH 48 of 50\n",
      "\n",
      "Training loss: 0.005126707385190659\n",
      "Validation loss: 0.003440892013410727\n",
      "\n",
      "EPOCH 49 of 50\n",
      "\n",
      "Training loss: 0.005616438885529836\n",
      "Validation loss: 0.003418102627620101\n",
      "\n",
      "EPOCH 50 of 50\n",
      "\n",
      "Training loss: 0.005736657159609927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-30 03:39:04,936] Trial 2 finished with value: 0.003451230082040032 and parameters: {'lr': 8.431054282248576e-05, 'weight_decay': 3.9126979062041685e-06, 'hidden_size': 460, 'dropout_rate': 0.43699941576079554}. Best is trial 0 with value: 0.003348852740600705.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.003451230082040032\n",
      "learning rate: 5.7674308320099585e-05\n",
      "weight_decay: 1.0117120068399508e-05\n",
      "hidden_size: 348\n",
      "dropout_rate: 0.47596968205841317\n",
      "\n",
      "EPOCH 1 of 50\n",
      "\n",
      "Training loss: 0.00554149163266023\n",
      "Validation loss: 0.0032986155711114407\n",
      "\n",
      "EPOCH 2 of 50\n",
      "\n",
      "Training loss: 0.005563787702057097\n",
      "Validation loss: 0.0033036289581408105\n",
      "\n",
      "EPOCH 3 of 50\n",
      "\n",
      "Training loss: 0.005376186584018999\n",
      "Validation loss: 0.0033061273861676455\n",
      "\n",
      "EPOCH 4 of 50\n",
      "\n",
      "Training loss: 0.005395873863663938\n",
      "Validation loss: 0.0033089271746575832\n",
      "\n",
      "EPOCH 5 of 50\n",
      "\n",
      "Training loss: 0.0052181291911337115\n",
      "Validation loss: 0.0033056430208186307\n",
      "\n",
      "EPOCH 6 of 50\n",
      "\n",
      "Training loss: 0.005580715265952879\n",
      "Validation loss: 0.003303265276675423\n",
      "\n",
      "EPOCH 7 of 50\n",
      "\n",
      "Training loss: 0.005854210247182184\n",
      "Validation loss: 0.0033064369733134904\n",
      "\n",
      "EPOCH 8 of 50\n",
      "\n",
      "Training loss: 0.00529487244784832\n",
      "Validation loss: 0.0033091078512370586\n",
      "\n",
      "EPOCH 9 of 50\n",
      "\n",
      "Training loss: 0.005359121831133962\n",
      "Validation loss: 0.003308745256314675\n",
      "\n",
      "EPOCH 10 of 50\n",
      "\n",
      "Training loss: 0.005541864730831649\n",
      "Validation loss: 0.0033035203038404384\n",
      "\n",
      "EPOCH 11 of 50\n",
      "\n",
      "Training loss: 0.005486121763371759\n",
      "Validation loss: 0.0033029441256076097\n",
      "\n",
      "EPOCH 12 of 50\n",
      "\n",
      "Training loss: 0.00518032000400126\n",
      "Validation loss: 0.0033066696487367153\n",
      "\n",
      "EPOCH 13 of 50\n",
      "\n",
      "Training loss: 0.005506050410783953\n",
      "Validation loss: 0.0033062660756210485\n",
      "\n",
      "EPOCH 14 of 50\n",
      "\n",
      "Training loss: 0.006002894602715969\n",
      "Validation loss: 0.0033106424380093813\n",
      "\n",
      "EPOCH 15 of 50\n",
      "\n",
      "Training loss: 0.005370008474629786\n",
      "Validation loss: 0.0033048061498751244\n",
      "\n",
      "EPOCH 16 of 50\n",
      "\n",
      "Training loss: 0.005574946291744709\n",
      "Validation loss: 0.0033077688422054052\n",
      "\n",
      "EPOCH 17 of 50\n",
      "\n",
      "Training loss: 0.005496752681210637\n",
      "Validation loss: 0.00330969481728971\n",
      "\n",
      "EPOCH 18 of 50\n",
      "\n",
      "Training loss: 0.005388854951080348\n",
      "Validation loss: 0.003306481366356214\n",
      "\n",
      "EPOCH 19 of 50\n",
      "\n",
      "Training loss: 0.005385413041545285\n",
      "Validation loss: 0.0033047222532331944\n",
      "\n",
      "EPOCH 20 of 50\n",
      "\n",
      "Training loss: 0.005495083259625567\n",
      "Validation loss: 0.0033051700641711554\n",
      "\n",
      "EPOCH 21 of 50\n",
      "\n",
      "Training loss: 0.005409526819777157\n",
      "Validation loss: 0.0033053160489847264\n",
      "\n",
      "EPOCH 22 of 50\n",
      "\n",
      "Training loss: 0.005488465540111065\n",
      "Validation loss: 0.0033006203981737294\n",
      "\n",
      "EPOCH 23 of 50\n",
      "\n",
      "Training loss: 0.005554634870754348\n",
      "Validation loss: 0.003302585876857241\n",
      "\n",
      "EPOCH 24 of 50\n",
      "\n",
      "Training loss: 0.006280485437148147\n",
      "Validation loss: 0.0033075010869652033\n",
      "\n",
      "EPOCH 25 of 50\n",
      "\n",
      "Training loss: 0.005537398004283507\n",
      "Validation loss: 0.0033063883893191814\n",
      "\n",
      "EPOCH 26 of 50\n",
      "\n",
      "Training loss: 0.005569768396930562\n",
      "Validation loss: 0.0033084528986364603\n",
      "\n",
      "EPOCH 27 of 50\n",
      "\n",
      "Training loss: 0.005463946869389879\n",
      "Validation loss: 0.0033082064085950456\n",
      "\n",
      "EPOCH 28 of 50\n",
      "\n",
      "Training loss: 0.0057613676310413415\n",
      "Validation loss: 0.003303269778067867\n",
      "\n",
      "EPOCH 29 of 50\n",
      "\n",
      "Training loss: 0.005504838734244307\n",
      "Validation loss: 0.003309421706944704\n",
      "\n",
      "EPOCH 30 of 50\n",
      "\n",
      "Training loss: 0.005375083121988509\n",
      "Validation loss: 0.0033080344243595996\n",
      "\n",
      "EPOCH 31 of 50\n",
      "\n",
      "Training loss: 0.0052984735359334284\n",
      "Validation loss: 0.00330923730507493\n",
      "\n",
      "EPOCH 32 of 50\n",
      "\n",
      "Training loss: 0.005425265317575799\n",
      "Validation loss: 0.0033142573665827513\n",
      "\n",
      "EPOCH 33 of 50\n",
      "\n",
      "Training loss: 0.005344681027862761\n",
      "Validation loss: 0.0033135490181545415\n",
      "\n",
      "EPOCH 34 of 50\n",
      "\n",
      "Training loss: 0.005653752117521233\n",
      "Validation loss: 0.003312516569470366\n",
      "\n",
      "EPOCH 35 of 50\n",
      "\n",
      "Training loss: 0.005300397001620796\n",
      "Validation loss: 0.003305029667293032\n",
      "\n",
      "EPOCH 36 of 50\n",
      "\n",
      "Training loss: 0.005820685480203893\n",
      "Validation loss: 0.003307677417372664\n",
      "\n",
      "EPOCH 37 of 50\n",
      "\n",
      "Training loss: 0.006022871316721042\n",
      "Validation loss: 0.003304879025866588\n",
      "\n",
      "EPOCH 38 of 50\n",
      "\n",
      "Training loss: 0.0053133917196343345\n",
      "Validation loss: 0.003303424920886755\n",
      "\n",
      "EPOCH 39 of 50\n",
      "\n",
      "Training loss: 0.00591721352086299\n",
      "Validation loss: 0.0033073830418288708\n",
      "\n",
      "EPOCH 40 of 50\n",
      "\n",
      "Training loss: 0.005460462791638242\n",
      "Validation loss: 0.003316667707016071\n",
      "\n",
      "EPOCH 41 of 50\n",
      "\n",
      "Training loss: 0.005724599688417382\n",
      "Validation loss: 0.003315524353335301\n",
      "\n",
      "EPOCH 42 of 50\n",
      "\n",
      "Training loss: 0.005406786998112996\n",
      "Validation loss: 0.003308563648412625\n",
      "\n",
      "EPOCH 43 of 50\n",
      "\n",
      "Training loss: 0.005627252109762695\n",
      "Validation loss: 0.003306895882512132\n",
      "\n",
      "EPOCH 44 of 50\n",
      "\n",
      "Training loss: 0.005100347256908814\n",
      "Validation loss: 0.003307891388734182\n",
      "\n",
      "EPOCH 45 of 50\n",
      "\n",
      "Training loss: 0.00545877730473876\n",
      "Validation loss: 0.0033047104564805827\n",
      "\n",
      "EPOCH 46 of 50\n",
      "\n",
      "Training loss: 0.005637805598477523\n",
      "Validation loss: 0.00330419714252154\n",
      "\n",
      "EPOCH 47 of 50\n",
      "\n",
      "Training loss: 0.005536618539028698\n",
      "Validation loss: 0.003302829572930932\n",
      "\n",
      "EPOCH 48 of 50\n",
      "\n",
      "Training loss: 0.005929577992194229\n",
      "Validation loss: 0.0033100181414435306\n",
      "\n",
      "EPOCH 49 of 50\n",
      "\n",
      "Training loss: 0.00553103784720103\n",
      "Validation loss: 0.0033071869208166995\n",
      "\n",
      "EPOCH 50 of 50\n",
      "\n",
      "Training loss: 0.0054902188583380645\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-30 03:43:07,527] Trial 3 finished with value: 0.0033067727151016393 and parameters: {'lr': 5.7674308320099585e-05, 'weight_decay': 1.0117120068399508e-05, 'hidden_size': 348, 'dropout_rate': 0.47596968205841317}. Best is trial 3 with value: 0.0033067727151016393.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.0033067727151016393\n",
      "learning rate: 0.0022880778854138923\n",
      "weight_decay: 1.0323338072000017e-05\n",
      "hidden_size: 465\n",
      "dropout_rate: 0.2185236150231209\n",
      "\n",
      "EPOCH 1 of 50\n",
      "\n",
      "Training loss: 0.004969509732392099\n",
      "Validation loss: 0.003334658918902278\n",
      "\n",
      "EPOCH 2 of 50\n",
      "\n",
      "Training loss: 0.0046342167382438975\n",
      "Validation loss: 0.0033497767678151527\n",
      "\n",
      "EPOCH 3 of 50\n",
      "\n",
      "Training loss: 0.00459329879635738\n",
      "Validation loss: 0.0033491116482764482\n",
      "\n",
      "EPOCH 4 of 50\n",
      "\n",
      "Training loss: 0.00448897793992526\n",
      "Validation loss: 0.0033632518413166204\n",
      "\n",
      "EPOCH 5 of 50\n",
      "\n",
      "Training loss: 0.004407595636116134\n",
      "Validation loss: 0.0033636963926255703\n",
      "\n",
      "EPOCH 6 of 50\n",
      "\n",
      "Training loss: 0.0045523318824254805\n",
      "Validation loss: 0.0033750763007750115\n",
      "\n",
      "EPOCH 7 of 50\n",
      "\n",
      "Training loss: 0.004589217973666059\n",
      "Validation loss: 0.00337360977816085\n",
      "\n",
      "EPOCH 8 of 50\n",
      "\n",
      "Training loss: 0.004740514740761783\n",
      "Validation loss: 0.0033788757864385843\n",
      "\n",
      "EPOCH 9 of 50\n",
      "\n",
      "Training loss: 0.004477589624002576\n",
      "Validation loss: 0.003377520634482304\n",
      "\n",
      "EPOCH 10 of 50\n",
      "\n",
      "Training loss: 0.004338048476104935\n",
      "Validation loss: 0.0033733926247805357\n",
      "\n",
      "EPOCH 11 of 50\n",
      "\n",
      "Training loss: 0.004648795687697\n",
      "Validation loss: 0.0033862582252671323\n",
      "\n",
      "EPOCH 12 of 50\n",
      "\n",
      "Training loss: 0.004685909332086642\n",
      "Validation loss: 0.003388020365188519\n",
      "\n",
      "EPOCH 13 of 50\n",
      "\n",
      "Training loss: 0.004549621785473492\n",
      "Validation loss: 0.0033720582723617554\n",
      "\n",
      "EPOCH 14 of 50\n",
      "\n",
      "Training loss: 0.004576689408471187\n",
      "Validation loss: 0.003374426858499646\n",
      "\n",
      "EPOCH 15 of 50\n",
      "\n",
      "Training loss: 0.004961538697696394\n",
      "Validation loss: 0.0033824535397191844\n",
      "\n",
      "EPOCH 16 of 50\n",
      "\n",
      "Training loss: 0.004389481101599004\n",
      "Validation loss: 0.003391218837350607\n",
      "\n",
      "EPOCH 17 of 50\n",
      "\n",
      "Training loss: 0.004359128471049998\n",
      "Validation loss: 0.0033925932366400957\n",
      "\n",
      "EPOCH 18 of 50\n",
      "\n",
      "Training loss: 0.004464371776622202\n",
      "Validation loss: 0.0033889045007526875\n",
      "\n",
      "EPOCH 19 of 50\n",
      "\n",
      "Training loss: 0.004680856570808424\n",
      "Validation loss: 0.003398604691028595\n",
      "\n",
      "EPOCH 20 of 50\n",
      "\n",
      "Training loss: 0.004565066942531202\n",
      "Validation loss: 0.0033876854771127305\n",
      "\n",
      "EPOCH 21 of 50\n",
      "\n",
      "Training loss: 0.0046741357331888545\n",
      "Validation loss: 0.0033784700402369103\n",
      "\n",
      "EPOCH 22 of 50\n",
      "\n",
      "Training loss: 0.0044269556852264535\n",
      "Validation loss: 0.0033762982736031213\n",
      "\n",
      "EPOCH 23 of 50\n",
      "\n",
      "Training loss: 0.004389861003599233\n",
      "Validation loss: 0.003397168436398109\n",
      "\n",
      "EPOCH 24 of 50\n",
      "\n",
      "Training loss: 0.00476933640634848\n",
      "Validation loss: 0.0033843968994915485\n",
      "\n",
      "EPOCH 25 of 50\n",
      "\n",
      "Training loss: 0.004624566658296519\n",
      "Validation loss: 0.003385988917822639\n",
      "\n",
      "EPOCH 26 of 50\n",
      "\n",
      "Training loss: 0.0044630594913744265\n",
      "Validation loss: 0.00337759320003291\n",
      "\n",
      "EPOCH 27 of 50\n",
      "\n",
      "Training loss: 0.004741135415517622\n",
      "Validation loss: 0.00337199781400462\n",
      "\n",
      "EPOCH 28 of 50\n",
      "\n",
      "Training loss: 0.004745807705654038\n",
      "Validation loss: 0.0033755942713469267\n",
      "\n",
      "EPOCH 29 of 50\n",
      "\n",
      "Training loss: 0.004586247028782964\n",
      "Validation loss: 0.003384050835544864\n",
      "\n",
      "EPOCH 30 of 50\n",
      "\n",
      "Training loss: 0.0048206178730146754\n",
      "Validation loss: 0.003382759944846233\n",
      "\n",
      "EPOCH 31 of 50\n",
      "\n",
      "Training loss: 0.004490667902347114\n",
      "Validation loss: 0.003381972433999181\n",
      "\n",
      "EPOCH 32 of 50\n",
      "\n",
      "Training loss: 0.004658759390521381\n",
      "Validation loss: 0.0033859876760592065\n",
      "\n",
      "EPOCH 33 of 50\n",
      "\n",
      "Training loss: 0.004961914253524608\n",
      "Validation loss: 0.0033836062066257\n",
      "\n",
      "EPOCH 34 of 50\n",
      "\n",
      "Training loss: 0.0045214696373376585\n",
      "Validation loss: 0.0033778490033000708\n",
      "\n",
      "EPOCH 35 of 50\n",
      "\n",
      "Training loss: 0.0045838439578397405\n",
      "Validation loss: 0.0033844984136521816\n",
      "\n",
      "EPOCH 36 of 50\n",
      "\n",
      "Training loss: 0.004591223576830493\n",
      "Validation loss: 0.0033770267230769\n",
      "\n",
      "EPOCH 37 of 50\n",
      "\n",
      "Training loss: 0.004399064333281583\n",
      "Validation loss: 0.0033725466734419265\n",
      "\n",
      "EPOCH 38 of 50\n",
      "\n",
      "Training loss: 0.0044266855499396724\n",
      "Validation loss: 0.003383385017514229\n",
      "\n",
      "EPOCH 39 of 50\n",
      "\n",
      "Training loss: 0.0045077391486201025\n",
      "Validation loss: 0.0033815594700475535\n",
      "\n",
      "EPOCH 40 of 50\n",
      "\n",
      "Training loss: 0.004603378759283159\n",
      "Validation loss: 0.003385102997223536\n",
      "\n",
      "EPOCH 41 of 50\n",
      "\n",
      "Training loss: 0.004591091484245326\n",
      "Validation loss: 0.003380761326601108\n",
      "\n",
      "EPOCH 42 of 50\n",
      "\n",
      "Training loss: 0.004370772913615737\n",
      "Validation loss: 0.0033732895584156117\n",
      "\n",
      "EPOCH 43 of 50\n",
      "\n",
      "Training loss: 0.004818426574476891\n",
      "Validation loss: 0.0033752204229434333\n",
      "\n",
      "EPOCH 44 of 50\n",
      "\n",
      "Training loss: 0.004659341363650229\n",
      "Validation loss: 0.003370612394064665\n",
      "\n",
      "EPOCH 45 of 50\n",
      "\n",
      "Training loss: 0.004622426556630267\n",
      "Validation loss: 0.0033832707752784095\n",
      "\n",
      "EPOCH 46 of 50\n",
      "\n",
      "Training loss: 0.00455247494392097\n",
      "Validation loss: 0.0033779976268609366\n",
      "\n",
      "EPOCH 47 of 50\n",
      "\n",
      "Training loss: 0.004481539880442951\n",
      "Validation loss: 0.0033766302901009717\n",
      "\n",
      "EPOCH 48 of 50\n",
      "\n",
      "Training loss: 0.004430149888826741\n",
      "Validation loss: 0.0033756031189113855\n",
      "\n",
      "EPOCH 49 of 50\n",
      "\n",
      "Training loss: 0.004571944087122877\n",
      "Validation loss: 0.0033746580593287945\n",
      "\n",
      "EPOCH 50 of 50\n",
      "\n",
      "Training loss: 0.0045793692115694284\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-30 03:47:08,554] Trial 4 finished with value: 0.0033866938514014087 and parameters: {'lr': 0.0022880778854138923, 'weight_decay': 1.0323338072000017e-05, 'hidden_size': 465, 'dropout_rate': 0.2185236150231209}. Best is trial 3 with value: 0.0033067727151016393.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.0033866938514014087\n",
      "learning rate: 2.787518059439578e-05\n",
      "weight_decay: 5.546858975399895e-05\n",
      "hidden_size: 398\n",
      "dropout_rate: 0.474798029710649\n",
      "\n",
      "EPOCH 1 of 50\n",
      "\n",
      "Training loss: 0.0057278650088442695\n",
      "Validation loss: 0.003318390420948466\n",
      "\n",
      "EPOCH 2 of 50\n",
      "\n",
      "Training loss: 0.005882452914698256\n",
      "Validation loss: 0.003302127898981174\n",
      "\n",
      "EPOCH 3 of 50\n",
      "\n",
      "Training loss: 0.0055749463434848524\n",
      "Validation loss: 0.003299469050640861\n",
      "\n",
      "EPOCH 4 of 50\n",
      "\n",
      "Training loss: 0.005254974123090506\n",
      "Validation loss: 0.003299469438691934\n",
      "\n",
      "EPOCH 5 of 50\n",
      "\n",
      "Training loss: 0.005413461424824264\n",
      "Validation loss: 0.003302334574982524\n",
      "\n",
      "EPOCH 6 of 50\n",
      "\n",
      "Training loss: 0.0054658651869330145\n",
      "Validation loss: 0.003304405060286323\n",
      "\n",
      "EPOCH 7 of 50\n",
      "\n",
      "Training loss: 0.005372162339174085\n",
      "Validation loss: 0.0033015143126249313\n",
      "\n",
      "EPOCH 8 of 50\n",
      "\n",
      "Training loss: 0.005594383821719223\n",
      "Validation loss: 0.003297138183067242\n",
      "\n",
      "EPOCH 9 of 50\n",
      "\n",
      "Training loss: 0.005496255380825864\n",
      "Validation loss: 0.0032971236699571214\n",
      "\n",
      "EPOCH 10 of 50\n",
      "\n",
      "Training loss: 0.005278891909660565\n",
      "Validation loss: 0.0033001122064888477\n",
      "\n",
      "EPOCH 11 of 50\n",
      "\n",
      "Training loss: 0.0057730758045282625\n",
      "Validation loss: 0.0032976112173249326\n",
      "\n",
      "EPOCH 12 of 50\n",
      "\n",
      "Training loss: 0.005658378410670493\n",
      "Validation loss: 0.0032998414244502783\n",
      "\n",
      "EPOCH 13 of 50\n",
      "\n",
      "Training loss: 0.005414250668966108\n",
      "Validation loss: 0.0032977769151329994\n",
      "\n",
      "EPOCH 14 of 50\n",
      "\n",
      "Training loss: 0.005717529449611902\n",
      "Validation loss: 0.003300007743140062\n",
      "\n",
      "EPOCH 15 of 50\n",
      "\n",
      "Training loss: 0.005401871477564176\n",
      "Validation loss: 0.0032998227203885713\n",
      "\n",
      "EPOCH 16 of 50\n",
      "\n",
      "Training loss: 0.005477417622589403\n",
      "Validation loss: 0.003298469508687655\n",
      "\n",
      "EPOCH 17 of 50\n",
      "\n",
      "Training loss: 0.005363118808923496\n",
      "Validation loss: 0.0032994995514551797\n",
      "\n",
      "EPOCH 18 of 50\n",
      "\n",
      "Training loss: 0.005770364465812842\n",
      "Validation loss: 0.0033014031747976937\n",
      "\n",
      "EPOCH 19 of 50\n",
      "\n",
      "Training loss: 0.005595066791607274\n",
      "Validation loss: 0.00330042225929598\n",
      "\n",
      "EPOCH 20 of 50\n",
      "\n",
      "Training loss: 0.005136159093429645\n",
      "Validation loss: 0.0033019802067428827\n",
      "\n",
      "EPOCH 21 of 50\n",
      "\n",
      "Training loss: 0.005573364440351725\n",
      "Validation loss: 0.0033002119356145463\n",
      "\n",
      "EPOCH 22 of 50\n",
      "\n",
      "Training loss: 0.00544984204073747\n",
      "Validation loss: 0.0033041584926346936\n",
      "\n",
      "EPOCH 23 of 50\n",
      "\n",
      "Training loss: 0.0055981602312790025\n",
      "Validation loss: 0.003298500319942832\n",
      "\n",
      "EPOCH 24 of 50\n",
      "\n",
      "Training loss: 0.0057303570210933685\n",
      "Validation loss: 0.0032990514300763607\n",
      "\n",
      "EPOCH 25 of 50\n",
      "\n",
      "Training loss: 0.00553224856654803\n",
      "Validation loss: 0.0032992683506260314\n",
      "\n",
      "EPOCH 26 of 50\n",
      "\n",
      "Training loss: 0.005616696499702003\n",
      "Validation loss: 0.0032991240732371807\n",
      "\n",
      "EPOCH 27 of 50\n",
      "\n",
      "Training loss: 0.00566425836748547\n",
      "Validation loss: 0.003299816744402051\n",
      "\n",
      "EPOCH 28 of 50\n",
      "\n",
      "Training loss: 0.0054886747772494955\n",
      "Validation loss: 0.003299857722595334\n",
      "\n",
      "EPOCH 29 of 50\n",
      "\n",
      "Training loss: 0.005553217397795783\n",
      "Validation loss: 0.0033000499630967775\n",
      "\n",
      "EPOCH 30 of 50\n",
      "\n",
      "Training loss: 0.005419499991047714\n",
      "Validation loss: 0.0033010540840526423\n",
      "\n",
      "EPOCH 31 of 50\n",
      "\n",
      "Training loss: 0.00522148319416576\n",
      "Validation loss: 0.003297738265246153\n",
      "\n",
      "EPOCH 32 of 50\n",
      "\n",
      "Training loss: 0.005564977777087026\n",
      "Validation loss: 0.0032991679230084023\n",
      "\n",
      "EPOCH 33 of 50\n",
      "\n",
      "Training loss: 0.005436759726661775\n",
      "Validation loss: 0.003297935239970684\n",
      "\n",
      "EPOCH 34 of 50\n",
      "\n",
      "Training loss: 0.005450350387642781\n",
      "Validation loss: 0.003300898397962252\n",
      "\n",
      "EPOCH 35 of 50\n",
      "\n",
      "Training loss: 0.005359125996215476\n",
      "Validation loss: 0.0033007130647699037\n",
      "\n",
      "EPOCH 36 of 50\n",
      "\n",
      "Training loss: 0.005691546377622419\n",
      "Validation loss: 0.003300401382148266\n",
      "\n",
      "EPOCH 37 of 50\n",
      "\n",
      "Training loss: 0.005510042214559184\n",
      "Validation loss: 0.0032989829778671265\n",
      "\n",
      "EPOCH 38 of 50\n",
      "\n",
      "Training loss: 0.005588137648171849\n",
      "Validation loss: 0.0032994553136328855\n",
      "\n",
      "EPOCH 39 of 50\n",
      "\n",
      "Training loss: 0.005416393461119797\n",
      "Validation loss: 0.0033016803208738565\n",
      "\n",
      "EPOCH 40 of 50\n",
      "\n",
      "Training loss: 0.005344548107435306\n",
      "Validation loss: 0.003302808307732145\n",
      "\n",
      "EPOCH 41 of 50\n",
      "\n",
      "Training loss: 0.0054487309729059534\n",
      "Validation loss: 0.003298430625970165\n",
      "\n",
      "EPOCH 42 of 50\n",
      "\n",
      "Training loss: 0.005658892035070393\n",
      "Validation loss: 0.003299156359086434\n",
      "\n",
      "EPOCH 43 of 50\n",
      "\n",
      "Training loss: 0.005822907357166211\n",
      "Validation loss: 0.003299700484300653\n",
      "\n",
      "EPOCH 44 of 50\n",
      "\n",
      "Training loss: 0.005395986760656039\n",
      "Validation loss: 0.003299321901674072\n",
      "\n",
      "EPOCH 45 of 50\n",
      "\n",
      "Training loss: 0.005607328118963374\n",
      "Validation loss: 0.003297631861642003\n",
      "\n",
      "EPOCH 46 of 50\n",
      "\n",
      "Training loss: 0.005344650811619229\n",
      "Validation loss: 0.0032988463062793016\n",
      "\n",
      "EPOCH 47 of 50\n",
      "\n",
      "Training loss: 0.0054899051578508485\n",
      "Validation loss: 0.0032998831011354923\n",
      "\n",
      "EPOCH 48 of 50\n",
      "\n",
      "Training loss: 0.005418634068013893\n",
      "Validation loss: 0.0032991869375109673\n",
      "\n",
      "EPOCH 49 of 50\n",
      "\n",
      "Training loss: 0.00537077777294649\n",
      "Validation loss: 0.0032991572127987943\n",
      "\n",
      "EPOCH 50 of 50\n",
      "\n",
      "Training loss: 0.005614133965637948\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-30 03:51:10,428] Trial 5 finished with value: 0.0032981353191037974 and parameters: {'lr': 2.787518059439578e-05, 'weight_decay': 5.546858975399895e-05, 'hidden_size': 398, 'dropout_rate': 0.474798029710649}. Best is trial 5 with value: 0.0032981353191037974.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.0032981353191037974\n",
      "learning rate: 0.0007629166850695844\n",
      "weight_decay: 2.3734470361076514e-05\n",
      "hidden_size: 318\n",
      "dropout_rate: 0.46783732526467453\n",
      "\n",
      "EPOCH 1 of 50\n",
      "\n",
      "Training loss: 0.005536204669624567\n",
      "Validation loss: 0.0033249282278120518\n",
      "\n",
      "EPOCH 2 of 50\n",
      "\n",
      "Training loss: 0.005190735786325402\n",
      "Validation loss: 0.0033338761422783136\n",
      "\n",
      "EPOCH 3 of 50\n",
      "\n",
      "Training loss: 0.005182699972970618\n",
      "Validation loss: 0.0033378476121773324\n",
      "\n",
      "EPOCH 4 of 50\n",
      "\n",
      "Training loss: 0.005220217630267143\n",
      "Validation loss: 0.0033416129493465028\n",
      "\n",
      "EPOCH 5 of 50\n",
      "\n",
      "Training loss: 0.005343627443330156\n",
      "Validation loss: 0.003338285411397616\n",
      "\n",
      "EPOCH 6 of 50\n",
      "\n",
      "Training loss: 0.005028791389324599\n",
      "Validation loss: 0.003350793461625775\n",
      "\n",
      "EPOCH 7 of 50\n",
      "\n",
      "Training loss: 0.005077555413461394\n",
      "Validation loss: 0.0033519255618254342\n",
      "\n",
      "EPOCH 8 of 50\n",
      "\n",
      "Training loss: 0.0050034621316525675\n",
      "Validation loss: 0.0033571019303053617\n",
      "\n",
      "EPOCH 9 of 50\n",
      "\n",
      "Training loss: 0.005133557366207242\n",
      "Validation loss: 0.003356410733734568\n",
      "\n",
      "EPOCH 10 of 50\n",
      "\n",
      "Training loss: 0.005111481063067913\n",
      "Validation loss: 0.0033544551891585193\n",
      "\n",
      "EPOCH 11 of 50\n",
      "\n",
      "Training loss: 0.00509084699054559\n",
      "Validation loss: 0.003358822393541535\n",
      "\n",
      "EPOCH 12 of 50\n",
      "\n",
      "Training loss: 0.005213145218375657\n",
      "Validation loss: 0.0033564836097260318\n",
      "\n",
      "EPOCH 13 of 50\n",
      "\n",
      "Training loss: 0.005127722604407204\n",
      "Validation loss: 0.0033571594394743443\n",
      "\n",
      "EPOCH 14 of 50\n",
      "\n",
      "Training loss: 0.005165929647369517\n",
      "Validation loss: 0.0033553031583627066\n",
      "\n",
      "EPOCH 15 of 50\n",
      "\n",
      "Training loss: 0.005189830463172661\n",
      "Validation loss: 0.003350629083191355\n",
      "\n",
      "EPOCH 16 of 50\n",
      "\n",
      "Training loss: 0.005075118840775556\n",
      "Validation loss: 0.003350220232581099\n",
      "\n",
      "EPOCH 17 of 50\n",
      "\n",
      "Training loss: 0.005304244191696246\n",
      "Validation loss: 0.003348855689788858\n",
      "\n",
      "EPOCH 18 of 50\n",
      "\n",
      "Training loss: 0.005035837517223424\n",
      "Validation loss: 0.0033537615090608597\n",
      "\n",
      "EPOCH 19 of 50\n",
      "\n",
      "Training loss: 0.005185409811221891\n",
      "Validation loss: 0.0033496687343964973\n",
      "\n",
      "EPOCH 20 of 50\n",
      "\n",
      "Training loss: 0.005231895897951391\n",
      "Validation loss: 0.003349596867337823\n",
      "\n",
      "EPOCH 21 of 50\n",
      "\n",
      "Training loss: 0.004948013824307256\n",
      "Validation loss: 0.0033503472028921046\n",
      "\n",
      "EPOCH 22 of 50\n",
      "\n",
      "Training loss: 0.0050582239249100285\n",
      "Validation loss: 0.0033598963636904955\n",
      "\n",
      "EPOCH 23 of 50\n",
      "\n",
      "Training loss: 0.005321201506174273\n",
      "Validation loss: 0.0033580997648338475\n",
      "\n",
      "EPOCH 24 of 50\n",
      "\n",
      "Training loss: 0.005247891207949983\n",
      "Validation loss: 0.0033591320582975945\n",
      "\n",
      "EPOCH 25 of 50\n",
      "\n",
      "Training loss: 0.004901216029086047\n",
      "Validation loss: 0.0033701679203659296\n",
      "\n",
      "EPOCH 26 of 50\n",
      "\n",
      "Training loss: 0.005348314324186908\n",
      "Validation loss: 0.0033626644096026817\n",
      "\n",
      "EPOCH 27 of 50\n",
      "\n",
      "Training loss: 0.00526984232581324\n",
      "Validation loss: 0.003357820368061463\n",
      "\n",
      "EPOCH 28 of 50\n",
      "\n",
      "Training loss: 0.004888422502618697\n",
      "Validation loss: 0.0033538898763557277\n",
      "\n",
      "EPOCH 29 of 50\n",
      "\n",
      "Training loss: 0.004998931609508064\n",
      "Validation loss: 0.003356594747553269\n",
      "\n",
      "EPOCH 30 of 50\n",
      "\n",
      "Training loss: 0.004980922417922152\n",
      "Validation loss: 0.0033530816435813904\n",
      "\n",
      "EPOCH 31 of 50\n",
      "\n",
      "Training loss: 0.004937190148565505\n",
      "Validation loss: 0.0033565200865268707\n",
      "\n",
      "EPOCH 32 of 50\n",
      "\n",
      "Training loss: 0.004945183793703715\n",
      "Validation loss: 0.003356414381414652\n",
      "\n",
      "EPOCH 33 of 50\n",
      "\n",
      "Training loss: 0.005131674025000798\n",
      "Validation loss: 0.003369211917743087\n",
      "\n",
      "EPOCH 34 of 50\n",
      "\n",
      "Training loss: 0.005063863781591256\n",
      "Validation loss: 0.0033614200074225664\n",
      "\n",
      "EPOCH 35 of 50\n",
      "\n",
      "Training loss: 0.004992053430113528\n",
      "Validation loss: 0.0033585811033844948\n",
      "\n",
      "EPOCH 36 of 50\n",
      "\n",
      "Training loss: 0.005561038127375973\n",
      "Validation loss: 0.003356464517613252\n",
      "\n",
      "EPOCH 37 of 50\n",
      "\n",
      "Training loss: 0.005052658211853769\n",
      "Validation loss: 0.0033570552865664163\n",
      "\n",
      "EPOCH 38 of 50\n",
      "\n",
      "Training loss: 0.0050033393781632185\n",
      "Validation loss: 0.003355615849917134\n",
      "\n",
      "EPOCH 39 of 50\n",
      "\n",
      "Training loss: 0.005059705995437171\n",
      "Validation loss: 0.0033490390051156282\n",
      "\n",
      "EPOCH 40 of 50\n",
      "\n",
      "Training loss: 0.004814915488370591\n",
      "Validation loss: 0.0033411477537204823\n",
      "\n",
      "EPOCH 41 of 50\n",
      "\n",
      "Training loss: 0.005166494313420521\n",
      "Validation loss: 0.0033546244570364556\n",
      "\n",
      "EPOCH 42 of 50\n",
      "\n",
      "Training loss: 0.0049904196833570795\n",
      "Validation loss: 0.0033587641858806214\n",
      "\n",
      "EPOCH 43 of 50\n",
      "\n",
      "Training loss: 0.005242919601086114\n",
      "Validation loss: 0.0033534846734255552\n",
      "\n",
      "EPOCH 44 of 50\n",
      "\n",
      "Training loss: 0.005108445624096526\n",
      "Validation loss: 0.003352835929642121\n",
      "\n",
      "EPOCH 45 of 50\n",
      "\n",
      "Training loss: 0.005337629725949632\n",
      "Validation loss: 0.00334995798766613\n",
      "\n",
      "EPOCH 46 of 50\n",
      "\n",
      "Training loss: 0.00544460987051328\n",
      "Validation loss: 0.003357298206537962\n",
      "\n",
      "EPOCH 47 of 50\n",
      "\n",
      "Training loss: 0.005156931053433154\n",
      "Validation loss: 0.0033515674682954946\n",
      "\n",
      "EPOCH 48 of 50\n",
      "\n",
      "Training loss: 0.005099320577250587\n",
      "Validation loss: 0.0033620515993485847\n",
      "\n",
      "EPOCH 49 of 50\n",
      "\n",
      "Training loss: 0.005024941224190924\n",
      "Validation loss: 0.0033531186636537313\n",
      "\n",
      "EPOCH 50 of 50\n",
      "\n",
      "Training loss: 0.0051122484469993245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-30 03:55:19,032] Trial 6 finished with value: 0.003363614591459433 and parameters: {'lr': 0.0007629166850695844, 'weight_decay': 2.3734470361076514e-05, 'hidden_size': 318, 'dropout_rate': 0.46783732526467453}. Best is trial 5 with value: 0.0032981353191037974.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.003363614591459433\n",
      "learning rate: 0.09748696392182407\n",
      "weight_decay: 7.288508107271174e-06\n",
      "hidden_size: 454\n",
      "dropout_rate: 0.27626506725054967\n",
      "\n",
      "EPOCH 1 of 50\n",
      "\n",
      "Training loss: 0.014984890145974027\n",
      "Validation loss: 387.8972676595052\n",
      "\n",
      "EPOCH 2 of 50\n",
      "\n",
      "Training loss: 0.006084169364637799\n",
      "Validation loss: 0.013097700042029222\n",
      "\n",
      "EPOCH 3 of 50\n",
      "\n",
      "Training loss: 0.004822883315177428\n",
      "Validation loss: 0.004334220740323265\n",
      "\n",
      "EPOCH 4 of 50\n",
      "\n",
      "Training loss: 0.004916952457278967\n",
      "Validation loss: 0.0036481439601629972\n",
      "\n",
      "EPOCH 5 of 50\n",
      "\n",
      "Training loss: 0.004924793882916371\n",
      "Validation loss: 0.0035591342796881995\n",
      "\n",
      "EPOCH 6 of 50\n",
      "\n",
      "Training loss: 0.004827767688160141\n",
      "Validation loss: 0.0035155452011773982\n",
      "\n",
      "EPOCH 7 of 50\n",
      "\n",
      "Training loss: 0.004788658116012812\n",
      "Validation loss: 0.0035195013818641505\n",
      "\n",
      "EPOCH 8 of 50\n",
      "\n",
      "Training loss: 0.004911058685845799\n",
      "Validation loss: 0.0035020480087647834\n",
      "\n",
      "EPOCH 9 of 50\n",
      "\n",
      "Training loss: 0.004933505526019467\n",
      "Validation loss: 0.0035179012144605317\n",
      "\n",
      "EPOCH 10 of 50\n",
      "\n",
      "Training loss: 0.004866377398785617\n",
      "Validation loss: 0.0034951492367933192\n",
      "\n",
      "EPOCH 11 of 50\n",
      "\n",
      "Training loss: 0.0047120403550151325\n",
      "Validation loss: 0.0034932948959370456\n",
      "\n",
      "EPOCH 12 of 50\n",
      "\n",
      "Training loss: 0.004676215764549043\n",
      "Validation loss: 0.0035001921157042184\n",
      "\n",
      "EPOCH 13 of 50\n",
      "\n",
      "Training loss: 0.0048190199045671355\n",
      "Validation loss: 0.0034965763334184885\n",
      "\n",
      "EPOCH 14 of 50\n",
      "\n",
      "Training loss: 0.0048692001598990625\n",
      "Validation loss: 0.0034971998538821936\n",
      "\n",
      "EPOCH 15 of 50\n",
      "\n",
      "Training loss: 0.004529650271352794\n",
      "Validation loss: 0.003500361073141297\n",
      "\n",
      "EPOCH 16 of 50\n",
      "\n",
      "Training loss: 0.0047831123746517636\n",
      "Validation loss: 0.0034806727586934962\n",
      "\n",
      "EPOCH 17 of 50\n",
      "\n",
      "Training loss: 0.004534236698721846\n",
      "Validation loss: 0.003495776327326894\n",
      "\n",
      "EPOCH 18 of 50\n",
      "\n",
      "Training loss: 0.004697849508374929\n",
      "Validation loss: 0.0034880594660838446\n",
      "\n",
      "EPOCH 19 of 50\n",
      "\n",
      "Training loss: 0.004900326590157217\n",
      "Validation loss: 0.0035053682513535023\n",
      "\n",
      "EPOCH 20 of 50\n",
      "\n",
      "Training loss: 0.0047893535035351915\n",
      "Validation loss: 0.0034891406539827585\n",
      "\n",
      "EPOCH 21 of 50\n",
      "\n",
      "Training loss: 0.004788176855072379\n",
      "Validation loss: 0.0035116230913748345\n",
      "\n",
      "EPOCH 22 of 50\n",
      "\n",
      "Training loss: 0.004812713220922483\n",
      "Validation loss: 0.0034823674553384385\n",
      "\n",
      "EPOCH 23 of 50\n",
      "\n",
      "Training loss: 0.004820707719773054\n",
      "Validation loss: 0.003512421933313211\n",
      "\n",
      "EPOCH 24 of 50\n",
      "\n",
      "Training loss: 0.00482756985972325\n",
      "Validation loss: 0.0034960022506614528\n",
      "\n",
      "EPOCH 25 of 50\n",
      "\n",
      "Training loss: 0.00471075882928239\n",
      "Validation loss: 0.0034875195318212113\n",
      "\n",
      "EPOCH 26 of 50\n",
      "\n",
      "Training loss: 0.004882421965400378\n",
      "Validation loss: 0.0035219641091922918\n",
      "\n",
      "EPOCH 27 of 50\n",
      "\n",
      "Training loss: 0.005111626685700483\n",
      "Validation loss: 0.003501980099827051\n",
      "\n",
      "EPOCH 28 of 50\n",
      "\n",
      "Training loss: 0.004877051467903786\n",
      "Validation loss: 0.0035139562096446753\n",
      "\n",
      "EPOCH 29 of 50\n",
      "\n",
      "Training loss: 0.004857020918279886\n",
      "Validation loss: 0.0035200809749464193\n",
      "\n",
      "EPOCH 30 of 50\n",
      "\n",
      "Training loss: 0.004779021747203337\n",
      "Validation loss: 0.0035109086893498898\n",
      "\n",
      "EPOCH 31 of 50\n",
      "\n",
      "Training loss: 0.004768839390534494\n",
      "Validation loss: 0.003513258028154572\n",
      "\n",
      "EPOCH 32 of 50\n",
      "\n",
      "Training loss: 0.0047597513120207526\n",
      "Validation loss: 0.003507327909270922\n",
      "\n",
      "EPOCH 33 of 50\n",
      "\n",
      "Training loss: 0.005139986855081386\n",
      "Validation loss: 0.0035001627014329038\n",
      "\n",
      "EPOCH 34 of 50\n",
      "\n",
      "Training loss: 0.004851385847561889\n",
      "Validation loss: 0.00350143457762897\n",
      "\n",
      "EPOCH 35 of 50\n",
      "\n",
      "Training loss: 0.004828213688193096\n",
      "Validation loss: 0.003484129517649611\n",
      "\n",
      "EPOCH 36 of 50\n",
      "\n",
      "Training loss: 0.004713789145979617\n",
      "Validation loss: 0.0035096217567722\n",
      "\n",
      "EPOCH 37 of 50\n",
      "\n",
      "Training loss: 0.0048360921080327695\n",
      "Validation loss: 0.003496327359850208\n",
      "\n",
      "EPOCH 38 of 50\n",
      "\n",
      "Training loss: 0.00462878645501203\n",
      "Validation loss: 0.0034883702949931226\n",
      "\n",
      "EPOCH 39 of 50\n",
      "\n",
      "Training loss: 0.004741199262854125\n",
      "Validation loss: 0.0034826857348283133\n",
      "\n",
      "EPOCH 40 of 50\n",
      "\n",
      "Training loss: 0.005040246424161726\n",
      "Validation loss: 0.0035020601159582534\n",
      "\n",
      "EPOCH 41 of 50\n",
      "\n",
      "Training loss: 0.004763101563892431\n",
      "Validation loss: 0.0035005977066854634\n",
      "\n",
      "EPOCH 42 of 50\n",
      "\n",
      "Training loss: 0.0049911110868884456\n",
      "Validation loss: 0.0034777598921209574\n",
      "\n",
      "EPOCH 43 of 50\n",
      "\n",
      "Training loss: 0.0048907777656697566\n",
      "Validation loss: 0.003531422931700945\n",
      "\n",
      "EPOCH 44 of 50\n",
      "\n",
      "Training loss: 0.0047628241073754095\n",
      "Validation loss: 0.0035252391826361418\n",
      "\n",
      "EPOCH 45 of 50\n",
      "\n",
      "Training loss: 0.004868478617734379\n",
      "Validation loss: 0.0034957618142167726\n",
      "\n",
      "EPOCH 46 of 50\n",
      "\n",
      "Training loss: 0.004839697956211037\n",
      "Validation loss: 0.0034955345715085664\n",
      "\n",
      "EPOCH 47 of 50\n",
      "\n",
      "Training loss: 0.004840174017267095\n",
      "Validation loss: 0.003514811551819245\n",
      "\n",
      "EPOCH 48 of 50\n",
      "\n",
      "Training loss: 0.00463662456928028\n",
      "Validation loss: 0.003510102474441131\n",
      "\n",
      "EPOCH 49 of 50\n",
      "\n",
      "Training loss: 0.004783259834059411\n",
      "Validation loss: 0.0035284508485347033\n",
      "\n",
      "EPOCH 50 of 50\n",
      "\n",
      "Training loss: 0.0047187644005235695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-30 03:59:55,105] Trial 7 finished with value: 0.0035004129943748317 and parameters: {'lr': 0.09748696392182407, 'weight_decay': 7.288508107271174e-06, 'hidden_size': 454, 'dropout_rate': 0.27626506725054967}. Best is trial 5 with value: 0.0032981353191037974.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.0035004129943748317\n",
      "learning rate: 0.0035908331969881774\n",
      "weight_decay: 0.0002180606244769963\n",
      "hidden_size: 497\n",
      "dropout_rate: 0.3821672900075471\n",
      "\n",
      "EPOCH 1 of 50\n",
      "\n",
      "Training loss: 0.005104255060561829\n",
      "Validation loss: 0.0039002279906223216\n",
      "\n",
      "EPOCH 2 of 50\n",
      "\n",
      "Training loss: 0.004784360683212678\n",
      "Validation loss: 0.003289161017164588\n",
      "\n",
      "EPOCH 3 of 50\n",
      "\n",
      "Training loss: 0.004501953307125304\n",
      "Validation loss: 0.003295629130055507\n",
      "\n",
      "EPOCH 4 of 50\n",
      "\n",
      "Training loss: 0.004676319374185469\n",
      "Validation loss: 0.0033026894088834524\n",
      "\n",
      "EPOCH 5 of 50\n",
      "\n",
      "Training loss: 0.004544020268238253\n",
      "Validation loss: 0.0033027383033186197\n",
      "\n",
      "EPOCH 6 of 50\n",
      "\n",
      "Training loss: 0.004833937001725038\n",
      "Validation loss: 0.0033045675760755935\n",
      "\n",
      "EPOCH 7 of 50\n",
      "\n",
      "Training loss: 0.0044938809393594665\n",
      "Validation loss: 0.003306714662661155\n",
      "\n",
      "EPOCH 8 of 50\n",
      "\n",
      "Training loss: 0.004646019958373573\n",
      "Validation loss: 0.0033122890163213015\n",
      "\n",
      "EPOCH 9 of 50\n",
      "\n",
      "Training loss: 0.004817293542954657\n",
      "Validation loss: 0.0033055408857762814\n",
      "\n",
      "EPOCH 10 of 50\n",
      "\n",
      "Training loss: 0.004931136707050933\n",
      "Validation loss: 0.0033060582354664803\n",
      "\n",
      "EPOCH 11 of 50\n",
      "\n",
      "Training loss: 0.004702975274994969\n",
      "Validation loss: 0.0033045046341915927\n",
      "\n",
      "EPOCH 12 of 50\n",
      "\n",
      "Training loss: 0.004761811966697375\n",
      "Validation loss: 0.0033075433845321336\n",
      "\n",
      "EPOCH 13 of 50\n",
      "\n",
      "Training loss: 0.004619074002322223\n",
      "Validation loss: 0.003312440433849891\n",
      "\n",
      "EPOCH 14 of 50\n",
      "\n",
      "Training loss: 0.004686710554071599\n",
      "Validation loss: 0.003308602453519901\n",
      "\n",
      "EPOCH 15 of 50\n",
      "\n",
      "Training loss: 0.004662400033945839\n",
      "Validation loss: 0.0033063096149514117\n",
      "\n",
      "EPOCH 16 of 50\n",
      "\n",
      "Training loss: 0.004751021740958095\n",
      "Validation loss: 0.003306731659298142\n",
      "\n",
      "EPOCH 17 of 50\n",
      "\n",
      "Training loss: 0.004645375767722726\n",
      "Validation loss: 0.003301893671353658\n",
      "\n",
      "EPOCH 18 of 50\n",
      "\n",
      "Training loss: 0.004923756855229537\n",
      "Validation loss: 0.0033027358974019685\n",
      "\n",
      "EPOCH 19 of 50\n",
      "\n",
      "Training loss: 0.004584408960201674\n",
      "Validation loss: 0.0033086168114095926\n",
      "\n",
      "EPOCH 20 of 50\n",
      "\n",
      "Training loss: 0.00465839803736243\n",
      "Validation loss: 0.003310473868623376\n",
      "\n",
      "EPOCH 21 of 50\n",
      "\n",
      "Training loss: 0.004591681813407276\n",
      "Validation loss: 0.003309973437959949\n",
      "\n",
      "EPOCH 22 of 50\n",
      "\n",
      "Training loss: 0.004543394341857897\n",
      "Validation loss: 0.0033052483728776374\n",
      "\n",
      "EPOCH 23 of 50\n",
      "\n",
      "Training loss: 0.004619429586455226\n",
      "Validation loss: 0.0033104432901988425\n",
      "\n",
      "EPOCH 24 of 50\n",
      "\n",
      "Training loss: 0.0046484616792036425\n",
      "Validation loss: 0.003303173463791609\n",
      "\n",
      "EPOCH 25 of 50\n",
      "\n",
      "Training loss: 0.004785001769454943\n",
      "Validation loss: 0.0033052186481654644\n",
      "\n",
      "EPOCH 26 of 50\n",
      "\n",
      "Training loss: 0.004732479806989431\n",
      "Validation loss: 0.003309115767478943\n",
      "\n",
      "EPOCH 27 of 50\n",
      "\n",
      "Training loss: 0.0045856266127278405\n",
      "Validation loss: 0.0033018365502357483\n",
      "\n",
      "EPOCH 28 of 50\n",
      "\n",
      "Training loss: 0.0045899473285923404\n",
      "Validation loss: 0.0033057269950707755\n",
      "\n",
      "EPOCH 29 of 50\n",
      "\n",
      "Training loss: 0.004755148974557717\n",
      "Validation loss: 0.003304095473140478\n",
      "\n",
      "EPOCH 30 of 50\n",
      "\n",
      "Training loss: 0.004732692510717445\n",
      "Validation loss: 0.003308557361985246\n",
      "\n",
      "EPOCH 31 of 50\n",
      "\n",
      "Training loss: 0.004629805270168517\n",
      "Validation loss: 0.003307931978876392\n",
      "\n",
      "EPOCH 32 of 50\n",
      "\n",
      "Training loss: 0.004642339759609765\n",
      "Validation loss: 0.003302752428377668\n",
      "\n",
      "EPOCH 33 of 50\n",
      "\n",
      "Training loss: 0.004718607265709175\n",
      "Validation loss: 0.0033044316805899143\n",
      "\n",
      "EPOCH 34 of 50\n",
      "\n",
      "Training loss: 0.004937347749041187\n",
      "Validation loss: 0.0033083302744974694\n",
      "\n",
      "EPOCH 35 of 50\n",
      "\n",
      "Training loss: 0.004943027419762479\n",
      "Validation loss: 0.0033094848040491343\n",
      "\n",
      "EPOCH 36 of 50\n",
      "\n",
      "Training loss: 0.004785412560320563\n",
      "Validation loss: 0.0033059794610987106\n",
      "\n",
      "EPOCH 37 of 50\n",
      "\n",
      "Training loss: 0.004744357222484218\n",
      "Validation loss: 0.0033051707626630864\n",
      "\n",
      "EPOCH 38 of 50\n",
      "\n",
      "Training loss: 0.004543759058126145\n",
      "Validation loss: 0.003305199866493543\n",
      "\n",
      "EPOCH 39 of 50\n",
      "\n",
      "Training loss: 0.004656138674666484\n",
      "Validation loss: 0.0033078057070573172\n",
      "\n",
      "EPOCH 40 of 50\n",
      "\n",
      "Training loss: 0.004917754765806926\n",
      "Validation loss: 0.0033117692607144513\n",
      "\n",
      "EPOCH 41 of 50\n",
      "\n",
      "Training loss: 0.004753867112514045\n",
      "Validation loss: 0.0033052690171947083\n",
      "\n",
      "EPOCH 42 of 50\n",
      "\n",
      "Training loss: 0.0048719035565025276\n",
      "Validation loss: 0.0032997277254859605\n",
      "\n",
      "EPOCH 43 of 50\n",
      "\n",
      "Training loss: 0.004982390311650104\n",
      "Validation loss: 0.0033070932452877364\n",
      "\n",
      "EPOCH 44 of 50\n",
      "\n",
      "Training loss: 0.004642284889188077\n",
      "Validation loss: 0.0033018565736711025\n",
      "\n",
      "EPOCH 45 of 50\n",
      "\n",
      "Training loss: 0.004714390909713175\n",
      "Validation loss: 0.003309151933838924\n",
      "\n",
      "EPOCH 46 of 50\n",
      "\n",
      "Training loss: 0.004615399520844221\n",
      "Validation loss: 0.003309243358671665\n",
      "\n",
      "EPOCH 47 of 50\n",
      "\n",
      "Training loss: 0.004675877642714315\n",
      "Validation loss: 0.0033071257639676332\n",
      "\n",
      "EPOCH 48 of 50\n",
      "\n",
      "Training loss: 0.00448137669203182\n",
      "Validation loss: 0.0033016016241163015\n",
      "\n",
      "EPOCH 49 of 50\n",
      "\n",
      "Training loss: 0.004581744834366772\n",
      "Validation loss: 0.003304513404145837\n",
      "\n",
      "EPOCH 50 of 50\n",
      "\n",
      "Training loss: 0.004974985857390695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-30 04:04:21,021] Trial 8 finished with value: 0.0033086518912265697 and parameters: {'lr': 0.0035908331969881774, 'weight_decay': 0.0002180606244769963, 'hidden_size': 497, 'dropout_rate': 0.3821672900075471}. Best is trial 5 with value: 0.0032981353191037974.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.0033086518912265697\n",
      "learning rate: 0.00023469994349541314\n",
      "weight_decay: 2.9031816641785626e-06\n",
      "hidden_size: 343\n",
      "dropout_rate: 0.4604286877153034\n",
      "\n",
      "EPOCH 1 of 50\n",
      "\n",
      "Training loss: 0.005595229514357116\n",
      "Validation loss: 0.0032951663403461375\n",
      "\n",
      "EPOCH 2 of 50\n",
      "\n",
      "Training loss: 0.005417255244942175\n",
      "Validation loss: 0.003290620787690083\n",
      "\n",
      "EPOCH 3 of 50\n",
      "\n",
      "Training loss: 0.0054617643925464815\n",
      "Validation loss: 0.003293131555741032\n",
      "\n",
      "EPOCH 4 of 50\n",
      "\n",
      "Training loss: 0.005565553541398711\n",
      "Validation loss: 0.003296186371395985\n",
      "\n",
      "EPOCH 5 of 50\n",
      "\n",
      "Training loss: 0.005259780213236809\n",
      "Validation loss: 0.0033015915347884097\n",
      "\n",
      "EPOCH 6 of 50\n",
      "\n",
      "Training loss: 0.005531234821925561\n",
      "Validation loss: 0.0032940521681060395\n",
      "\n",
      "EPOCH 7 of 50\n",
      "\n",
      "Training loss: 0.005266092588297195\n",
      "Validation loss: 0.0032944990477214255\n",
      "\n",
      "EPOCH 8 of 50\n",
      "\n",
      "Training loss: 0.005572398167310489\n",
      "Validation loss: 0.003299832266444961\n",
      "\n",
      "EPOCH 9 of 50\n",
      "\n",
      "Training loss: 0.005285837118410402\n",
      "Validation loss: 0.003300356911495328\n",
      "\n",
      "EPOCH 10 of 50\n",
      "\n",
      "Training loss: 0.0052740387359841\n",
      "Validation loss: 0.003305291368936499\n",
      "\n",
      "EPOCH 11 of 50\n",
      "\n",
      "Training loss: 0.005484622178806199\n",
      "Validation loss: 0.003302966787790259\n",
      "\n",
      "EPOCH 12 of 50\n",
      "\n",
      "Training loss: 0.005755607038736343\n",
      "Validation loss: 0.0033023812963316837\n",
      "\n",
      "EPOCH 13 of 50\n",
      "\n",
      "Training loss: 0.005535333469096158\n",
      "Validation loss: 0.003297405239815513\n",
      "\n",
      "EPOCH 14 of 50\n",
      "\n",
      "Training loss: 0.005421556765213609\n",
      "Validation loss: 0.003310718418409427\n",
      "\n",
      "EPOCH 15 of 50\n",
      "\n",
      "Training loss: 0.005364982049084372\n",
      "Validation loss: 0.0033043265187491975\n",
      "\n",
      "EPOCH 16 of 50\n",
      "\n",
      "Training loss: 0.005442486092862155\n",
      "Validation loss: 0.0033033423436184726\n",
      "\n",
      "EPOCH 17 of 50\n",
      "\n",
      "Training loss: 0.005395322468959623\n",
      "Validation loss: 0.003304063342511654\n",
      "\n",
      "EPOCH 18 of 50\n",
      "\n",
      "Training loss: 0.0054350182310574586\n",
      "Validation loss: 0.0033028436979899802\n",
      "\n",
      "EPOCH 19 of 50\n",
      "\n",
      "Training loss: 0.005991585604432557\n",
      "Validation loss: 0.0032976807560771704\n",
      "\n",
      "EPOCH 20 of 50\n",
      "\n",
      "Training loss: 0.0053711820186840165\n",
      "Validation loss: 0.0033045505018283925\n",
      "\n",
      "EPOCH 21 of 50\n",
      "\n",
      "Training loss: 0.0053647047995279236\n",
      "Validation loss: 0.0033093305149426064\n",
      "\n",
      "EPOCH 22 of 50\n",
      "\n",
      "Training loss: 0.0052909131110128425\n",
      "Validation loss: 0.003304391478498777\n",
      "\n",
      "EPOCH 23 of 50\n",
      "\n",
      "Training loss: 0.005477869210557805\n",
      "Validation loss: 0.0033002247412999472\n",
      "\n",
      "EPOCH 24 of 50\n",
      "\n",
      "Training loss: 0.005156955060859521\n",
      "Validation loss: 0.003306521258006493\n",
      "\n",
      "EPOCH 25 of 50\n",
      "\n",
      "Training loss: 0.005484551398290528\n",
      "Validation loss: 0.0033040201912323632\n",
      "\n",
      "EPOCH 26 of 50\n",
      "\n",
      "Training loss: 0.005420350987050269\n",
      "Validation loss: 0.0033080073383947215\n",
      "\n",
      "EPOCH 27 of 50\n",
      "\n",
      "Training loss: 0.005270414623535342\n",
      "Validation loss: 0.0033062093425542116\n",
      "\n",
      "EPOCH 28 of 50\n",
      "\n",
      "Training loss: 0.005460713110450242\n",
      "Validation loss: 0.0032989029617359242\n",
      "\n",
      "EPOCH 29 of 50\n",
      "\n",
      "Training loss: 0.005385342597340544\n",
      "Validation loss: 0.0033008998725563288\n",
      "\n",
      "EPOCH 30 of 50\n",
      "\n",
      "Training loss: 0.005502421512371964\n",
      "Validation loss: 0.0033019138500094414\n",
      "\n",
      "EPOCH 31 of 50\n",
      "\n",
      "Training loss: 0.005631620685259501\n",
      "Validation loss: 0.0033028367906808853\n",
      "\n",
      "EPOCH 32 of 50\n",
      "\n",
      "Training loss: 0.0053278721558551\n",
      "Validation loss: 0.00330534391105175\n",
      "\n",
      "EPOCH 33 of 50\n",
      "\n",
      "Training loss: 0.0058441731250948375\n",
      "Validation loss: 0.003301506939654549\n",
      "\n",
      "EPOCH 34 of 50\n",
      "\n",
      "Training loss: 0.005377741789238321\n",
      "Validation loss: 0.0032997316836069026\n",
      "\n",
      "EPOCH 35 of 50\n",
      "\n",
      "Training loss: 0.005432056625270181\n",
      "Validation loss: 0.0032975987220803895\n",
      "\n",
      "EPOCH 36 of 50\n",
      "\n",
      "Training loss: 0.005594614712107513\n",
      "Validation loss: 0.003293671722834309\n",
      "\n",
      "EPOCH 37 of 50\n",
      "\n",
      "Training loss: 0.005372684241996871\n",
      "Validation loss: 0.0032968943317731223\n",
      "\n",
      "EPOCH 38 of 50\n",
      "\n",
      "Training loss: 0.005303597129467461\n",
      "Validation loss: 0.003302783782904347\n",
      "\n",
      "EPOCH 39 of 50\n",
      "\n",
      "Training loss: 0.005462625426136785\n",
      "Validation loss: 0.003303072415292263\n",
      "\n",
      "EPOCH 40 of 50\n",
      "\n",
      "Training loss: 0.005525232447932164\n",
      "Validation loss: 0.0032951516720155873\n",
      "\n",
      "EPOCH 41 of 50\n",
      "\n",
      "Training loss: 0.005444105559339126\n",
      "Validation loss: 0.0033051309486230216\n",
      "\n",
      "EPOCH 42 of 50\n",
      "\n",
      "Training loss: 0.0054819887607461875\n",
      "Validation loss: 0.003313191933557391\n",
      "\n",
      "EPOCH 43 of 50\n",
      "\n",
      "Training loss: 0.005372752073324389\n",
      "Validation loss: 0.003302124561741948\n",
      "\n",
      "EPOCH 44 of 50\n",
      "\n",
      "Training loss: 0.005283427502339085\n",
      "Validation loss: 0.0033050718096395335\n",
      "\n",
      "EPOCH 45 of 50\n",
      "\n",
      "Training loss: 0.005651085947950681\n",
      "Validation loss: 0.0032948129810392857\n",
      "\n",
      "EPOCH 46 of 50\n",
      "\n",
      "Training loss: 0.005425735531995694\n",
      "Validation loss: 0.003297356112549702\n",
      "\n",
      "EPOCH 47 of 50\n",
      "\n",
      "Training loss: 0.005474984024961789\n",
      "Validation loss: 0.003298216499388218\n",
      "\n",
      "EPOCH 48 of 50\n",
      "\n",
      "Training loss: 0.005410644639697339\n",
      "Validation loss: 0.0033008980875213942\n",
      "\n",
      "EPOCH 49 of 50\n",
      "\n",
      "Training loss: 0.0055121200987034375\n",
      "Validation loss: 0.003311124940713247\n",
      "\n",
      "EPOCH 50 of 50\n",
      "\n",
      "Training loss: 0.005422898206031985\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-30 04:08:29,047] Trial 9 finished with value: 0.003310044373696049 and parameters: {'lr': 0.00023469994349541314, 'weight_decay': 2.9031816641785626e-06, 'hidden_size': 343, 'dropout_rate': 0.4604286877153034}. Best is trial 5 with value: 0.0032981353191037974.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.003310044373696049\n",
      "learning rate: 1.7091303807737332e-05\n",
      "weight_decay: 9.134816346506885e-05\n",
      "hidden_size: 591\n",
      "dropout_rate: 0.39823792229494537\n",
      "\n",
      "EPOCH 1 of 50\n",
      "\n",
      "Training loss: 0.005356781081193023\n",
      "Validation loss: 0.0033244455698877573\n",
      "\n",
      "EPOCH 2 of 50\n",
      "\n",
      "Training loss: 0.005462041642102931\n",
      "Validation loss: 0.003366156325985988\n",
      "\n",
      "EPOCH 3 of 50\n",
      "\n",
      "Training loss: 0.005294091094078289\n",
      "Validation loss: 0.0034227353365470967\n",
      "\n",
      "EPOCH 4 of 50\n",
      "\n",
      "Training loss: 0.005470528060363399\n",
      "Validation loss: 0.0034234674337009587\n",
      "\n",
      "EPOCH 5 of 50\n",
      "\n",
      "Training loss: 0.005257511356224616\n",
      "Validation loss: 0.003409316452840964\n",
      "\n",
      "EPOCH 6 of 50\n",
      "\n",
      "Training loss: 0.005446094295216931\n",
      "Validation loss: 0.003443483030423522\n",
      "\n",
      "EPOCH 7 of 50\n",
      "\n",
      "Training loss: 0.005439537784291638\n",
      "Validation loss: 0.0034562787817170224\n",
      "\n",
      "EPOCH 8 of 50\n",
      "\n",
      "Training loss: 0.005818645159403483\n",
      "Validation loss: 0.0034180758520960808\n",
      "\n",
      "EPOCH 9 of 50\n",
      "\n",
      "Training loss: 0.005241655071990358\n",
      "Validation loss: 0.0034067388623952866\n",
      "\n",
      "EPOCH 10 of 50\n",
      "\n",
      "Training loss: 0.005483569318635596\n",
      "Validation loss: 0.0034011253155767918\n",
      "\n",
      "EPOCH 11 of 50\n",
      "\n",
      "Training loss: 0.005391223614828454\n",
      "Validation loss: 0.0034010253536204496\n",
      "\n",
      "EPOCH 12 of 50\n",
      "\n",
      "Training loss: 0.005262291446949045\n",
      "Validation loss: 0.0034388345666229725\n",
      "\n",
      "EPOCH 13 of 50\n",
      "\n",
      "Training loss: 0.00552023093526562\n",
      "Validation loss: 0.0034320988537122807\n",
      "\n",
      "EPOCH 14 of 50\n",
      "\n",
      "Training loss: 0.005263900824098123\n",
      "Validation loss: 0.0034468850741783776\n",
      "\n",
      "EPOCH 15 of 50\n",
      "\n",
      "Training loss: 0.00541696425837775\n",
      "Validation loss: 0.003465756851558884\n",
      "\n",
      "EPOCH 16 of 50\n",
      "\n",
      "Training loss: 0.005332902798222171\n",
      "Validation loss: 0.0034719272516667843\n",
      "\n",
      "EPOCH 17 of 50\n",
      "\n",
      "Training loss: 0.005248330378284057\n",
      "Validation loss: 0.0034492396904776492\n",
      "\n",
      "EPOCH 18 of 50\n",
      "\n",
      "Training loss: 0.005091115806458725\n",
      "Validation loss: 0.0034311733519037566\n",
      "\n",
      "EPOCH 19 of 50\n",
      "\n",
      "Training loss: 0.005312130010376374\n",
      "Validation loss: 0.0034209750592708588\n",
      "\n",
      "EPOCH 20 of 50\n",
      "\n",
      "Training loss: 0.0054550825411246884\n",
      "Validation loss: 0.003434297007819017\n",
      "\n",
      "EPOCH 21 of 50\n",
      "\n",
      "Training loss: 0.005594280755354298\n",
      "Validation loss: 0.0034443827656408152\n",
      "\n",
      "EPOCH 22 of 50\n",
      "\n",
      "Training loss: 0.00557345115683145\n",
      "Validation loss: 0.0034364720340818167\n",
      "\n",
      "EPOCH 23 of 50\n",
      "\n",
      "Training loss: 0.005440006860428386\n",
      "Validation loss: 0.0034564898815006018\n",
      "\n",
      "EPOCH 24 of 50\n",
      "\n",
      "Training loss: 0.005449013060165776\n",
      "Validation loss: 0.0034505085398753486\n",
      "\n",
      "EPOCH 25 of 50\n",
      "\n",
      "Training loss: 0.005385510002573331\n",
      "Validation loss: 0.0034776806520919004\n",
      "\n",
      "EPOCH 26 of 50\n",
      "\n",
      "Training loss: 0.0052859071228239275\n",
      "Validation loss: 0.0034625333889077106\n",
      "\n",
      "EPOCH 27 of 50\n",
      "\n",
      "Training loss: 0.005259084618753857\n",
      "Validation loss: 0.003422330366447568\n",
      "\n",
      "EPOCH 28 of 50\n",
      "\n",
      "Training loss: 0.005431362479511235\n",
      "Validation loss: 0.0034229835340132317\n",
      "\n",
      "EPOCH 29 of 50\n",
      "\n",
      "Training loss: 0.005586307858013445\n",
      "Validation loss: 0.003413825063034892\n",
      "\n",
      "EPOCH 30 of 50\n",
      "\n",
      "Training loss: 0.0056833320090340245\n",
      "Validation loss: 0.003442882870634397\n",
      "\n",
      "EPOCH 31 of 50\n",
      "\n",
      "Training loss: 0.00526923678504924\n",
      "Validation loss: 0.0034357989206910133\n",
      "\n",
      "EPOCH 32 of 50\n",
      "\n",
      "Training loss: 0.005373967863205407\n",
      "Validation loss: 0.0034253249565760293\n",
      "\n",
      "EPOCH 33 of 50\n",
      "\n",
      "Training loss: 0.00525251149924265\n",
      "Validation loss: 0.0034273879136890173\n",
      "\n",
      "EPOCH 34 of 50\n",
      "\n",
      "Training loss: 0.0054316539834770895\n",
      "Validation loss: 0.003422454232349992\n",
      "\n",
      "EPOCH 35 of 50\n",
      "\n",
      "Training loss: 0.005558624553183715\n",
      "Validation loss: 0.0034331083297729492\n",
      "\n",
      "EPOCH 36 of 50\n",
      "\n",
      "Training loss: 0.005219855526876118\n",
      "Validation loss: 0.003424725184837977\n",
      "\n",
      "EPOCH 37 of 50\n",
      "\n",
      "Training loss: 0.005415856631265746\n",
      "Validation loss: 0.003438384893039862\n",
      "\n",
      "EPOCH 38 of 50\n",
      "\n",
      "Training loss: 0.00520740408036444\n",
      "Validation loss: 0.003443200762073199\n",
      "\n",
      "EPOCH 39 of 50\n",
      "\n",
      "Training loss: 0.005285478533349103\n",
      "Validation loss: 0.00343753257766366\n",
      "\n",
      "EPOCH 40 of 50\n",
      "\n",
      "Training loss: 0.005387201439589262\n",
      "Validation loss: 0.003448713648443421\n",
      "\n",
      "EPOCH 41 of 50\n",
      "\n",
      "Training loss: 0.00538884951836533\n",
      "Validation loss: 0.0034423061491300664\n",
      "\n",
      "EPOCH 42 of 50\n",
      "\n",
      "Training loss: 0.005368667991004056\n",
      "Validation loss: 0.0034500551410019398\n",
      "\n",
      "EPOCH 43 of 50\n",
      "\n",
      "Training loss: 0.005467187716729111\n",
      "Validation loss: 0.0034451904551436505\n",
      "\n",
      "EPOCH 44 of 50\n",
      "\n",
      "Training loss: 0.005753065355949932\n",
      "Validation loss: 0.0034372093311200538\n",
      "\n",
      "EPOCH 45 of 50\n",
      "\n",
      "Training loss: 0.005372282169345353\n",
      "Validation loss: 0.003454383462667465\n",
      "\n",
      "EPOCH 46 of 50\n",
      "\n",
      "Training loss: 0.005336705905695756\n",
      "Validation loss: 0.0034610613559683165\n",
      "\n",
      "EPOCH 47 of 50\n",
      "\n",
      "Training loss: 0.0055083005378643675\n",
      "Validation loss: 0.003466533807416757\n",
      "\n",
      "EPOCH 48 of 50\n",
      "\n",
      "Training loss: 0.005386760510090325\n",
      "Validation loss: 0.0034425645135343075\n",
      "\n",
      "EPOCH 49 of 50\n",
      "\n",
      "Training loss: 0.005398669331851933\n",
      "Validation loss: 0.003434199606999755\n",
      "\n",
      "EPOCH 50 of 50\n",
      "\n",
      "Training loss: 0.006034946948703792\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-30 04:12:29,693] Trial 10 finished with value: 0.0034444170693556466 and parameters: {'lr': 1.7091303807737332e-05, 'weight_decay': 9.134816346506885e-05, 'hidden_size': 591, 'dropout_rate': 0.39823792229494537}. Best is trial 5 with value: 0.0032981353191037974.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.0034444170693556466\n",
      "learning rate: 1.706364564226606e-05\n",
      "weight_decay: 0.0009144715477078534\n",
      "hidden_size: 372\n",
      "dropout_rate: 0.49522730201767634\n",
      "\n",
      "EPOCH 1 of 50\n",
      "\n",
      "Training loss: 0.0055402279314067625\n",
      "Validation loss: 0.0033077027183026075\n",
      "\n",
      "EPOCH 2 of 50\n",
      "\n",
      "Training loss: 0.005498913737634818\n",
      "Validation loss: 0.0033872516360133886\n",
      "\n",
      "EPOCH 3 of 50\n",
      "\n",
      "Training loss: 0.00583728470115198\n",
      "Validation loss: 0.0034557179703066745\n",
      "\n",
      "EPOCH 4 of 50\n",
      "\n",
      "Training loss: 0.005404422421836191\n",
      "Validation loss: 0.00348918450375398\n",
      "\n",
      "EPOCH 5 of 50\n",
      "\n",
      "Training loss: 0.0054419981315732\n",
      "Validation loss: 0.0034939005660514035\n",
      "\n",
      "EPOCH 6 of 50\n",
      "\n",
      "Training loss: 0.005642027645889256\n",
      "Validation loss: 0.003501307607317964\n",
      "\n",
      "EPOCH 7 of 50\n",
      "\n",
      "Training loss: 0.005625720756749312\n",
      "Validation loss: 0.0034742788411676884\n",
      "\n",
      "EPOCH 8 of 50\n",
      "\n",
      "Training loss: 0.0053130405075434185\n",
      "Validation loss: 0.0034942072040090957\n",
      "\n",
      "EPOCH 9 of 50\n",
      "\n",
      "Training loss: 0.005258075142693188\n",
      "Validation loss: 0.00351464981213212\n",
      "\n",
      "EPOCH 10 of 50\n",
      "\n",
      "Training loss: 0.005458212561077542\n",
      "Validation loss: 0.00350581055196623\n",
      "\n",
      "EPOCH 11 of 50\n",
      "\n",
      "Training loss: 0.005370127968490124\n",
      "Validation loss: 0.0035096437204629183\n",
      "\n",
      "EPOCH 12 of 50\n",
      "\n",
      "Training loss: 0.005276731913909316\n",
      "Validation loss: 0.003514006889114777\n",
      "\n",
      "EPOCH 13 of 50\n",
      "\n",
      "Training loss: 0.005485520905090703\n",
      "Validation loss: 0.0035153063169370093\n",
      "\n",
      "EPOCH 14 of 50\n",
      "\n",
      "Training loss: 0.005327337395606769\n",
      "Validation loss: 0.0035236289259046316\n",
      "\n",
      "EPOCH 15 of 50\n",
      "\n",
      "Training loss: 0.00532431248575449\n",
      "Validation loss: 0.003511940206711491\n",
      "\n",
      "EPOCH 16 of 50\n",
      "\n",
      "Training loss: 0.005376918965743648\n",
      "Validation loss: 0.0035177997779101133\n",
      "\n",
      "EPOCH 17 of 50\n",
      "\n",
      "Training loss: 0.005602752324193716\n",
      "Validation loss: 0.003545206350584825\n",
      "\n",
      "EPOCH 18 of 50\n",
      "\n",
      "Training loss: 0.005314980581816699\n",
      "Validation loss: 0.003543365669126312\n",
      "\n",
      "EPOCH 19 of 50\n",
      "\n",
      "Training loss: 0.005306913258714808\n",
      "Validation loss: 0.0035093451539675393\n",
      "\n",
      "EPOCH 20 of 50\n",
      "\n",
      "Training loss: 0.005568501022126939\n",
      "Validation loss: 0.003520676555732886\n",
      "\n",
      "EPOCH 21 of 50\n",
      "\n",
      "Training loss: 0.0057628049204746885\n",
      "Validation loss: 0.0034881092918415866\n",
      "\n",
      "EPOCH 22 of 50\n",
      "\n",
      "Training loss: 0.005425315867695544\n",
      "Validation loss: 0.0034917755207667747\n",
      "\n",
      "EPOCH 23 of 50\n",
      "\n",
      "Training loss: 0.005553799836585919\n",
      "Validation loss: 0.0035098219911257424\n",
      "\n",
      "EPOCH 24 of 50\n",
      "\n",
      "Training loss: 0.0053524514142837785\n",
      "Validation loss: 0.0035214899107813835\n",
      "\n",
      "EPOCH 25 of 50\n",
      "\n",
      "Training loss: 0.005604654084891081\n",
      "Validation loss: 0.003513641112173597\n",
      "\n",
      "EPOCH 26 of 50\n",
      "\n",
      "Training loss: 0.005349753683225976\n",
      "Validation loss: 0.0035207907979687056\n",
      "\n",
      "EPOCH 27 of 50\n",
      "\n",
      "Training loss: 0.005477303949495156\n",
      "Validation loss: 0.0034952565717200437\n",
      "\n",
      "EPOCH 28 of 50\n",
      "\n",
      "Training loss: 0.005503643459330003\n",
      "Validation loss: 0.0034882106507817903\n",
      "\n",
      "EPOCH 29 of 50\n",
      "\n",
      "Training loss: 0.005481943177680175\n",
      "Validation loss: 0.003509088962649306\n",
      "\n",
      "EPOCH 30 of 50\n",
      "\n",
      "Training loss: 0.00542987236339185\n",
      "Validation loss: 0.003497159186129769\n",
      "\n",
      "EPOCH 31 of 50\n",
      "\n",
      "Training loss: 0.005576746952202585\n",
      "Validation loss: 0.003508727609490355\n",
      "\n",
      "EPOCH 32 of 50\n",
      "\n",
      "Training loss: 0.005456795709000694\n",
      "Validation loss: 0.003480365267023444\n",
      "\n",
      "EPOCH 33 of 50\n",
      "\n",
      "Training loss: 0.0052342139598396085\n",
      "Validation loss: 0.0035017962412287793\n",
      "\n",
      "EPOCH 34 of 50\n",
      "\n",
      "Training loss: 0.005431760102510452\n",
      "Validation loss: 0.0035069844840715327\n",
      "\n",
      "EPOCH 35 of 50\n",
      "\n",
      "Training loss: 0.005384021800839239\n",
      "Validation loss: 0.003497451931859056\n",
      "\n",
      "EPOCH 36 of 50\n",
      "\n",
      "Training loss: 0.0057138134725391865\n",
      "Validation loss: 0.003494044921050469\n",
      "\n",
      "EPOCH 37 of 50\n",
      "\n",
      "Training loss: 0.0053832855644739335\n",
      "Validation loss: 0.0035092985102285943\n",
      "\n",
      "EPOCH 38 of 50\n",
      "\n",
      "Training loss: 0.005340226874169376\n",
      "Validation loss: 0.0034877791379888854\n",
      "\n",
      "EPOCH 39 of 50\n",
      "\n",
      "Training loss: 0.005268152906662888\n",
      "Validation loss: 0.0034772753715515137\n",
      "\n",
      "EPOCH 40 of 50\n",
      "\n",
      "Training loss: 0.005265769652194447\n",
      "Validation loss: 0.003506585101907452\n",
      "\n",
      "EPOCH 41 of 50\n",
      "\n",
      "Training loss: 0.005918033938441012\n",
      "Validation loss: 0.003506899500886599\n",
      "\n",
      "EPOCH 42 of 50\n",
      "\n",
      "Training loss: 0.005559744934240977\n",
      "Validation loss: 0.0035119401291012764\n",
      "\n",
      "EPOCH 43 of 50\n",
      "\n",
      "Training loss: 0.005406498184634579\n",
      "Validation loss: 0.0034823773118356862\n",
      "\n",
      "EPOCH 44 of 50\n",
      "\n",
      "Training loss: 0.005592776307215293\n",
      "Validation loss: 0.0034837295146038136\n",
      "\n",
      "EPOCH 45 of 50\n",
      "\n",
      "Training loss: 0.005577107477519248\n",
      "Validation loss: 0.0035015849862247705\n",
      "\n",
      "EPOCH 46 of 50\n",
      "\n",
      "Training loss: 0.005611422782142957\n",
      "Validation loss: 0.003499835884819428\n",
      "\n",
      "EPOCH 47 of 50\n",
      "\n",
      "Training loss: 0.0052509680907759405\n",
      "Validation loss: 0.003500854829326272\n",
      "\n",
      "EPOCH 48 of 50\n",
      "\n",
      "Training loss: 0.005437172768223617\n",
      "Validation loss: 0.00349104474298656\n",
      "\n",
      "EPOCH 49 of 50\n",
      "\n",
      "Training loss: 0.005465659183553523\n",
      "Validation loss: 0.003499391178290049\n",
      "\n",
      "EPOCH 50 of 50\n",
      "\n",
      "Training loss: 0.005369424561245574\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-30 04:16:33,113] Trial 11 finished with value: 0.0034725617927809558 and parameters: {'lr': 1.706364564226606e-05, 'weight_decay': 0.0009144715477078534, 'hidden_size': 372, 'dropout_rate': 0.49522730201767634}. Best is trial 5 with value: 0.0032981353191037974.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.0034725617927809558\n",
      "learning rate: 1.8192614533662074e-05\n",
      "weight_decay: 1.759728244790968e-06\n",
      "hidden_size: 390\n",
      "dropout_rate: 0.49515536969068685\n",
      "\n",
      "EPOCH 1 of 50\n",
      "\n",
      "Training loss: 0.006048133338077201\n",
      "Validation loss: 0.0033593851452072463\n",
      "\n",
      "EPOCH 2 of 50\n",
      "\n",
      "Training loss: 0.005638282125194867\n",
      "Validation loss: 0.0034128868331511817\n",
      "\n",
      "EPOCH 3 of 50\n",
      "\n",
      "Training loss: 0.005464217987739378\n",
      "Validation loss: 0.0034737703390419483\n",
      "\n",
      "EPOCH 4 of 50\n",
      "\n",
      "Training loss: 0.005508344465245803\n",
      "Validation loss: 0.0034956523838142553\n",
      "\n",
      "EPOCH 5 of 50\n",
      "\n",
      "Training loss: 0.0057395327732794816\n",
      "Validation loss: 0.0034845919193079076\n",
      "\n",
      "EPOCH 6 of 50\n",
      "\n",
      "Training loss: 0.005457985370109479\n",
      "Validation loss: 0.0035066616255789995\n",
      "\n",
      "EPOCH 7 of 50\n",
      "\n",
      "Training loss: 0.005692333940209614\n",
      "Validation loss: 0.003487211807320515\n",
      "\n",
      "EPOCH 8 of 50\n",
      "\n",
      "Training loss: 0.005214194198035532\n",
      "Validation loss: 0.00347457950313886\n",
      "\n",
      "EPOCH 9 of 50\n",
      "\n",
      "Training loss: 0.005428787864123781\n",
      "Validation loss: 0.0034728942749400935\n",
      "\n",
      "EPOCH 10 of 50\n",
      "\n",
      "Training loss: 0.005664820679359966\n",
      "Validation loss: 0.0034780523274093866\n",
      "\n",
      "EPOCH 11 of 50\n",
      "\n",
      "Training loss: 0.0053870028091801535\n",
      "Validation loss: 0.003492446228240927\n",
      "\n",
      "EPOCH 12 of 50\n",
      "\n",
      "Training loss: 0.0055226588414775\n",
      "Validation loss: 0.003526632053156694\n",
      "\n",
      "EPOCH 13 of 50\n",
      "\n",
      "Training loss: 0.005282200976378388\n",
      "Validation loss: 0.0034994499292224646\n",
      "\n",
      "EPOCH 14 of 50\n",
      "\n",
      "Training loss: 0.00555489460627238\n",
      "Validation loss: 0.0035009782295674086\n",
      "\n",
      "EPOCH 15 of 50\n",
      "\n",
      "Training loss: 0.005340796455533968\n",
      "Validation loss: 0.003496277378872037\n",
      "\n",
      "EPOCH 16 of 50\n",
      "\n",
      "Training loss: 0.00550943866579069\n",
      "Validation loss: 0.003515308490023017\n",
      "\n",
      "EPOCH 17 of 50\n",
      "\n",
      "Training loss: 0.005345322502156098\n",
      "Validation loss: 0.003490286568800608\n",
      "\n",
      "EPOCH 18 of 50\n",
      "\n",
      "Training loss: 0.00530292730157574\n",
      "Validation loss: 0.003479497507214546\n",
      "\n",
      "EPOCH 19 of 50\n",
      "\n",
      "Training loss: 0.005608446921946274\n",
      "Validation loss: 0.0035004032930980125\n",
      "\n",
      "EPOCH 20 of 50\n",
      "\n",
      "Training loss: 0.0055762952090137536\n",
      "Validation loss: 0.00351074174977839\n",
      "\n",
      "EPOCH 21 of 50\n",
      "\n",
      "Training loss: 0.005400099041354325\n",
      "Validation loss: 0.003512747585773468\n",
      "\n",
      "EPOCH 22 of 50\n",
      "\n",
      "Training loss: 0.005546613389419185\n",
      "Validation loss: 0.00351231227008005\n",
      "\n",
      "EPOCH 23 of 50\n",
      "\n",
      "Training loss: 0.005681001012110048\n",
      "Validation loss: 0.0034838124799231687\n",
      "\n",
      "EPOCH 24 of 50\n",
      "\n",
      "Training loss: 0.0055592255666852\n",
      "Validation loss: 0.0034973310151447854\n",
      "\n",
      "EPOCH 25 of 50\n",
      "\n",
      "Training loss: 0.005403229475228323\n",
      "Validation loss: 0.0035068188638736806\n",
      "\n",
      "EPOCH 26 of 50\n",
      "\n",
      "Training loss: 0.005533026945259836\n",
      "Validation loss: 0.0034953815241654715\n",
      "\n",
      "EPOCH 27 of 50\n",
      "\n",
      "Training loss: 0.00574630747238795\n",
      "Validation loss: 0.0034919004732122025\n",
      "\n",
      "EPOCH 28 of 50\n",
      "\n",
      "Training loss: 0.005298151505283183\n",
      "Validation loss: 0.0034964438527822495\n",
      "\n",
      "EPOCH 29 of 50\n",
      "\n",
      "Training loss: 0.0056330399173829294\n",
      "Validation loss: 0.0035130211617797613\n",
      "\n",
      "EPOCH 30 of 50\n",
      "\n",
      "Training loss: 0.005331430972243349\n",
      "Validation loss: 0.0035098561396201453\n",
      "\n",
      "EPOCH 31 of 50\n",
      "\n",
      "Training loss: 0.005431270071615775\n",
      "Validation loss: 0.0035035513962308564\n",
      "\n",
      "EPOCH 32 of 50\n",
      "\n",
      "Training loss: 0.005432366134805812\n",
      "Validation loss: 0.003529266299058994\n",
      "\n",
      "EPOCH 33 of 50\n",
      "\n",
      "Training loss: 0.005282769393589761\n",
      "Validation loss: 0.003521480675165852\n",
      "\n",
      "EPOCH 34 of 50\n",
      "\n",
      "Training loss: 0.00544110646781822\n",
      "Validation loss: 0.0035135597766687474\n",
      "\n",
      "EPOCH 35 of 50\n",
      "\n",
      "Training loss: 0.00556479213345382\n",
      "Validation loss: 0.0035214859526604414\n",
      "\n",
      "EPOCH 36 of 50\n",
      "\n",
      "Training loss: 0.00555109279230237\n",
      "Validation loss: 0.003523342621823152\n",
      "\n",
      "EPOCH 37 of 50\n",
      "\n",
      "Training loss: 0.005951526160869334\n",
      "Validation loss: 0.0035037553558746972\n",
      "\n",
      "EPOCH 38 of 50\n",
      "\n",
      "Training loss: 0.005497393819193046\n",
      "Validation loss: 0.0035328276765843234\n",
      "\n",
      "EPOCH 39 of 50\n",
      "\n",
      "Training loss: 0.005959330644044612\n",
      "Validation loss: 0.0035317229727904\n",
      "\n",
      "EPOCH 40 of 50\n",
      "\n",
      "Training loss: 0.0056539372437530095\n",
      "Validation loss: 0.0035360793117433786\n",
      "\n",
      "EPOCH 41 of 50\n",
      "\n",
      "Training loss: 0.005379862307260434\n",
      "Validation loss: 0.003527045715600252\n",
      "\n",
      "EPOCH 42 of 50\n",
      "\n",
      "Training loss: 0.005391730487139689\n",
      "Validation loss: 0.0034888555916647115\n",
      "\n",
      "EPOCH 43 of 50\n",
      "\n",
      "Training loss: 0.005172188730082578\n",
      "Validation loss: 0.0035042997915297747\n",
      "\n",
      "EPOCH 44 of 50\n",
      "\n",
      "Training loss: 0.005702117406245735\n",
      "Validation loss: 0.003481104193876187\n",
      "\n",
      "EPOCH 45 of 50\n",
      "\n",
      "Training loss: 0.005834489491664701\n",
      "Validation loss: 0.003507905174046755\n",
      "\n",
      "EPOCH 46 of 50\n",
      "\n",
      "Training loss: 0.005637994967401028\n",
      "Validation loss: 0.0034930116962641478\n",
      "\n",
      "EPOCH 47 of 50\n",
      "\n",
      "Training loss: 0.005499180509812302\n",
      "Validation loss: 0.0035003780697782836\n",
      "\n",
      "EPOCH 48 of 50\n",
      "\n",
      "Training loss: 0.005336463813566499\n",
      "Validation loss: 0.0035051810555160046\n",
      "\n",
      "EPOCH 49 of 50\n",
      "\n",
      "Training loss: 0.005287060307131873\n",
      "Validation loss: 0.00350382326481243\n",
      "\n",
      "EPOCH 50 of 50\n",
      "\n",
      "Training loss: 0.005547582430558072\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-30 04:20:39,539] Trial 12 finished with value: 0.0034577654829869666 and parameters: {'lr': 1.8192614533662074e-05, 'weight_decay': 1.759728244790968e-06, 'hidden_size': 390, 'dropout_rate': 0.49515536969068685}. Best is trial 5 with value: 0.0032981353191037974.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.0034577654829869666\n",
      "learning rate: 0.00025424430741752393\n",
      "weight_decay: 4.853224953102217e-05\n",
      "hidden_size: 377\n",
      "dropout_rate: 0.42662248217273047\n",
      "\n",
      "EPOCH 1 of 50\n",
      "\n",
      "Training loss: 0.0055402217743297415\n",
      "Validation loss: 0.003293933036426703\n",
      "\n",
      "EPOCH 2 of 50\n",
      "\n",
      "Training loss: 0.0055021922000580365\n",
      "Validation loss: 0.0032931966707110405\n",
      "\n",
      "EPOCH 3 of 50\n",
      "\n",
      "Training loss: 0.005167002194664544\n",
      "Validation loss: 0.00329068877423803\n",
      "\n",
      "EPOCH 4 of 50\n",
      "\n",
      "Training loss: 0.005350592028763559\n",
      "Validation loss: 0.003290529983739058\n",
      "\n",
      "EPOCH 5 of 50\n",
      "\n",
      "Training loss: 0.005411066787524356\n",
      "Validation loss: 0.003291745592529575\n",
      "\n",
      "EPOCH 6 of 50\n",
      "\n",
      "Training loss: 0.005024947200177444\n",
      "Validation loss: 0.0032902877622594437\n",
      "\n",
      "EPOCH 7 of 50\n",
      "\n",
      "Training loss: 0.005200097647806008\n",
      "Validation loss: 0.003289782131711642\n",
      "\n",
      "EPOCH 8 of 50\n",
      "\n",
      "Training loss: 0.005315079095049037\n",
      "Validation loss: 0.0032895694797237716\n",
      "\n",
      "EPOCH 9 of 50\n",
      "\n",
      "Training loss: 0.005373656232323911\n",
      "Validation loss: 0.0032875719480216503\n",
      "\n",
      "EPOCH 10 of 50\n",
      "\n",
      "Training loss: 0.005593477800074551\n",
      "Validation loss: 0.0032888018370916447\n",
      "\n",
      "EPOCH 11 of 50\n",
      "\n",
      "Training loss: 0.005073393177655008\n",
      "Validation loss: 0.0032912925040970245\n",
      "\n",
      "EPOCH 12 of 50\n",
      "\n",
      "Training loss: 0.005415286403149366\n",
      "Validation loss: 0.003287691002090772\n",
      "\n",
      "EPOCH 13 of 50\n",
      "\n",
      "Training loss: 0.005066191932807366\n",
      "Validation loss: 0.003291184470678369\n",
      "\n",
      "EPOCH 14 of 50\n",
      "\n",
      "Training loss: 0.005372366143597497\n",
      "Validation loss: 0.0032874789709846177\n",
      "\n",
      "EPOCH 15 of 50\n",
      "\n",
      "Training loss: 0.005241047849671708\n",
      "Validation loss: 0.0032920225833853087\n",
      "\n",
      "EPOCH 16 of 50\n",
      "\n",
      "Training loss: 0.005253620963129733\n",
      "Validation loss: 0.0032894412676493325\n",
      "\n",
      "EPOCH 17 of 50\n",
      "\n",
      "Training loss: 0.005268703111343914\n",
      "Validation loss: 0.0032914215698838234\n",
      "\n",
      "EPOCH 18 of 50\n",
      "\n",
      "Training loss: 0.005389682301837537\n",
      "Validation loss: 0.0032899193465709686\n",
      "\n",
      "EPOCH 19 of 50\n",
      "\n",
      "Training loss: 0.005115270433533523\n",
      "Validation loss: 0.0032947401826580367\n",
      "\n",
      "EPOCH 20 of 50\n",
      "\n",
      "Training loss: 0.005430595276670324\n",
      "Validation loss: 0.0032927467642972865\n",
      "\n",
      "EPOCH 21 of 50\n",
      "\n",
      "Training loss: 0.005398074268466896\n",
      "Validation loss: 0.0032931408689667783\n",
      "\n",
      "EPOCH 22 of 50\n",
      "\n",
      "Training loss: 0.005683891061279509\n",
      "Validation loss: 0.00329003327836593\n",
      "\n",
      "EPOCH 23 of 50\n",
      "\n",
      "Training loss: 0.005453761304832167\n",
      "Validation loss: 0.0033010655703643956\n",
      "\n",
      "EPOCH 24 of 50\n",
      "\n",
      "Training loss: 0.005050650021682183\n",
      "Validation loss: 0.0032921666279435158\n",
      "\n",
      "EPOCH 25 of 50\n",
      "\n",
      "Training loss: 0.005344730776010288\n",
      "Validation loss: 0.0032949564047157764\n",
      "\n",
      "EPOCH 26 of 50\n",
      "\n",
      "Training loss: 0.005135972389123506\n",
      "Validation loss: 0.0032971332936237254\n",
      "\n",
      "EPOCH 27 of 50\n",
      "\n",
      "Training loss: 0.005554253804600901\n",
      "Validation loss: 0.0032960436462114253\n",
      "\n",
      "EPOCH 28 of 50\n",
      "\n",
      "Training loss: 0.005388689279142354\n",
      "Validation loss: 0.0032949148832509914\n",
      "\n",
      "EPOCH 29 of 50\n",
      "\n",
      "Training loss: 0.005347203773756822\n",
      "Validation loss: 0.0032912578899413347\n",
      "\n",
      "EPOCH 30 of 50\n",
      "\n",
      "Training loss: 0.005486507589618365\n",
      "Validation loss: 0.0032925556879490614\n",
      "\n",
      "EPOCH 31 of 50\n",
      "\n",
      "Training loss: 0.005152135776976745\n",
      "Validation loss: 0.003288242810716232\n",
      "\n",
      "EPOCH 32 of 50\n",
      "\n",
      "Training loss: 0.0052921763466050225\n",
      "Validation loss: 0.0032928171567618847\n",
      "\n",
      "EPOCH 33 of 50\n",
      "\n",
      "Training loss: 0.005273555922839377\n",
      "Validation loss: 0.003293722247083982\n",
      "\n",
      "EPOCH 34 of 50\n",
      "\n",
      "Training loss: 0.005284549771911568\n",
      "Validation loss: 0.0032943462332089743\n",
      "\n",
      "EPOCH 35 of 50\n",
      "\n",
      "Training loss: 0.005334995376567046\n",
      "Validation loss: 0.00329222297295928\n",
      "\n",
      "EPOCH 36 of 50\n",
      "\n",
      "Training loss: 0.005374929764204555\n",
      "Validation loss: 0.003296844350794951\n",
      "\n",
      "EPOCH 37 of 50\n",
      "\n",
      "Training loss: 0.005442733979887432\n",
      "Validation loss: 0.0032893608634670577\n",
      "\n",
      "EPOCH 38 of 50\n",
      "\n",
      "Training loss: 0.005466570819003714\n",
      "Validation loss: 0.003294063421587149\n",
      "\n",
      "EPOCH 39 of 50\n",
      "\n",
      "Training loss: 0.005343674733820889\n",
      "Validation loss: 0.0032885713347544274\n",
      "\n",
      "EPOCH 40 of 50\n",
      "\n",
      "Training loss: 0.0055466262727148\n",
      "Validation loss: 0.0032924444725116095\n",
      "\n",
      "EPOCH 41 of 50\n",
      "\n",
      "Training loss: 0.005261291491074694\n",
      "Validation loss: 0.0032877766837676368\n",
      "\n",
      "EPOCH 42 of 50\n",
      "\n",
      "Training loss: 0.005284648233403762\n",
      "Validation loss: 0.0032894217098752656\n",
      "\n",
      "EPOCH 43 of 50\n",
      "\n",
      "Training loss: 0.0056024920712742544\n",
      "Validation loss: 0.003291137982159853\n",
      "\n",
      "EPOCH 44 of 50\n",
      "\n",
      "Training loss: 0.005315517489280965\n",
      "Validation loss: 0.0032897640485316515\n",
      "\n",
      "EPOCH 45 of 50\n",
      "\n",
      "Training loss: 0.005232427993582355\n",
      "Validation loss: 0.003287011757493019\n",
      "\n",
      "EPOCH 46 of 50\n",
      "\n",
      "Training loss: 0.005336871706984109\n",
      "Validation loss: 0.0032920135806004205\n",
      "\n",
      "EPOCH 47 of 50\n",
      "\n",
      "Training loss: 0.005276595604502493\n",
      "Validation loss: 0.003297059331089258\n",
      "\n",
      "EPOCH 48 of 50\n",
      "\n",
      "Training loss: 0.005280002537700865\n",
      "Validation loss: 0.0032881128136068583\n",
      "\n",
      "EPOCH 49 of 50\n",
      "\n",
      "Training loss: 0.0053542345607032376\n",
      "Validation loss: 0.003286056530972322\n",
      "\n",
      "EPOCH 50 of 50\n",
      "\n",
      "Training loss: 0.005313857096350855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-30 04:24:45,608] Trial 13 finished with value: 0.0032907865631083646 and parameters: {'lr': 0.00025424430741752393, 'weight_decay': 4.853224953102217e-05, 'hidden_size': 377, 'dropout_rate': 0.42662248217273047}. Best is trial 13 with value: 0.0032907865631083646.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.0032907865631083646\n",
      "learning rate: 0.0003066065234111946\n",
      "weight_decay: 5.3158130272964435e-05\n",
      "hidden_size: 405\n",
      "dropout_rate: 0.42686234242989957\n",
      "\n",
      "EPOCH 1 of 50\n",
      "\n",
      "Training loss: 0.005343497446220782\n",
      "Validation loss: 0.003307102403293053\n",
      "\n",
      "EPOCH 2 of 50\n",
      "\n",
      "Training loss: 0.005666362406272028\n",
      "Validation loss: 0.003298420226201415\n",
      "\n",
      "EPOCH 3 of 50\n",
      "\n",
      "Training loss: 0.0051312069925997\n",
      "Validation loss: 0.003292203259964784\n",
      "\n",
      "EPOCH 4 of 50\n",
      "\n",
      "Training loss: 0.005400143253306548\n",
      "Validation loss: 0.003293769977365931\n",
      "\n",
      "EPOCH 5 of 50\n",
      "\n",
      "Training loss: 0.005452647908694214\n",
      "Validation loss: 0.003291984321549535\n",
      "\n",
      "EPOCH 6 of 50\n",
      "\n",
      "Training loss: 0.005217817896563146\n",
      "Validation loss: 0.003301219160978993\n",
      "\n",
      "EPOCH 7 of 50\n",
      "\n",
      "Training loss: 0.005228483376817571\n",
      "Validation loss: 0.0032980370645721755\n",
      "\n",
      "EPOCH 8 of 50\n",
      "\n",
      "Training loss: 0.005129210935491655\n",
      "Validation loss: 0.003302569423491756\n",
      "\n",
      "EPOCH 9 of 50\n",
      "\n",
      "Training loss: 0.005200448497715924\n",
      "Validation loss: 0.003298449795693159\n",
      "\n",
      "EPOCH 10 of 50\n",
      "\n",
      "Training loss: 0.005076613872208529\n",
      "Validation loss: 0.003299403004348278\n",
      "\n",
      "EPOCH 11 of 50\n",
      "\n",
      "Training loss: 0.005063556212310989\n",
      "Validation loss: 0.003297055528188745\n",
      "\n",
      "EPOCH 12 of 50\n",
      "\n",
      "Training loss: 0.004962250202273329\n",
      "Validation loss: 0.003306252338613073\n",
      "\n",
      "EPOCH 13 of 50\n",
      "\n",
      "Training loss: 0.005203994249718057\n",
      "Validation loss: 0.0033061581198126078\n",
      "\n",
      "EPOCH 14 of 50\n",
      "\n",
      "Training loss: 0.005165632245027357\n",
      "Validation loss: 0.0033003437953690686\n",
      "\n",
      "EPOCH 15 of 50\n",
      "\n",
      "Training loss: 0.0052843422163277864\n",
      "Validation loss: 0.0032949776699145636\n",
      "\n",
      "EPOCH 16 of 50\n",
      "\n",
      "Training loss: 0.005239106145583921\n",
      "Validation loss: 0.0032998061118026576\n",
      "\n",
      "EPOCH 17 of 50\n",
      "\n",
      "Training loss: 0.005433753391520845\n",
      "Validation loss: 0.003308364422991872\n",
      "\n",
      "EPOCH 18 of 50\n",
      "\n",
      "Training loss: 0.005607929986177219\n",
      "Validation loss: 0.0033019527327269316\n",
      "\n",
      "EPOCH 19 of 50\n",
      "\n",
      "Training loss: 0.005208655467463864\n",
      "Validation loss: 0.003307825264831384\n",
      "\n",
      "EPOCH 20 of 50\n",
      "\n",
      "Training loss: 0.005385163705796003\n",
      "Validation loss: 0.0033021772590776286\n",
      "\n",
      "EPOCH 21 of 50\n",
      "\n",
      "Training loss: 0.005254268128838804\n",
      "Validation loss: 0.0033025736920535564\n",
      "\n",
      "EPOCH 22 of 50\n",
      "\n",
      "Training loss: 0.005367340571764443\n",
      "Validation loss: 0.0033049300933877626\n",
      "\n",
      "EPOCH 23 of 50\n",
      "\n",
      "Training loss: 0.005243722556365861\n",
      "Validation loss: 0.0033108980860561132\n",
      "\n",
      "EPOCH 24 of 50\n",
      "\n",
      "Training loss: 0.00528826533506314\n",
      "Validation loss: 0.0033076894469559193\n",
      "\n",
      "EPOCH 25 of 50\n",
      "\n",
      "Training loss: 0.005428624701582723\n",
      "Validation loss: 0.00330190461439391\n",
      "\n",
      "EPOCH 26 of 50\n",
      "\n",
      "Training loss: 0.005279698978281683\n",
      "Validation loss: 0.003303004972015818\n",
      "\n",
      "EPOCH 27 of 50\n",
      "\n",
      "Training loss: 0.004993900941270921\n",
      "Validation loss: 0.0033017636742442846\n",
      "\n",
      "EPOCH 28 of 50\n",
      "\n",
      "Training loss: 0.005118782580312755\n",
      "Validation loss: 0.003306589787825942\n",
      "\n",
      "EPOCH 29 of 50\n",
      "\n",
      "Training loss: 0.00530597779692875\n",
      "Validation loss: 0.0033075485844165087\n",
      "\n",
      "EPOCH 30 of 50\n",
      "\n",
      "Training loss: 0.005562470139314731\n",
      "Validation loss: 0.003315168200060725\n",
      "\n",
      "EPOCH 31 of 50\n",
      "\n",
      "Training loss: 0.00510969768381781\n",
      "Validation loss: 0.0033087721870591245\n",
      "\n",
      "EPOCH 32 of 50\n",
      "\n",
      "Training loss: 0.005177081278007891\n",
      "Validation loss: 0.0033037712952742973\n",
      "\n",
      "EPOCH 33 of 50\n",
      "\n",
      "Training loss: 0.005293207191344764\n",
      "Validation loss: 0.003307987625400225\n",
      "\n",
      "EPOCH 34 of 50\n",
      "\n",
      "Training loss: 0.005172246653172705\n",
      "Validation loss: 0.0032945300141970315\n",
      "\n",
      "EPOCH 35 of 50\n",
      "\n",
      "Training loss: 0.005203917260385222\n",
      "Validation loss: 0.0033002516720443964\n",
      "\n",
      "EPOCH 36 of 50\n",
      "\n",
      "Training loss: 0.005220388967750801\n",
      "Validation loss: 0.003297976218163967\n",
      "\n",
      "EPOCH 37 of 50\n",
      "\n",
      "Training loss: 0.005072950126810206\n",
      "Validation loss: 0.003306969224164883\n",
      "\n",
      "EPOCH 38 of 50\n",
      "\n",
      "Training loss: 0.005309801290018691\n",
      "Validation loss: 0.0033103665336966515\n",
      "\n",
      "EPOCH 39 of 50\n",
      "\n",
      "Training loss: 0.005506610135651297\n",
      "Validation loss: 0.003305949425945679\n",
      "\n",
      "EPOCH 40 of 50\n",
      "\n",
      "Training loss: 0.005296781814346711\n",
      "Validation loss: 0.0033062017367531857\n",
      "\n",
      "EPOCH 41 of 50\n",
      "\n",
      "Training loss: 0.005224929268782337\n",
      "Validation loss: 0.0033089302014559507\n",
      "\n",
      "EPOCH 42 of 50\n",
      "\n",
      "Training loss: 0.005067330733355548\n",
      "Validation loss: 0.003299183212220669\n",
      "\n",
      "EPOCH 43 of 50\n",
      "\n",
      "Training loss: 0.005309977672166294\n",
      "Validation loss: 0.0032996326529731355\n",
      "\n",
      "EPOCH 44 of 50\n",
      "\n",
      "Training loss: 0.005157625923554103\n",
      "Validation loss: 0.003308483399450779\n",
      "\n",
      "EPOCH 45 of 50\n",
      "\n",
      "Training loss: 0.00505045079626143\n",
      "Validation loss: 0.0033064103530098996\n",
      "\n",
      "EPOCH 46 of 50\n",
      "\n",
      "Training loss: 0.0050603893533762954\n",
      "Validation loss: 0.003296049234146873\n",
      "\n",
      "EPOCH 47 of 50\n",
      "\n",
      "Training loss: 0.005305054804517163\n",
      "Validation loss: 0.00330156033548216\n",
      "\n",
      "EPOCH 48 of 50\n",
      "\n",
      "Training loss: 0.005353556221557988\n",
      "Validation loss: 0.003296337245653073\n",
      "\n",
      "EPOCH 49 of 50\n",
      "\n",
      "Training loss: 0.0051103820248196525\n",
      "Validation loss: 0.0032966971242179475\n",
      "\n",
      "EPOCH 50 of 50\n",
      "\n",
      "Training loss: 0.005154675002106362\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-30 04:28:51,345] Trial 14 finished with value: 0.0032992351334542036 and parameters: {'lr': 0.0003066065234111946, 'weight_decay': 5.3158130272964435e-05, 'hidden_size': 405, 'dropout_rate': 0.42686234242989957}. Best is trial 13 with value: 0.0032907865631083646.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.0032992351334542036\n",
      "learning rate: 1.1031626173491418e-05\n",
      "weight_decay: 4.214407218369864e-05\n",
      "hidden_size: 422\n",
      "dropout_rate: 0.35002327166392705\n",
      "\n",
      "EPOCH 1 of 50\n",
      "\n",
      "Training loss: 0.005175238009542227\n",
      "Validation loss: 0.0032970168006916842\n",
      "\n",
      "EPOCH 2 of 50\n",
      "\n",
      "Training loss: 0.005361965702225764\n",
      "Validation loss: 0.003304135131960114\n",
      "\n",
      "EPOCH 3 of 50\n",
      "\n",
      "Training loss: 0.00565247092809942\n",
      "Validation loss: 0.0033135712146759033\n",
      "\n",
      "EPOCH 4 of 50\n",
      "\n",
      "Training loss: 0.005407987136600746\n",
      "Validation loss: 0.0033251182176172733\n",
      "\n",
      "EPOCH 5 of 50\n",
      "\n",
      "Training loss: 0.005569386994466186\n",
      "Validation loss: 0.0033264771724740663\n",
      "\n",
      "EPOCH 6 of 50\n",
      "\n",
      "Training loss: 0.0052798403840925955\n",
      "Validation loss: 0.0033236535576482615\n",
      "\n",
      "EPOCH 7 of 50\n",
      "\n",
      "Training loss: 0.005254203169089224\n",
      "Validation loss: 0.003327562939375639\n",
      "\n",
      "EPOCH 8 of 50\n",
      "\n",
      "Training loss: 0.005240164774780472\n",
      "Validation loss: 0.0033318796195089817\n",
      "\n",
      "EPOCH 9 of 50\n",
      "\n",
      "Training loss: 0.00566948081056277\n",
      "Validation loss: 0.003332228632643819\n",
      "\n",
      "EPOCH 10 of 50\n",
      "\n",
      "Training loss: 0.005518264137208462\n",
      "Validation loss: 0.0033310630824416876\n",
      "\n",
      "EPOCH 11 of 50\n",
      "\n",
      "Training loss: 0.00531348730954859\n",
      "Validation loss: 0.0033341058685133853\n",
      "\n",
      "EPOCH 12 of 50\n",
      "\n",
      "Training loss: 0.0054833659281333285\n",
      "Validation loss: 0.003337133675813675\n",
      "\n",
      "EPOCH 13 of 50\n",
      "\n",
      "Training loss: 0.00519095335362686\n",
      "Validation loss: 0.0033379207209994397\n",
      "\n",
      "EPOCH 14 of 50\n",
      "\n",
      "Training loss: 0.005563265799234311\n",
      "Validation loss: 0.003332667906458179\n",
      "\n",
      "EPOCH 15 of 50\n",
      "\n",
      "Training loss: 0.005210959042112033\n",
      "Validation loss: 0.0033313281989345946\n",
      "\n",
      "EPOCH 16 of 50\n",
      "\n",
      "Training loss: 0.005288812228375011\n",
      "Validation loss: 0.0033267249818891287\n",
      "\n",
      "EPOCH 17 of 50\n",
      "\n",
      "Training loss: 0.005655634630885389\n",
      "Validation loss: 0.0033284584836413464\n",
      "\n",
      "EPOCH 18 of 50\n",
      "\n",
      "Training loss: 0.0054085383502145605\n",
      "Validation loss: 0.0033263593601683774\n",
      "\n",
      "EPOCH 19 of 50\n",
      "\n",
      "Training loss: 0.005643968780835469\n",
      "Validation loss: 0.003332340158522129\n",
      "\n",
      "EPOCH 20 of 50\n",
      "\n",
      "Training loss: 0.005315115209668875\n",
      "Validation loss: 0.0033226617767165103\n",
      "\n",
      "EPOCH 21 of 50\n",
      "\n",
      "Training loss: 0.005356145401795705\n",
      "Validation loss: 0.003327569691464305\n",
      "\n",
      "EPOCH 22 of 50\n",
      "\n",
      "Training loss: 0.005244121679829227\n",
      "Validation loss: 0.0033279020184030137\n",
      "\n",
      "EPOCH 23 of 50\n",
      "\n",
      "Training loss: 0.0054690054514341885\n",
      "Validation loss: 0.0033315051502237716\n",
      "\n",
      "EPOCH 24 of 50\n",
      "\n",
      "Training loss: 0.005451628369175726\n",
      "Validation loss: 0.003331454936414957\n",
      "\n",
      "EPOCH 25 of 50\n",
      "\n",
      "Training loss: 0.005437463004555967\n",
      "Validation loss: 0.0033277754361430802\n",
      "\n",
      "EPOCH 26 of 50\n",
      "\n",
      "Training loss: 0.0054320528482397394\n",
      "Validation loss: 0.00332666685183843\n",
      "\n",
      "EPOCH 27 of 50\n",
      "\n",
      "Training loss: 0.005262536488266455\n",
      "Validation loss: 0.0033314311876893044\n",
      "\n",
      "EPOCH 28 of 50\n",
      "\n",
      "Training loss: 0.005588096049096849\n",
      "Validation loss: 0.0033321034473677478\n",
      "\n",
      "EPOCH 29 of 50\n",
      "\n",
      "Training loss: 0.0051356593353880774\n",
      "Validation loss: 0.003327953784416119\n",
      "\n",
      "EPOCH 30 of 50\n",
      "\n",
      "Training loss: 0.00536158758526047\n",
      "Validation loss: 0.0033277097002913556\n",
      "\n",
      "EPOCH 31 of 50\n",
      "\n",
      "Training loss: 0.005293913289076752\n",
      "Validation loss: 0.0033329600313057504\n",
      "\n",
      "EPOCH 32 of 50\n",
      "\n",
      "Training loss: 0.005260000160584847\n",
      "Validation loss: 0.003334060776978731\n",
      "\n",
      "EPOCH 33 of 50\n",
      "\n",
      "Training loss: 0.00549890985712409\n",
      "Validation loss: 0.003334767728423079\n",
      "\n",
      "EPOCH 34 of 50\n",
      "\n",
      "Training loss: 0.005464796649499072\n",
      "Validation loss: 0.003329192210609714\n",
      "\n",
      "EPOCH 35 of 50\n",
      "\n",
      "Training loss: 0.0053084345224003\n",
      "Validation loss: 0.0033332400489598513\n",
      "\n",
      "EPOCH 36 of 50\n",
      "\n",
      "Training loss: 0.005381763602296512\n",
      "Validation loss: 0.0033334247612704835\n",
      "\n",
      "EPOCH 37 of 50\n",
      "\n",
      "Training loss: 0.005269972529883186\n",
      "Validation loss: 0.003327557584270835\n",
      "\n",
      "EPOCH 38 of 50\n",
      "\n",
      "Training loss: 0.005281881274034579\n",
      "Validation loss: 0.003327536474292477\n",
      "\n",
      "EPOCH 39 of 50\n",
      "\n",
      "Training loss: 0.005183252298997508\n",
      "Validation loss: 0.0033315459731966257\n",
      "\n",
      "EPOCH 40 of 50\n",
      "\n",
      "Training loss: 0.005395657175944911\n",
      "Validation loss: 0.00333149335347116\n",
      "\n",
      "EPOCH 41 of 50\n",
      "\n",
      "Training loss: 0.005277775848905246\n",
      "Validation loss: 0.003334140017007788\n",
      "\n",
      "EPOCH 42 of 50\n",
      "\n",
      "Training loss: 0.005322855431586504\n",
      "Validation loss: 0.0033351678090790906\n",
      "\n",
      "EPOCH 43 of 50\n",
      "\n",
      "Training loss: 0.00537441179363264\n",
      "Validation loss: 0.0033260257138560214\n",
      "\n",
      "EPOCH 44 of 50\n",
      "\n",
      "Training loss: 0.005187193707873424\n",
      "Validation loss: 0.0033289941493421793\n",
      "\n",
      "EPOCH 45 of 50\n",
      "\n",
      "Training loss: 0.005543189330233468\n",
      "Validation loss: 0.0033340650455405316\n",
      "\n",
      "EPOCH 46 of 50\n",
      "\n",
      "Training loss: 0.005203559813607071\n",
      "Validation loss: 0.0033392426557838917\n",
      "\n",
      "EPOCH 47 of 50\n",
      "\n",
      "Training loss: 0.00559599982160661\n",
      "Validation loss: 0.003332816995680332\n",
      "\n",
      "EPOCH 48 of 50\n",
      "\n",
      "Training loss: 0.005490768390397231\n",
      "Validation loss: 0.0033317044532547393\n",
      "\n",
      "EPOCH 49 of 50\n",
      "\n",
      "Training loss: 0.0051097155859073\n",
      "Validation loss: 0.00333456345833838\n",
      "\n",
      "EPOCH 50 of 50\n",
      "\n",
      "Training loss: 0.005469373065150446\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-30 04:33:04,901] Trial 15 finished with value: 0.0033318758166084685 and parameters: {'lr': 1.1031626173491418e-05, 'weight_decay': 4.214407218369864e-05, 'hidden_size': 422, 'dropout_rate': 0.35002327166392705}. Best is trial 13 with value: 0.0032907865631083646.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.0033318758166084685\n",
      "learning rate: 0.00022213513827246681\n",
      "weight_decay: 0.00011222525082498122\n",
      "hidden_size: 305\n",
      "dropout_rate: 0.4318767432591459\n",
      "\n",
      "EPOCH 1 of 50\n",
      "\n",
      "Training loss: 0.005242466409173276\n",
      "Validation loss: 0.003299010917544365\n",
      "\n",
      "EPOCH 2 of 50\n",
      "\n",
      "Training loss: 0.005154566503026419\n",
      "Validation loss: 0.003304968820884824\n",
      "\n",
      "EPOCH 3 of 50\n",
      "\n",
      "Training loss: 0.005384639733367496\n",
      "Validation loss: 0.0032938700169324875\n",
      "\n",
      "EPOCH 4 of 50\n",
      "\n",
      "Training loss: 0.005160314444866445\n",
      "Validation loss: 0.003286858322098851\n",
      "\n",
      "EPOCH 5 of 50\n",
      "\n",
      "Training loss: 0.005229236325249076\n",
      "Validation loss: 0.003287769310797254\n",
      "\n",
      "EPOCH 6 of 50\n",
      "\n",
      "Training loss: 0.005464016227051616\n",
      "Validation loss: 0.0032884872828920684\n",
      "\n",
      "EPOCH 7 of 50\n",
      "\n",
      "Training loss: 0.005438361730840471\n",
      "Validation loss: 0.0032894222531467676\n",
      "\n",
      "EPOCH 8 of 50\n",
      "\n",
      "Training loss: 0.005167146446183324\n",
      "Validation loss: 0.0032900394095728793\n",
      "\n",
      "EPOCH 9 of 50\n",
      "\n",
      "Training loss: 0.005320755609621604\n",
      "Validation loss: 0.003289888302485148\n",
      "\n",
      "EPOCH 10 of 50\n",
      "\n",
      "Training loss: 0.005327847631027301\n",
      "Validation loss: 0.0032859925801555314\n",
      "\n",
      "EPOCH 11 of 50\n",
      "\n",
      "Training loss: 0.0051731317717995905\n",
      "Validation loss: 0.003285411512479186\n",
      "\n",
      "EPOCH 12 of 50\n",
      "\n",
      "Training loss: 0.005372268354727162\n",
      "Validation loss: 0.0032898331992328167\n",
      "\n",
      "EPOCH 13 of 50\n",
      "\n",
      "Training loss: 0.005416122575600942\n",
      "Validation loss: 0.00328665878623724\n",
      "\n",
      "EPOCH 14 of 50\n",
      "\n",
      "Training loss: 0.005294132149881787\n",
      "Validation loss: 0.003286386917655667\n",
      "\n",
      "EPOCH 15 of 50\n",
      "\n",
      "Training loss: 0.005386688849992222\n",
      "Validation loss: 0.003288842039182782\n",
      "\n",
      "EPOCH 16 of 50\n",
      "\n",
      "Training loss: 0.00529800326977339\n",
      "Validation loss: 0.0032930263162901006\n",
      "\n",
      "EPOCH 17 of 50\n",
      "\n",
      "Training loss: 0.005237823947229319\n",
      "Validation loss: 0.0032899565218637386\n",
      "\n",
      "EPOCH 18 of 50\n",
      "\n",
      "Training loss: 0.005114049547248417\n",
      "Validation loss: 0.003286181561027964\n",
      "\n",
      "EPOCH 19 of 50\n",
      "\n",
      "Training loss: 0.005329031057448851\n",
      "Validation loss: 0.0032889072317630053\n",
      "\n",
      "EPOCH 20 of 50\n",
      "\n",
      "Training loss: 0.005521477251831029\n",
      "Validation loss: 0.0032870063247780004\n",
      "\n",
      "EPOCH 21 of 50\n",
      "\n",
      "Training loss: 0.005654990000443326\n",
      "Validation loss: 0.0032860501669347286\n",
      "\n",
      "EPOCH 22 of 50\n",
      "\n",
      "Training loss: 0.005461264298193985\n",
      "Validation loss: 0.003286912261197964\n",
      "\n",
      "EPOCH 23 of 50\n",
      "\n",
      "Training loss: 0.005744096667816241\n",
      "Validation loss: 0.003285592266668876\n",
      "\n",
      "EPOCH 24 of 50\n",
      "\n",
      "Training loss: 0.005280857103773289\n",
      "Validation loss: 0.0032870437329014144\n",
      "\n",
      "EPOCH 25 of 50\n",
      "\n",
      "Training loss: 0.0056197135450525414\n",
      "Validation loss: 0.003286830149590969\n",
      "\n",
      "EPOCH 26 of 50\n",
      "\n",
      "Training loss: 0.005305650799224774\n",
      "Validation loss: 0.0032905233092606068\n",
      "\n",
      "EPOCH 27 of 50\n",
      "\n",
      "Training loss: 0.00553655360514919\n",
      "Validation loss: 0.003290171269327402\n",
      "\n",
      "EPOCH 28 of 50\n",
      "\n",
      "Training loss: 0.005109408094237248\n",
      "Validation loss: 0.0032880013653387627\n",
      "\n",
      "EPOCH 29 of 50\n",
      "\n",
      "Training loss: 0.005353337464233239\n",
      "Validation loss: 0.0032886710638801255\n",
      "\n",
      "EPOCH 30 of 50\n",
      "\n",
      "Training loss: 0.00524461574645506\n",
      "Validation loss: 0.0032887430861592293\n",
      "\n",
      "EPOCH 31 of 50\n",
      "\n",
      "Training loss: 0.005436376409812106\n",
      "Validation loss: 0.00329005834646523\n",
      "\n",
      "EPOCH 32 of 50\n",
      "\n",
      "Training loss: 0.005347429024469521\n",
      "Validation loss: 0.0032908214100946984\n",
      "\n",
      "EPOCH 33 of 50\n",
      "\n",
      "Training loss: 0.00551775698032644\n",
      "Validation loss: 0.0032870041516919932\n",
      "\n",
      "EPOCH 34 of 50\n",
      "\n",
      "Training loss: 0.005470167638527023\n",
      "Validation loss: 0.0032919006577382484\n",
      "\n",
      "EPOCH 35 of 50\n",
      "\n",
      "Training loss: 0.0053413124858505195\n",
      "Validation loss: 0.003291113612552484\n",
      "\n",
      "EPOCH 36 of 50\n",
      "\n",
      "Training loss: 0.005102435023420387\n",
      "Validation loss: 0.0032900931934515634\n",
      "\n",
      "EPOCH 37 of 50\n",
      "\n",
      "Training loss: 0.005480283509112067\n",
      "Validation loss: 0.003287593058000008\n",
      "\n",
      "EPOCH 38 of 50\n",
      "\n",
      "Training loss: 0.00538707384839654\n",
      "Validation loss: 0.0032895562859872975\n",
      "\n",
      "EPOCH 39 of 50\n",
      "\n",
      "Training loss: 0.005622040842556291\n",
      "Validation loss: 0.003281808535878857\n",
      "\n",
      "EPOCH 40 of 50\n",
      "\n",
      "Training loss: 0.005506347450945113\n",
      "Validation loss: 0.003289366684233149\n",
      "\n",
      "EPOCH 41 of 50\n",
      "\n",
      "Training loss: 0.005182699869490332\n",
      "Validation loss: 0.0032856022007763386\n",
      "\n",
      "EPOCH 42 of 50\n",
      "\n",
      "Training loss: 0.005417220811876986\n",
      "Validation loss: 0.003285034326836467\n",
      "\n",
      "EPOCH 43 of 50\n",
      "\n",
      "Training loss: 0.005322199625273545\n",
      "Validation loss: 0.003286918858066201\n",
      "\n",
      "EPOCH 44 of 50\n",
      "\n",
      "Training loss: 0.005190508207306266\n",
      "Validation loss: 0.0032854302941511073\n",
      "\n",
      "EPOCH 45 of 50\n",
      "\n",
      "Training loss: 0.004980491991672251\n",
      "Validation loss: 0.0032889768481254578\n",
      "\n",
      "EPOCH 46 of 50\n",
      "\n",
      "Training loss: 0.005294021684676409\n",
      "Validation loss: 0.0032849354514231286\n",
      "\n",
      "EPOCH 47 of 50\n",
      "\n",
      "Training loss: 0.005206364827851455\n",
      "Validation loss: 0.003284459856028358\n",
      "\n",
      "EPOCH 48 of 50\n",
      "\n",
      "Training loss: 0.005318520280222098\n",
      "Validation loss: 0.003287743932257096\n",
      "\n",
      "EPOCH 49 of 50\n",
      "\n",
      "Training loss: 0.005282913075966967\n",
      "Validation loss: 0.0032844343222677708\n",
      "\n",
      "EPOCH 50 of 50\n",
      "\n",
      "Training loss: 0.005057326673219602\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-30 04:37:26,643] Trial 16 finished with value: 0.0032828766076515117 and parameters: {'lr': 0.00022213513827246681, 'weight_decay': 0.00011222525082498122, 'hidden_size': 305, 'dropout_rate': 0.4318767432591459}. Best is trial 16 with value: 0.0032828766076515117.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.0032828766076515117\n",
      "learning rate: 0.00019478582263426697\n",
      "weight_decay: 0.00016387687974498885\n",
      "hidden_size: 305\n",
      "dropout_rate: 0.416075884479356\n",
      "\n",
      "EPOCH 1 of 50\n",
      "\n",
      "Training loss: 0.005393956115262376\n",
      "Validation loss: 0.0033614860537151494\n",
      "\n",
      "EPOCH 2 of 50\n",
      "\n",
      "Training loss: 0.005385975198199351\n",
      "Validation loss: 0.003396101063117385\n",
      "\n",
      "EPOCH 3 of 50\n",
      "\n",
      "Training loss: 0.005424969208737214\n",
      "Validation loss: 0.0034291412060459456\n",
      "\n",
      "EPOCH 4 of 50\n",
      "\n",
      "Training loss: 0.005205056216153834\n",
      "Validation loss: 0.0034548055846244097\n",
      "\n",
      "EPOCH 5 of 50\n",
      "\n",
      "Training loss: 0.005329030979838636\n",
      "Validation loss: 0.003491449480255445\n",
      "\n",
      "EPOCH 6 of 50\n",
      "\n",
      "Training loss: 0.005497154727992084\n",
      "Validation loss: 0.003488905650253097\n",
      "\n",
      "EPOCH 7 of 50\n",
      "\n",
      "Training loss: 0.005372038887192805\n",
      "Validation loss: 0.003507515493159493\n",
      "\n",
      "EPOCH 8 of 50\n",
      "\n",
      "Training loss: 0.0051278989348146654\n",
      "Validation loss: 0.003513000284632047\n",
      "\n",
      "EPOCH 9 of 50\n",
      "\n",
      "Training loss: 0.0053041747822943665\n",
      "Validation loss: 0.0034864707849919796\n",
      "\n",
      "EPOCH 10 of 50\n",
      "\n",
      "Training loss: 0.0054578663677805\n",
      "Validation loss: 0.0035013245263447366\n",
      "\n",
      "EPOCH 11 of 50\n",
      "\n",
      "Training loss: 0.005150932378859984\n",
      "Validation loss: 0.0034837148462732634\n",
      "\n",
      "EPOCH 12 of 50\n",
      "\n",
      "Training loss: 0.0057043507177796625\n",
      "Validation loss: 0.003454246480638782\n",
      "\n",
      "EPOCH 13 of 50\n",
      "\n",
      "Training loss: 0.005348935050682889\n",
      "Validation loss: 0.0034732065784434476\n",
      "\n",
      "EPOCH 14 of 50\n",
      "\n",
      "Training loss: 0.005327072718905078\n",
      "Validation loss: 0.003494006193553408\n",
      "\n",
      "EPOCH 15 of 50\n",
      "\n",
      "Training loss: 0.005316443974152207\n",
      "Validation loss: 0.003504831421499451\n",
      "\n",
      "EPOCH 16 of 50\n",
      "\n",
      "Training loss: 0.005391004986854063\n",
      "Validation loss: 0.003484926496942838\n",
      "\n",
      "EPOCH 17 of 50\n",
      "\n",
      "Training loss: 0.0052003752078033155\n",
      "Validation loss: 0.003469194130351146\n",
      "\n",
      "EPOCH 18 of 50\n",
      "\n",
      "Training loss: 0.005723379888675279\n",
      "Validation loss: 0.0034860233621050916\n",
      "\n",
      "EPOCH 19 of 50\n",
      "\n",
      "Training loss: 0.005374029433975617\n",
      "Validation loss: 0.003476783943672975\n",
      "\n",
      "EPOCH 20 of 50\n",
      "\n",
      "Training loss: 0.005419565468198723\n",
      "Validation loss: 0.0034867553040385246\n",
      "\n",
      "EPOCH 21 of 50\n",
      "\n",
      "Training loss: 0.005361542467855745\n",
      "Validation loss: 0.0034804548292110362\n",
      "\n",
      "EPOCH 22 of 50\n",
      "\n",
      "Training loss: 0.005672319999171628\n",
      "Validation loss: 0.00347406060124437\n",
      "\n",
      "EPOCH 23 of 50\n",
      "\n",
      "Training loss: 0.0055068109391464126\n",
      "Validation loss: 0.003479587767894069\n",
      "\n",
      "EPOCH 24 of 50\n",
      "\n",
      "Training loss: 0.005383125713302029\n",
      "Validation loss: 0.003490289207547903\n",
      "\n",
      "EPOCH 25 of 50\n",
      "\n",
      "Training loss: 0.005220456540377604\n",
      "Validation loss: 0.003456912158677975\n",
      "\n",
      "EPOCH 26 of 50\n",
      "\n",
      "Training loss: 0.005536979167825646\n",
      "Validation loss: 0.0034875955122212567\n",
      "\n",
      "EPOCH 27 of 50\n",
      "\n",
      "Training loss: 0.005487861784381999\n",
      "Validation loss: 0.0034855178091675043\n",
      "\n",
      "EPOCH 28 of 50\n",
      "\n",
      "Training loss: 0.005407840013504028\n",
      "Validation loss: 0.0035030696696291366\n",
      "\n",
      "EPOCH 29 of 50\n",
      "\n",
      "Training loss: 0.005278702307906415\n",
      "Validation loss: 0.003481827676296234\n",
      "\n",
      "EPOCH 30 of 50\n",
      "\n",
      "Training loss: 0.005561683068258895\n",
      "Validation loss: 0.0034738092217594385\n",
      "\n",
      "EPOCH 31 of 50\n",
      "\n",
      "Training loss: 0.005356518939758341\n",
      "Validation loss: 0.003478678253789743\n",
      "\n",
      "EPOCH 32 of 50\n",
      "\n",
      "Training loss: 0.005388147715065215\n",
      "Validation loss: 0.0034672426215062537\n",
      "\n",
      "EPOCH 33 of 50\n",
      "\n",
      "Training loss: 0.005387834273278713\n",
      "Validation loss: 0.0034537164804836116\n",
      "\n",
      "EPOCH 34 of 50\n",
      "\n",
      "Training loss: 0.005285374483921462\n",
      "Validation loss: 0.0034581884586562714\n",
      "\n",
      "EPOCH 35 of 50\n",
      "\n",
      "Training loss: 0.005387867180009683\n",
      "Validation loss: 0.003476156077037255\n",
      "\n",
      "EPOCH 36 of 50\n",
      "\n",
      "Training loss: 0.005146326548937295\n",
      "Validation loss: 0.003478358577316006\n",
      "\n",
      "EPOCH 37 of 50\n",
      "\n",
      "Training loss: 0.005146396165299747\n",
      "Validation loss: 0.0035030790604650974\n",
      "\n",
      "EPOCH 38 of 50\n",
      "\n",
      "Training loss: 0.005636974263729321\n",
      "Validation loss: 0.003505473102753361\n",
      "\n",
      "EPOCH 39 of 50\n",
      "\n",
      "Training loss: 0.0055111965630203485\n",
      "Validation loss: 0.0035167724515000978\n",
      "\n",
      "EPOCH 40 of 50\n",
      "\n",
      "Training loss: 0.005522503362347682\n",
      "Validation loss: 0.0034709482764204345\n",
      "\n",
      "EPOCH 41 of 50\n",
      "\n",
      "Training loss: 0.005496796272281144\n",
      "Validation loss: 0.003488227336977919\n",
      "\n",
      "EPOCH 42 of 50\n",
      "\n",
      "Training loss: 0.005223673147459825\n",
      "Validation loss: 0.003506955380241076\n",
      "\n",
      "EPOCH 43 of 50\n",
      "\n",
      "Training loss: 0.005865798409407337\n",
      "Validation loss: 0.003523352323099971\n",
      "\n",
      "EPOCH 44 of 50\n",
      "\n",
      "Training loss: 0.005575675206879775\n",
      "Validation loss: 0.0035357816765705743\n",
      "\n",
      "EPOCH 45 of 50\n",
      "\n",
      "Training loss: 0.005344726300487916\n",
      "Validation loss: 0.0034918785095214844\n",
      "\n",
      "EPOCH 46 of 50\n",
      "\n",
      "Training loss: 0.0054404474794864655\n",
      "Validation loss: 0.003532708389684558\n",
      "\n",
      "EPOCH 47 of 50\n",
      "\n",
      "Training loss: 0.005360003095120192\n",
      "Validation loss: 0.003512921587874492\n",
      "\n",
      "EPOCH 48 of 50\n",
      "\n",
      "Training loss: 0.005829102256231838\n",
      "Validation loss: 0.003507056040689349\n",
      "\n",
      "EPOCH 49 of 50\n",
      "\n",
      "Training loss: 0.005292702931910753\n",
      "Validation loss: 0.003450813548018535\n",
      "\n",
      "EPOCH 50 of 50\n",
      "\n",
      "Training loss: 0.005163879082020786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-30 04:41:54,892] Trial 17 finished with value: 0.0034759363625198603 and parameters: {'lr': 0.00019478582263426697, 'weight_decay': 0.00016387687974498885, 'hidden_size': 305, 'dropout_rate': 0.416075884479356}. Best is trial 16 with value: 0.0032828766076515117.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.0034759363625198603\n",
      "learning rate: 0.0008802312402834637\n",
      "weight_decay: 2.3051204306127317e-05\n",
      "hidden_size: 346\n",
      "dropout_rate: 0.36524944238956836\n",
      "\n",
      "EPOCH 1 of 50\n",
      "\n",
      "Training loss: 0.0049844656605273485\n",
      "Validation loss: 0.0033283209583411613\n",
      "\n",
      "EPOCH 2 of 50\n",
      "\n",
      "Training loss: 0.004752645864047938\n",
      "Validation loss: 0.003329354105517268\n",
      "\n",
      "EPOCH 3 of 50\n",
      "\n",
      "Training loss: 0.004828042014398509\n",
      "Validation loss: 0.003368401899933815\n",
      "\n",
      "EPOCH 4 of 50\n",
      "\n",
      "Training loss: 0.005384025086338322\n",
      "Validation loss: 0.003407898359000683\n",
      "\n",
      "EPOCH 5 of 50\n",
      "\n",
      "Training loss: 0.004875562878118621\n",
      "Validation loss: 0.0034437441111852727\n",
      "\n",
      "EPOCH 6 of 50\n",
      "\n",
      "Training loss: 0.004844165639951825\n",
      "Validation loss: 0.0034399107098579407\n",
      "\n",
      "EPOCH 7 of 50\n",
      "\n",
      "Training loss: 0.0047398842095086975\n",
      "Validation loss: 0.003426663422336181\n",
      "\n",
      "EPOCH 8 of 50\n",
      "\n",
      "Training loss: 0.004732104846172863\n",
      "Validation loss: 0.0034458518493920565\n",
      "\n",
      "EPOCH 9 of 50\n",
      "\n",
      "Training loss: 0.004860226194270783\n",
      "Validation loss: 0.0034276231502493224\n",
      "\n",
      "EPOCH 10 of 50\n",
      "\n",
      "Training loss: 0.00480726203467283\n",
      "Validation loss: 0.003425551267961661\n",
      "\n",
      "EPOCH 11 of 50\n",
      "\n",
      "Training loss: 0.0048670064554446274\n",
      "Validation loss: 0.003440757825349768\n",
      "\n",
      "EPOCH 12 of 50\n",
      "\n",
      "Training loss: 0.005055704956046409\n",
      "Validation loss: 0.0034479656411955753\n",
      "\n",
      "EPOCH 13 of 50\n",
      "\n",
      "Training loss: 0.005053793649292654\n",
      "Validation loss: 0.003410471215223273\n",
      "\n",
      "EPOCH 14 of 50\n",
      "\n",
      "Training loss: 0.004587468561819858\n",
      "Validation loss: 0.0034118815480420985\n",
      "\n",
      "EPOCH 15 of 50\n",
      "\n",
      "Training loss: 0.0048424484622147344\n",
      "Validation loss: 0.003435231667632858\n",
      "\n",
      "EPOCH 16 of 50\n",
      "\n",
      "Training loss: 0.004846631989089979\n",
      "Validation loss: 0.0034252294960121312\n",
      "\n",
      "EPOCH 17 of 50\n",
      "\n",
      "Training loss: 0.004903530701994896\n",
      "Validation loss: 0.003423887693012754\n",
      "\n",
      "EPOCH 18 of 50\n",
      "\n",
      "Training loss: 0.004889857877666752\n",
      "Validation loss: 0.003452591753254334\n",
      "\n",
      "EPOCH 19 of 50\n",
      "\n",
      "Training loss: 0.004780972790386941\n",
      "Validation loss: 0.003435914016639193\n",
      "\n",
      "EPOCH 20 of 50\n",
      "\n",
      "Training loss: 0.004834369782151448\n",
      "Validation loss: 0.003440143152450522\n",
      "\n",
      "EPOCH 21 of 50\n",
      "\n",
      "Training loss: 0.004766225452638335\n",
      "Validation loss: 0.0034247831596682468\n",
      "\n",
      "EPOCH 22 of 50\n",
      "\n",
      "Training loss: 0.00522089540027082\n",
      "Validation loss: 0.0034443518767754235\n",
      "\n",
      "EPOCH 23 of 50\n",
      "\n",
      "Training loss: 0.00472788008240362\n",
      "Validation loss: 0.0034460905008018017\n",
      "\n",
      "EPOCH 24 of 50\n",
      "\n",
      "Training loss: 0.004692964747341143\n",
      "Validation loss: 0.0034243143939723573\n",
      "\n",
      "EPOCH 25 of 50\n",
      "\n",
      "Training loss: 0.004801381068925063\n",
      "Validation loss: 0.003421652518833677\n",
      "\n",
      "EPOCH 26 of 50\n",
      "\n",
      "Training loss: 0.00481091788969934\n",
      "Validation loss: 0.0034383563324809074\n",
      "\n",
      "EPOCH 27 of 50\n",
      "\n",
      "Training loss: 0.004943383236726125\n",
      "Validation loss: 0.0034437028225511312\n",
      "\n",
      "EPOCH 28 of 50\n",
      "\n",
      "Training loss: 0.004910950600687001\n",
      "Validation loss: 0.0034366602388521037\n",
      "\n",
      "EPOCH 29 of 50\n",
      "\n",
      "Training loss: 0.0047541850298229195\n",
      "Validation loss: 0.0034377782916029296\n",
      "\n",
      "EPOCH 30 of 50\n",
      "\n",
      "Training loss: 0.004670127967579497\n",
      "Validation loss: 0.0034311323737104735\n",
      "\n",
      "EPOCH 31 of 50\n",
      "\n",
      "Training loss: 0.004776341193873022\n",
      "Validation loss: 0.0034384972726305327\n",
      "\n",
      "EPOCH 32 of 50\n",
      "\n",
      "Training loss: 0.0046514311494926614\n",
      "Validation loss: 0.003434470078597466\n",
      "\n",
      "EPOCH 33 of 50\n",
      "\n",
      "Training loss: 0.0047945827504413\n",
      "Validation loss: 0.003430270589888096\n",
      "\n",
      "EPOCH 34 of 50\n",
      "\n",
      "Training loss: 0.0047929542553093694\n",
      "Validation loss: 0.0034176905173808336\n",
      "\n",
      "EPOCH 35 of 50\n",
      "\n",
      "Training loss: 0.0046889207636316614\n",
      "Validation loss: 0.003444021179651221\n",
      "\n",
      "EPOCH 36 of 50\n",
      "\n",
      "Training loss: 0.004856887868502074\n",
      "Validation loss: 0.0034466064535081387\n",
      "\n",
      "EPOCH 37 of 50\n",
      "\n",
      "Training loss: 0.005231812932632036\n",
      "Validation loss: 0.003448858313883344\n",
      "\n",
      "EPOCH 38 of 50\n",
      "\n",
      "Training loss: 0.004964546559171544\n",
      "Validation loss: 0.003451801370829344\n",
      "\n",
      "EPOCH 39 of 50\n",
      "\n",
      "Training loss: 0.004968309361073706\n",
      "Validation loss: 0.0034459377638995647\n",
      "\n",
      "EPOCH 40 of 50\n",
      "\n",
      "Training loss: 0.0049746533752315575\n",
      "Validation loss: 0.003428492695093155\n",
      "\n",
      "EPOCH 41 of 50\n",
      "\n",
      "Training loss: 0.004744755802676082\n",
      "Validation loss: 0.0034370661402742067\n",
      "\n",
      "EPOCH 42 of 50\n",
      "\n",
      "Training loss: 0.005356586072593927\n",
      "Validation loss: 0.0034340592101216316\n",
      "\n",
      "EPOCH 43 of 50\n",
      "\n",
      "Training loss: 0.005178622901439667\n",
      "Validation loss: 0.003444146364927292\n",
      "\n",
      "EPOCH 44 of 50\n",
      "\n",
      "Training loss: 0.0047616449236455895\n",
      "Validation loss: 0.003456834858904282\n",
      "\n",
      "EPOCH 45 of 50\n",
      "\n",
      "Training loss: 0.004747678085954653\n",
      "Validation loss: 0.003453359861547748\n",
      "\n",
      "EPOCH 46 of 50\n",
      "\n",
      "Training loss: 0.004726548756783207\n",
      "Validation loss: 0.003442414104938507\n",
      "\n",
      "EPOCH 47 of 50\n",
      "\n",
      "Training loss: 0.004787082266476419\n",
      "Validation loss: 0.0034418799138317504\n",
      "\n",
      "EPOCH 48 of 50\n",
      "\n",
      "Training loss: 0.004886382569869359\n",
      "Validation loss: 0.0034472744446247816\n",
      "\n",
      "EPOCH 49 of 50\n",
      "\n",
      "Training loss: 0.005017568978170554\n",
      "Validation loss: 0.0034231781028211117\n",
      "\n",
      "EPOCH 50 of 50\n",
      "\n",
      "Training loss: 0.004965730994525883\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-30 04:46:22,099] Trial 18 finished with value: 0.0034344093097994723 and parameters: {'lr': 0.0008802312402834637, 'weight_decay': 2.3051204306127317e-05, 'hidden_size': 346, 'dropout_rate': 0.36524944238956836}. Best is trial 16 with value: 0.0032828766076515117.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.0034344093097994723\n",
      "learning rate: 0.0001530000071354922\n",
      "weight_decay: 0.00012825099634030442\n",
      "hidden_size: 302\n",
      "dropout_rate: 0.3967995335354043\n",
      "\n",
      "EPOCH 1 of 50\n",
      "\n",
      "Training loss: 0.005758528494172626\n",
      "Validation loss: 0.003294504169995586\n",
      "\n",
      "EPOCH 2 of 50\n",
      "\n",
      "Training loss: 0.005590655892673466\n",
      "Validation loss: 0.0033057224160681167\n",
      "\n",
      "EPOCH 3 of 50\n",
      "\n",
      "Training loss: 0.005406925972137187\n",
      "Validation loss: 0.003310640575364232\n",
      "\n",
      "EPOCH 4 of 50\n",
      "\n",
      "Training loss: 0.005728481026987235\n",
      "Validation loss: 0.0033136794809252024\n",
      "\n",
      "EPOCH 5 of 50\n",
      "\n",
      "Training loss: 0.0060509188204175895\n",
      "Validation loss: 0.0033103271077076593\n",
      "\n",
      "EPOCH 6 of 50\n",
      "\n",
      "Training loss: 0.0055984569609993035\n",
      "Validation loss: 0.0033215131455411515\n",
      "\n",
      "EPOCH 7 of 50\n",
      "\n",
      "Training loss: 0.005683020792073674\n",
      "Validation loss: 0.0033179679109404483\n",
      "\n",
      "EPOCH 8 of 50\n",
      "\n",
      "Training loss: 0.005526399214027656\n",
      "Validation loss: 0.0033174530447771153\n",
      "\n",
      "EPOCH 9 of 50\n",
      "\n",
      "Training loss: 0.005547809621526135\n",
      "Validation loss: 0.0033188931023081145\n",
      "\n",
      "EPOCH 10 of 50\n",
      "\n",
      "Training loss: 0.005746563016954396\n",
      "Validation loss: 0.0033159664987275996\n",
      "\n",
      "EPOCH 11 of 50\n",
      "\n",
      "Training loss: 0.005556413489911292\n",
      "Validation loss: 0.0033135267440229654\n",
      "\n",
      "EPOCH 12 of 50\n",
      "\n",
      "Training loss: 0.005420135023693244\n",
      "Validation loss: 0.0033143158070743084\n",
      "\n",
      "EPOCH 13 of 50\n",
      "\n",
      "Training loss: 0.0056153303788354\n",
      "Validation loss: 0.003319579021384319\n",
      "\n",
      "EPOCH 14 of 50\n",
      "\n",
      "Training loss: 0.005756297303984563\n",
      "Validation loss: 0.003321568559234341\n",
      "\n",
      "EPOCH 15 of 50\n",
      "\n",
      "Training loss: 0.0053348309981326265\n",
      "Validation loss: 0.0033288341170797744\n",
      "\n",
      "EPOCH 16 of 50\n",
      "\n",
      "Training loss: 0.005841226058287753\n",
      "Validation loss: 0.0033223965050031743\n",
      "\n",
      "EPOCH 17 of 50\n",
      "\n",
      "Training loss: 0.005620965578903754\n",
      "Validation loss: 0.003316714040314158\n",
      "\n",
      "EPOCH 18 of 50\n",
      "\n",
      "Training loss: 0.005645164392060704\n",
      "Validation loss: 0.0033254430163651705\n",
      "\n",
      "EPOCH 19 of 50\n",
      "\n",
      "Training loss: 0.005808263266873028\n",
      "Validation loss: 0.0033207212885220847\n",
      "\n",
      "EPOCH 20 of 50\n",
      "\n",
      "Training loss: 0.005820645898994472\n",
      "Validation loss: 0.003319018054753542\n",
      "\n",
      "EPOCH 21 of 50\n",
      "\n",
      "Training loss: 0.0055170488647288745\n",
      "Validation loss: 0.003323405903453628\n",
      "\n",
      "EPOCH 22 of 50\n",
      "\n",
      "Training loss: 0.00611421921186977\n",
      "Validation loss: 0.0033111409284174442\n",
      "\n",
      "EPOCH 23 of 50\n",
      "\n",
      "Training loss: 0.005588577154816853\n",
      "Validation loss: 0.0033268322392056384\n",
      "\n",
      "EPOCH 24 of 50\n",
      "\n",
      "Training loss: 0.005610852502286434\n",
      "Validation loss: 0.0033225311587254205\n",
      "\n",
      "EPOCH 25 of 50\n",
      "\n",
      "Training loss: 0.005942261933038632\n",
      "Validation loss: 0.003319490390519301\n",
      "\n",
      "EPOCH 26 of 50\n",
      "\n",
      "Training loss: 0.005683812881923384\n",
      "Validation loss: 0.0033144510816782713\n",
      "\n",
      "EPOCH 27 of 50\n",
      "\n",
      "Training loss: 0.005383065203204751\n",
      "Validation loss: 0.00332427901836733\n",
      "\n",
      "EPOCH 28 of 50\n",
      "\n",
      "Training loss: 0.005607979993025462\n",
      "Validation loss: 0.0033187201091398797\n",
      "\n",
      "EPOCH 29 of 50\n",
      "\n",
      "Training loss: 0.005614757900022798\n",
      "Validation loss: 0.0033186642297854028\n",
      "\n",
      "EPOCH 30 of 50\n",
      "\n",
      "Training loss: 0.005369685253956252\n",
      "Validation loss: 0.0033216789985696473\n",
      "\n",
      "EPOCH 31 of 50\n",
      "\n",
      "Training loss: 0.005857019271287654\n",
      "Validation loss: 0.0033200636195639768\n",
      "\n",
      "EPOCH 32 of 50\n",
      "\n",
      "Training loss: 0.0054929568949672915\n",
      "Validation loss: 0.0033113432582467794\n",
      "\n",
      "EPOCH 33 of 50\n",
      "\n",
      "Training loss: 0.005792284901771281\n",
      "Validation loss: 0.0033148590785761676\n",
      "\n",
      "EPOCH 34 of 50\n",
      "\n",
      "Training loss: 0.005906752051992549\n",
      "Validation loss: 0.0033155683583269515\n",
      "\n",
      "EPOCH 35 of 50\n",
      "\n",
      "Training loss: 0.00534970277092523\n",
      "Validation loss: 0.003309919498860836\n",
      "\n",
      "EPOCH 36 of 50\n",
      "\n",
      "Training loss: 0.005583832143909401\n",
      "Validation loss: 0.0033110008419801793\n",
      "\n",
      "EPOCH 37 of 50\n",
      "\n",
      "Training loss: 0.00577164557762444\n",
      "Validation loss: 0.003314812512447437\n",
      "\n",
      "EPOCH 38 of 50\n",
      "\n",
      "Training loss: 0.005640508167238699\n",
      "Validation loss: 0.0033200230294217667\n",
      "\n",
      "EPOCH 39 of 50\n",
      "\n",
      "Training loss: 0.006044005199025075\n",
      "Validation loss: 0.0033156477535764375\n",
      "\n",
      "EPOCH 40 of 50\n",
      "\n",
      "Training loss: 0.0054726433008909225\n",
      "Validation loss: 0.0033153564048310122\n",
      "\n",
      "EPOCH 41 of 50\n",
      "\n",
      "Training loss: 0.005752102678848637\n",
      "Validation loss: 0.0033242303567628064\n",
      "\n",
      "EPOCH 42 of 50\n",
      "\n",
      "Training loss: 0.005326932296156883\n",
      "Validation loss: 0.003321111357460419\n",
      "\n",
      "EPOCH 43 of 50\n",
      "\n",
      "Training loss: 0.005472312914207578\n",
      "Validation loss: 0.003319102106615901\n",
      "\n",
      "EPOCH 44 of 50\n",
      "\n",
      "Training loss: 0.005800843238830566\n",
      "Validation loss: 0.003315890518327554\n",
      "\n",
      "EPOCH 45 of 50\n",
      "\n",
      "Training loss: 0.005570457265194919\n",
      "Validation loss: 0.0033165367785841227\n",
      "\n",
      "EPOCH 46 of 50\n",
      "\n",
      "Training loss: 0.005468726054661804\n",
      "Validation loss: 0.0033142775452385345\n",
      "\n",
      "EPOCH 47 of 50\n",
      "\n",
      "Training loss: 0.005617220368650224\n",
      "Validation loss: 0.003315698898707827\n",
      "\n",
      "EPOCH 48 of 50\n",
      "\n",
      "Training loss: 0.005872654107709725\n",
      "Validation loss: 0.0033186464570462704\n",
      "\n",
      "EPOCH 49 of 50\n",
      "\n",
      "Training loss: 0.00562388196380602\n",
      "Validation loss: 0.003317943774163723\n",
      "\n",
      "EPOCH 50 of 50\n",
      "\n",
      "Training loss: 0.005599734062949817\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-30 04:51:31,194] Trial 19 finished with value: 0.0033180452107141414 and parameters: {'lr': 0.0001530000071354922, 'weight_decay': 0.00012825099634030442, 'hidden_size': 302, 'dropout_rate': 0.3967995335354043}. Best is trial 16 with value: 0.0032828766076515117.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.0033180452107141414\n",
      "learning rate: 0.0007473762309101906\n",
      "weight_decay: 0.000309003407091421\n",
      "hidden_size: 373\n",
      "dropout_rate: 0.3269714140246852\n",
      "\n",
      "EPOCH 1 of 50\n",
      "\n",
      "Training loss: 0.005111769954156544\n",
      "Validation loss: 0.0032971033360809088\n",
      "\n",
      "EPOCH 2 of 50\n",
      "\n",
      "Training loss: 0.00511858092310528\n",
      "Validation loss: 0.0033143050192544856\n",
      "\n",
      "EPOCH 3 of 50\n",
      "\n",
      "Training loss: 0.004908255508376492\n",
      "Validation loss: 0.0033428340684622526\n",
      "\n",
      "EPOCH 4 of 50\n",
      "\n",
      "Training loss: 0.005091483135604196\n",
      "Validation loss: 0.0033816916402429342\n",
      "\n",
      "EPOCH 5 of 50\n",
      "\n",
      "Training loss: 0.005339422314945195\n",
      "Validation loss: 0.0034127844652781882\n",
      "\n",
      "EPOCH 6 of 50\n",
      "\n",
      "Training loss: 0.005013320284585158\n",
      "Validation loss: 0.003425527519236008\n",
      "\n",
      "EPOCH 7 of 50\n",
      "\n",
      "Training loss: 0.004841505912029081\n",
      "Validation loss: 0.003440934233367443\n",
      "\n",
      "EPOCH 8 of 50\n",
      "\n",
      "Training loss: 0.005215475697898203\n",
      "Validation loss: 0.0034384018120666346\n",
      "\n",
      "EPOCH 9 of 50\n",
      "\n",
      "Training loss: 0.005068717498539222\n",
      "Validation loss: 0.003431672385583321\n",
      "\n",
      "EPOCH 10 of 50\n",
      "\n",
      "Training loss: 0.0051383634304834735\n",
      "Validation loss: 0.0034183822572231293\n",
      "\n",
      "EPOCH 11 of 50\n",
      "\n",
      "Training loss: 0.004745518504124548\n",
      "Validation loss: 0.0034269141809393964\n",
      "\n",
      "EPOCH 12 of 50\n",
      "\n",
      "Training loss: 0.005103886515522997\n",
      "Validation loss: 0.003428633945683638\n",
      "\n",
      "EPOCH 13 of 50\n",
      "\n",
      "Training loss: 0.005060042875508468\n",
      "Validation loss: 0.003433854474375645\n",
      "\n",
      "EPOCH 14 of 50\n",
      "\n",
      "Training loss: 0.004835942527279258\n",
      "Validation loss: 0.0034264250037570796\n",
      "\n",
      "EPOCH 15 of 50\n",
      "\n",
      "Training loss: 0.005131490347493026\n",
      "Validation loss: 0.0034501858366032443\n",
      "\n",
      "EPOCH 16 of 50\n",
      "\n",
      "Training loss: 0.004929051087755296\n",
      "Validation loss: 0.003442812788610657\n",
      "\n",
      "EPOCH 17 of 50\n",
      "\n",
      "Training loss: 0.004875102830636833\n",
      "Validation loss: 0.0034310442861169577\n",
      "\n",
      "EPOCH 18 of 50\n",
      "\n",
      "Training loss: 0.004880032605595059\n",
      "Validation loss: 0.0034456527791917324\n",
      "\n",
      "EPOCH 19 of 50\n",
      "\n",
      "Training loss: 0.005005025848125418\n",
      "Validation loss: 0.0034236226541300616\n",
      "\n",
      "EPOCH 20 of 50\n",
      "\n",
      "Training loss: 0.0055836590472608805\n",
      "Validation loss: 0.003447741735726595\n",
      "\n",
      "EPOCH 21 of 50\n",
      "\n",
      "Training loss: 0.005103840440925624\n",
      "Validation loss: 0.00344394085307916\n",
      "\n",
      "EPOCH 22 of 50\n",
      "\n",
      "Training loss: 0.004864387343534165\n",
      "Validation loss: 0.0034306341937432685\n",
      "\n",
      "EPOCH 23 of 50\n",
      "\n",
      "Training loss: 0.004847032406056921\n",
      "Validation loss: 0.0034202494037648043\n",
      "\n",
      "EPOCH 24 of 50\n",
      "\n",
      "Training loss: 0.004880143432981438\n",
      "Validation loss: 0.0034302147881438336\n",
      "\n",
      "EPOCH 25 of 50\n",
      "\n",
      "Training loss: 0.004893356908319725\n",
      "Validation loss: 0.003451806881154577\n",
      "\n",
      "EPOCH 26 of 50\n",
      "\n",
      "Training loss: 0.0049774071926044095\n",
      "Validation loss: 0.003435387664164106\n",
      "\n",
      "EPOCH 27 of 50\n",
      "\n",
      "Training loss: 0.005048064980655909\n",
      "Validation loss: 0.0034339543587217727\n",
      "\n",
      "EPOCH 28 of 50\n",
      "\n",
      "Training loss: 0.004917541648157769\n",
      "Validation loss: 0.003448892772818605\n",
      "\n",
      "EPOCH 29 of 50\n",
      "\n",
      "Training loss: 0.004912137234997418\n",
      "Validation loss: 0.003455805747459332\n",
      "\n",
      "EPOCH 30 of 50\n",
      "\n",
      "Training loss: 0.005148485665106111\n",
      "Validation loss: 0.0033997821932037673\n",
      "\n",
      "EPOCH 31 of 50\n",
      "\n",
      "Training loss: 0.005013756816171938\n",
      "Validation loss: 0.0034280812833458185\n",
      "\n",
      "EPOCH 32 of 50\n",
      "\n",
      "Training loss: 0.005281522921803925\n",
      "Validation loss: 0.0034191321271161237\n",
      "\n",
      "EPOCH 33 of 50\n",
      "\n",
      "Training loss: 0.0047825882987429695\n",
      "Validation loss: 0.0034322925688078008\n",
      "\n",
      "EPOCH 34 of 50\n",
      "\n",
      "Training loss: 0.005232187971058819\n",
      "Validation loss: 0.003454727198307713\n",
      "\n",
      "EPOCH 35 of 50\n",
      "\n",
      "Training loss: 0.005010579040067064\n",
      "Validation loss: 0.003453950552890698\n",
      "\n",
      "EPOCH 36 of 50\n",
      "\n",
      "Training loss: 0.005011074684767259\n",
      "Validation loss: 0.0034302795150627694\n",
      "\n",
      "EPOCH 37 of 50\n",
      "\n",
      "Training loss: 0.0050851826349066365\n",
      "Validation loss: 0.0034425728178272643\n",
      "\n",
      "EPOCH 38 of 50\n",
      "\n",
      "Training loss: 0.005196921475645568\n",
      "Validation loss: 0.003433609769369165\n",
      "\n",
      "EPOCH 39 of 50\n",
      "\n",
      "Training loss: 0.004885956412181258\n",
      "Validation loss: 0.0034203388107319674\n",
      "\n",
      "EPOCH 40 of 50\n",
      "\n",
      "Training loss: 0.005328759240607421\n",
      "Validation loss: 0.003416456049308181\n",
      "\n",
      "EPOCH 41 of 50\n",
      "\n",
      "Training loss: 0.005153321247133944\n",
      "Validation loss: 0.003431004316856464\n",
      "\n",
      "EPOCH 42 of 50\n",
      "\n",
      "Training loss: 0.005072052409458492\n",
      "Validation loss: 0.00345191964879632\n",
      "\n",
      "EPOCH 43 of 50\n",
      "\n",
      "Training loss: 0.00505352892085082\n",
      "Validation loss: 0.0034210774271438518\n",
      "\n",
      "EPOCH 44 of 50\n",
      "\n",
      "Training loss: 0.005012750858440995\n",
      "Validation loss: 0.0034280718148996434\n",
      "\n",
      "EPOCH 45 of 50\n",
      "\n",
      "Training loss: 0.004957499241249429\n",
      "Validation loss: 0.0034603405899057784\n",
      "\n",
      "EPOCH 46 of 50\n",
      "\n",
      "Training loss: 0.005628905259072781\n",
      "Validation loss: 0.0034405464151253304\n",
      "\n",
      "EPOCH 47 of 50\n",
      "\n",
      "Training loss: 0.004813686996284459\n",
      "Validation loss: 0.0034341621988763413\n",
      "\n",
      "EPOCH 48 of 50\n",
      "\n",
      "Training loss: 0.005009133808521761\n",
      "Validation loss: 0.0034346760561068854\n",
      "\n",
      "EPOCH 49 of 50\n",
      "\n",
      "Training loss: 0.005372083745896816\n",
      "Validation loss: 0.003443751328935226\n",
      "\n",
      "EPOCH 50 of 50\n",
      "\n",
      "Training loss: 0.005056631130476792\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-30 04:56:39,683] Trial 20 finished with value: 0.0034280362694213786 and parameters: {'lr': 0.0007473762309101906, 'weight_decay': 0.000309003407091421, 'hidden_size': 373, 'dropout_rate': 0.3269714140246852}. Best is trial 16 with value: 0.0032828766076515117.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.0034280362694213786\n",
      "learning rate: 3.4843735009309125e-05\n",
      "weight_decay: 5.984028747321326e-05\n",
      "hidden_size: 425\n",
      "dropout_rate: 0.44757093809184767\n",
      "\n",
      "EPOCH 1 of 50\n",
      "\n",
      "Training loss: 0.005201999201542801\n",
      "Validation loss: 0.003315369909008344\n",
      "\n",
      "EPOCH 2 of 50\n",
      "\n",
      "Training loss: 0.005748620205041435\n",
      "Validation loss: 0.003320269441852967\n",
      "\n",
      "EPOCH 3 of 50\n",
      "\n",
      "Training loss: 0.005318992460767428\n",
      "Validation loss: 0.0033236417608956494\n",
      "\n",
      "EPOCH 4 of 50\n",
      "\n",
      "Training loss: 0.005452558708687623\n",
      "Validation loss: 0.0033215200528502464\n",
      "\n",
      "EPOCH 5 of 50\n",
      "\n",
      "Training loss: 0.005192212812188599\n",
      "Validation loss: 0.0033254992061605058\n",
      "\n",
      "EPOCH 6 of 50\n",
      "\n",
      "Training loss: 0.005305267146064175\n",
      "Validation loss: 0.0033255526019881168\n",
      "\n",
      "EPOCH 7 of 50\n",
      "\n",
      "Training loss: 0.00533043939827217\n",
      "Validation loss: 0.0033247671090066433\n",
      "\n",
      "EPOCH 8 of 50\n",
      "\n",
      "Training loss: 0.005362627510395315\n",
      "Validation loss: 0.0033241657850643\n",
      "\n",
      "EPOCH 9 of 50\n",
      "\n",
      "Training loss: 0.005242060714711745\n",
      "Validation loss: 0.00332851925243934\n",
      "\n",
      "EPOCH 10 of 50\n",
      "\n",
      "Training loss: 0.005532336266090472\n",
      "Validation loss: 0.0033289726513127484\n",
      "\n",
      "EPOCH 11 of 50\n",
      "\n",
      "Training loss: 0.005617394215530819\n",
      "Validation loss: 0.003327839386959871\n",
      "\n",
      "EPOCH 12 of 50\n",
      "\n",
      "Training loss: 0.005388224600917763\n",
      "Validation loss: 0.0033253656389812627\n",
      "\n",
      "EPOCH 13 of 50\n",
      "\n",
      "Training loss: 0.005367328645661473\n",
      "Validation loss: 0.0033253924921154976\n",
      "\n",
      "EPOCH 14 of 50\n",
      "\n",
      "Training loss: 0.005605229176580906\n",
      "Validation loss: 0.0033222640243669352\n",
      "\n",
      "EPOCH 15 of 50\n",
      "\n",
      "Training loss: 0.0058164335787296295\n",
      "Validation loss: 0.003326529714589318\n",
      "\n",
      "EPOCH 16 of 50\n",
      "\n",
      "Training loss: 0.005253213690593839\n",
      "Validation loss: 0.0033263540050635734\n",
      "\n",
      "EPOCH 17 of 50\n",
      "\n",
      "Training loss: 0.0053184739210539395\n",
      "Validation loss: 0.00332715370071431\n",
      "\n",
      "EPOCH 18 of 50\n",
      "\n",
      "Training loss: 0.005714679602533579\n",
      "Validation loss: 0.0033246309030801058\n",
      "\n",
      "EPOCH 19 of 50\n",
      "\n",
      "Training loss: 0.005532908046411144\n",
      "Validation loss: 0.0033259520617624125\n",
      "\n",
      "EPOCH 20 of 50\n",
      "\n",
      "Training loss: 0.005420266547136837\n",
      "Validation loss: 0.003324597685908278\n",
      "\n",
      "EPOCH 21 of 50\n",
      "\n",
      "Training loss: 0.005336719616833661\n",
      "Validation loss: 0.003327798486376802\n",
      "\n",
      "EPOCH 22 of 50\n",
      "\n",
      "Training loss: 0.005514928915848334\n",
      "Validation loss: 0.003325267074008783\n",
      "\n",
      "EPOCH 23 of 50\n",
      "\n",
      "Training loss: 0.005257979268208146\n",
      "Validation loss: 0.003323193018635114\n",
      "\n",
      "EPOCH 24 of 50\n",
      "\n",
      "Training loss: 0.005296091756059064\n",
      "Validation loss: 0.0033255926488588252\n",
      "\n",
      "EPOCH 25 of 50\n",
      "\n",
      "Training loss: 0.005166374353898896\n",
      "Validation loss: 0.003324787908544143\n",
      "\n",
      "EPOCH 26 of 50\n",
      "\n",
      "Training loss: 0.005511678807023499\n",
      "Validation loss: 0.0033237209233144918\n",
      "\n",
      "EPOCH 27 of 50\n",
      "\n",
      "Training loss: 0.0054597389987773364\n",
      "Validation loss: 0.003322240198031068\n",
      "\n",
      "EPOCH 28 of 50\n",
      "\n",
      "Training loss: 0.005565544176432822\n",
      "Validation loss: 0.003327863678957025\n",
      "\n",
      "EPOCH 29 of 50\n",
      "\n",
      "Training loss: 0.005306112217820353\n",
      "Validation loss: 0.0033307161647826433\n",
      "\n",
      "EPOCH 30 of 50\n",
      "\n",
      "Training loss: 0.00558412525181969\n",
      "Validation loss: 0.0033265698390702405\n",
      "\n",
      "EPOCH 31 of 50\n",
      "\n",
      "Training loss: 0.005380179215636518\n",
      "Validation loss: 0.0033300993187973895\n",
      "\n",
      "EPOCH 32 of 50\n",
      "\n",
      "Training loss: 0.005801445545835627\n",
      "Validation loss: 0.003324900676185886\n",
      "\n",
      "EPOCH 33 of 50\n",
      "\n",
      "Training loss: 0.0053220937132007545\n",
      "Validation loss: 0.0033256458894660077\n",
      "\n",
      "EPOCH 34 of 50\n",
      "\n",
      "Training loss: 0.005220684455707669\n",
      "Validation loss: 0.0033233786622683206\n",
      "\n",
      "EPOCH 35 of 50\n",
      "\n",
      "Training loss: 0.005308569978094763\n",
      "Validation loss: 0.003323520844181379\n",
      "\n",
      "EPOCH 36 of 50\n",
      "\n",
      "Training loss: 0.005440970261891683\n",
      "Validation loss: 0.003325115811700622\n",
      "\n",
      "EPOCH 37 of 50\n",
      "\n",
      "Training loss: 0.005419077092988623\n",
      "Validation loss: 0.0033245690477391085\n",
      "\n",
      "EPOCH 38 of 50\n",
      "\n",
      "Training loss: 0.005424898376481401\n",
      "Validation loss: 0.0033256312987456718\n",
      "\n",
      "EPOCH 39 of 50\n",
      "\n",
      "Training loss: 0.00535684190173116\n",
      "Validation loss: 0.003321988352884849\n",
      "\n",
      "EPOCH 40 of 50\n",
      "\n",
      "Training loss: 0.005411048316293293\n",
      "Validation loss: 0.003330333779255549\n",
      "\n",
      "EPOCH 41 of 50\n",
      "\n",
      "Training loss: 0.005458858950684468\n",
      "Validation loss: 0.0033214572661866746\n",
      "\n",
      "EPOCH 42 of 50\n",
      "\n",
      "Training loss: 0.005366555156393183\n",
      "Validation loss: 0.003328625267992417\n",
      "\n",
      "EPOCH 43 of 50\n",
      "\n",
      "Training loss: 0.005497677613877588\n",
      "Validation loss: 0.0033309091813862324\n",
      "\n",
      "EPOCH 44 of 50\n",
      "\n",
      "Training loss: 0.005282944378753503\n",
      "Validation loss: 0.0033268763218075037\n",
      "\n",
      "EPOCH 45 of 50\n",
      "\n",
      "Training loss: 0.005611921608861949\n",
      "Validation loss: 0.003329192598660787\n",
      "\n",
      "EPOCH 46 of 50\n",
      "\n",
      "Training loss: 0.005439929301953978\n",
      "Validation loss: 0.0033266483806073666\n",
      "\n",
      "EPOCH 47 of 50\n",
      "\n",
      "Training loss: 0.005735717843183213\n",
      "Validation loss: 0.0033243147966762385\n",
      "\n",
      "EPOCH 48 of 50\n",
      "\n",
      "Training loss: 0.005361584584332175\n",
      "Validation loss: 0.0033247649359206357\n",
      "\n",
      "EPOCH 49 of 50\n",
      "\n",
      "Training loss: 0.0057199023560517365\n",
      "Validation loss: 0.0033275275491178036\n",
      "\n",
      "EPOCH 50 of 50\n",
      "\n",
      "Training loss: 0.005385470887025197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-30 05:01:10,708] Trial 21 finished with value: 0.0033247103759398064 and parameters: {'lr': 3.4843735009309125e-05, 'weight_decay': 5.984028747321326e-05, 'hidden_size': 425, 'dropout_rate': 0.44757093809184767}. Best is trial 16 with value: 0.0032828766076515117.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.0033247103759398064\n",
      "learning rate: 0.0001196770804222289\n",
      "weight_decay: 3.3902949349790114e-05\n",
      "hidden_size: 334\n",
      "dropout_rate: 0.43400714682780606\n",
      "\n",
      "EPOCH 1 of 50\n",
      "\n",
      "Training loss: 0.005869885803096824\n",
      "Validation loss: 0.0033932370909800134\n",
      "\n",
      "EPOCH 2 of 50\n",
      "\n",
      "Training loss: 0.006026344917093714\n",
      "Validation loss: 0.003396694160376986\n",
      "\n",
      "EPOCH 3 of 50\n",
      "\n",
      "Training loss: 0.005450252728122804\n",
      "Validation loss: 0.0033905102560917535\n",
      "\n",
      "EPOCH 4 of 50\n",
      "\n",
      "Training loss: 0.005510059650987387\n",
      "Validation loss: 0.0033694865802923837\n",
      "\n",
      "EPOCH 5 of 50\n",
      "\n",
      "Training loss: 0.005360262235626578\n",
      "Validation loss: 0.0033665512843678394\n",
      "\n",
      "EPOCH 6 of 50\n",
      "\n",
      "Training loss: 0.005417526570252246\n",
      "Validation loss: 0.0033815078592548766\n",
      "\n",
      "EPOCH 7 of 50\n",
      "\n",
      "Training loss: 0.005406842515286472\n",
      "Validation loss: 0.0034021508569518724\n",
      "\n",
      "EPOCH 8 of 50\n",
      "\n",
      "Training loss: 0.005725515023287799\n",
      "Validation loss: 0.0033936272375285625\n",
      "\n",
      "EPOCH 9 of 50\n",
      "\n",
      "Training loss: 0.005433989067872365\n",
      "Validation loss: 0.0033809716502825418\n",
      "\n",
      "EPOCH 10 of 50\n",
      "\n",
      "Training loss: 0.005474050063639879\n",
      "Validation loss: 0.0033831459004431963\n",
      "\n",
      "EPOCH 11 of 50\n",
      "\n",
      "Training loss: 0.0054974207240674235\n",
      "Validation loss: 0.0033933863354225955\n",
      "\n",
      "EPOCH 12 of 50\n",
      "\n",
      "Training loss: 0.005575885427080923\n",
      "Validation loss: 0.003367761382833123\n",
      "\n",
      "EPOCH 13 of 50\n",
      "\n",
      "Training loss: 0.0054487115186121725\n",
      "Validation loss: 0.003362762819354733\n",
      "\n",
      "EPOCH 14 of 50\n",
      "\n",
      "Training loss: 0.0056070913560688496\n",
      "Validation loss: 0.0033774659192810455\n",
      "\n",
      "EPOCH 15 of 50\n",
      "\n",
      "Training loss: 0.005554110898325841\n",
      "Validation loss: 0.003369609359651804\n",
      "\n",
      "EPOCH 16 of 50\n",
      "\n",
      "Training loss: 0.005645610961235232\n",
      "Validation loss: 0.0033669558664162955\n",
      "\n",
      "EPOCH 17 of 50\n",
      "\n",
      "Training loss: 0.005468069679207272\n",
      "Validation loss: 0.0033714994012067714\n",
      "\n",
      "EPOCH 18 of 50\n",
      "\n",
      "Training loss: 0.005612692743953731\n",
      "Validation loss: 0.003385200553263227\n",
      "\n",
      "EPOCH 19 of 50\n",
      "\n",
      "Training loss: 0.005933343484583829\n",
      "Validation loss: 0.003381566765407721\n",
      "\n",
      "EPOCH 20 of 50\n",
      "\n",
      "Training loss: 0.005383230228390958\n",
      "Validation loss: 0.003390808046484987\n",
      "\n",
      "EPOCH 21 of 50\n",
      "\n",
      "Training loss: 0.0053872717544436455\n",
      "Validation loss: 0.0033807304377357164\n",
      "\n",
      "EPOCH 22 of 50\n",
      "\n",
      "Training loss: 0.005406529409810901\n",
      "Validation loss: 0.003382393236582478\n",
      "\n",
      "EPOCH 23 of 50\n",
      "\n",
      "Training loss: 0.005403435866658886\n",
      "Validation loss: 0.003380904827887813\n",
      "\n",
      "EPOCH 24 of 50\n",
      "\n",
      "Training loss: 0.005699648573580716\n",
      "Validation loss: 0.0033866445689151683\n",
      "\n",
      "EPOCH 25 of 50\n",
      "\n",
      "Training loss: 0.005801248364150524\n",
      "Validation loss: 0.0034036881600817046\n",
      "\n",
      "EPOCH 26 of 50\n",
      "\n",
      "Training loss: 0.005434554070234299\n",
      "Validation loss: 0.0033680756265918412\n",
      "\n",
      "EPOCH 27 of 50\n",
      "\n",
      "Training loss: 0.005607799989067846\n",
      "Validation loss: 0.0033597423074146113\n",
      "\n",
      "EPOCH 28 of 50\n",
      "\n",
      "Training loss: 0.005561251451985704\n",
      "Validation loss: 0.0033736026380211115\n",
      "\n",
      "EPOCH 29 of 50\n",
      "\n",
      "Training loss: 0.00559236631832189\n",
      "Validation loss: 0.003366545851652821\n",
      "\n",
      "EPOCH 30 of 50\n",
      "\n",
      "Training loss: 0.005397632666346099\n",
      "Validation loss: 0.003376730795328816\n",
      "\n",
      "EPOCH 31 of 50\n",
      "\n",
      "Training loss: 0.005477155248324077\n",
      "Validation loss: 0.0033771563321352005\n",
      "\n",
      "EPOCH 32 of 50\n",
      "\n",
      "Training loss: 0.00529044206875066\n",
      "Validation loss: 0.003383518119032184\n",
      "\n",
      "EPOCH 33 of 50\n",
      "\n",
      "Training loss: 0.005682255607098341\n",
      "Validation loss: 0.0033983252166459956\n",
      "\n",
      "EPOCH 34 of 50\n",
      "\n",
      "Training loss: 0.005563914439537459\n",
      "Validation loss: 0.0033905073069036007\n",
      "\n",
      "EPOCH 35 of 50\n",
      "\n",
      "Training loss: 0.00550121754511363\n",
      "Validation loss: 0.0033725804338852563\n",
      "\n",
      "EPOCH 36 of 50\n",
      "\n",
      "Training loss: 0.005468070481179489\n",
      "Validation loss: 0.0033892435021698475\n",
      "\n",
      "EPOCH 37 of 50\n",
      "\n",
      "Training loss: 0.006036833290838533\n",
      "Validation loss: 0.0033930121765782437\n",
      "\n",
      "EPOCH 38 of 50\n",
      "\n",
      "Training loss: 0.005778772652977043\n",
      "Validation loss: 0.0033690142445266247\n",
      "\n",
      "EPOCH 39 of 50\n",
      "\n",
      "Training loss: 0.005244543232644598\n",
      "Validation loss: 0.003382611476505796\n",
      "\n",
      "EPOCH 40 of 50\n",
      "\n",
      "Training loss: 0.005585706378850672\n",
      "Validation loss: 0.00336705365528663\n",
      "\n",
      "EPOCH 41 of 50\n",
      "\n",
      "Training loss: 0.005513696672601832\n",
      "Validation loss: 0.003373098404457172\n",
      "\n",
      "EPOCH 42 of 50\n",
      "\n",
      "Training loss: 0.005499886038402717\n",
      "Validation loss: 0.00338672388655444\n",
      "\n",
      "EPOCH 43 of 50\n",
      "\n",
      "Training loss: 0.0057104383077886366\n",
      "Validation loss: 0.0033795073007543883\n",
      "\n",
      "EPOCH 44 of 50\n",
      "\n",
      "Training loss: 0.005459357725663317\n",
      "Validation loss: 0.003396221281339725\n",
      "\n",
      "EPOCH 45 of 50\n",
      "\n",
      "Training loss: 0.00549110873705811\n",
      "Validation loss: 0.0034037300695975623\n",
      "\n",
      "EPOCH 46 of 50\n",
      "\n",
      "Training loss: 0.0054032573890354894\n",
      "Validation loss: 0.003378024324774742\n",
      "\n",
      "EPOCH 47 of 50\n",
      "\n",
      "Training loss: 0.005532093760040071\n",
      "Validation loss: 0.003386970842257142\n",
      "\n",
      "EPOCH 48 of 50\n",
      "\n",
      "Training loss: 0.005541502549830411\n",
      "Validation loss: 0.0033696806834389768\n",
      "\n",
      "EPOCH 49 of 50\n",
      "\n",
      "Training loss: 0.005671892211669021\n",
      "Validation loss: 0.0033761331190665564\n",
      "\n",
      "EPOCH 50 of 50\n",
      "\n",
      "Training loss: 0.006009198259562254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-30 05:05:32,912] Trial 22 finished with value: 0.003383135966335734 and parameters: {'lr': 0.0001196770804222289, 'weight_decay': 3.3902949349790114e-05, 'hidden_size': 334, 'dropout_rate': 0.43400714682780606}. Best is trial 16 with value: 0.0032828766076515117.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.003383135966335734\n",
      "learning rate: 3.516198769306717e-05\n",
      "weight_decay: 9.563087462230968e-05\n",
      "hidden_size: 379\n",
      "dropout_rate: 0.46391549399918924\n",
      "\n",
      "EPOCH 1 of 50\n",
      "\n",
      "Training loss: 0.005728556567596065\n",
      "Validation loss: 0.003297097127263745\n",
      "\n",
      "EPOCH 2 of 50\n",
      "\n",
      "Training loss: 0.005614410775403182\n",
      "Validation loss: 0.0032950587725887695\n",
      "\n",
      "EPOCH 3 of 50\n",
      "\n",
      "Training loss: 0.005532963615324762\n",
      "Validation loss: 0.003296872368082404\n",
      "\n",
      "EPOCH 4 of 50\n",
      "\n",
      "Training loss: 0.005752371908682916\n",
      "Validation loss: 0.003307574506228169\n",
      "\n",
      "EPOCH 5 of 50\n",
      "\n",
      "Training loss: 0.005353715969249606\n",
      "Validation loss: 0.0032994706028451524\n",
      "\n",
      "EPOCH 6 of 50\n",
      "\n",
      "Training loss: 0.005449990069286691\n",
      "Validation loss: 0.0032985537157704434\n",
      "\n",
      "EPOCH 7 of 50\n",
      "\n",
      "Training loss: 0.005359042668715119\n",
      "Validation loss: 0.003301705000922084\n",
      "\n",
      "EPOCH 8 of 50\n",
      "\n",
      "Training loss: 0.005273319263425138\n",
      "Validation loss: 0.0033026000019162893\n",
      "\n",
      "EPOCH 9 of 50\n",
      "\n",
      "Training loss: 0.0052268836750752395\n",
      "Validation loss: 0.003303223211939136\n",
      "\n",
      "EPOCH 10 of 50\n",
      "\n",
      "Training loss: 0.00560513495778044\n",
      "Validation loss: 0.0033017349584649005\n",
      "\n",
      "EPOCH 11 of 50\n",
      "\n",
      "Training loss: 0.005549254930681652\n",
      "Validation loss: 0.003300320745135347\n",
      "\n",
      "EPOCH 12 of 50\n",
      "\n",
      "Training loss: 0.005338251047457258\n",
      "Validation loss: 0.003305587529515227\n",
      "\n",
      "EPOCH 13 of 50\n",
      "\n",
      "Training loss: 0.005286953877657652\n",
      "Validation loss: 0.0033032288774847984\n",
      "\n",
      "EPOCH 14 of 50\n",
      "\n",
      "Training loss: 0.005446111679904991\n",
      "Validation loss: 0.003308014323314031\n",
      "\n",
      "EPOCH 15 of 50\n",
      "\n",
      "Training loss: 0.005318979473991526\n",
      "Validation loss: 0.0033109855527679124\n",
      "\n",
      "EPOCH 16 of 50\n",
      "\n",
      "Training loss: 0.005669970272315873\n",
      "Validation loss: 0.0033093181749184928\n",
      "\n",
      "EPOCH 17 of 50\n",
      "\n",
      "Training loss: 0.005377729423344135\n",
      "Validation loss: 0.003299684418986241\n",
      "\n",
      "EPOCH 18 of 50\n",
      "\n",
      "Training loss: 0.005477186499370469\n",
      "Validation loss: 0.003303076606243849\n",
      "\n",
      "EPOCH 19 of 50\n",
      "\n",
      "Training loss: 0.005444937747799688\n",
      "Validation loss: 0.0033045338932424784\n",
      "\n",
      "EPOCH 20 of 50\n",
      "\n",
      "Training loss: 0.00520006952703827\n",
      "Validation loss: 0.0033026674451927343\n",
      "\n",
      "EPOCH 21 of 50\n",
      "\n",
      "Training loss: 0.005432993380559815\n",
      "Validation loss: 0.0033069785373906293\n",
      "\n",
      "EPOCH 22 of 50\n",
      "\n",
      "Training loss: 0.0055712562882237965\n",
      "Validation loss: 0.003309178864583373\n",
      "\n",
      "EPOCH 23 of 50\n",
      "\n",
      "Training loss: 0.005554982978436682\n",
      "Validation loss: 0.0033096501138061285\n",
      "\n",
      "EPOCH 24 of 50\n",
      "\n",
      "Training loss: 0.005281759322517448\n",
      "Validation loss: 0.0033003263330707946\n",
      "\n",
      "EPOCH 25 of 50\n",
      "\n",
      "Training loss: 0.005500802175245351\n",
      "Validation loss: 0.0033000639329353967\n",
      "\n",
      "EPOCH 26 of 50\n",
      "\n",
      "Training loss: 0.005539092493967878\n",
      "Validation loss: 0.0033039928724368415\n",
      "\n",
      "EPOCH 27 of 50\n",
      "\n",
      "Training loss: 0.005434389329618878\n",
      "Validation loss: 0.0033022312757869563\n",
      "\n",
      "EPOCH 28 of 50\n",
      "\n",
      "Training loss: 0.005621085098634164\n",
      "Validation loss: 0.0033058310703684888\n",
      "\n",
      "EPOCH 29 of 50\n",
      "\n",
      "Training loss: 0.005390769698553615\n",
      "Validation loss: 0.0033007030530522266\n",
      "\n",
      "EPOCH 30 of 50\n",
      "\n",
      "Training loss: 0.005501238655091988\n",
      "Validation loss: 0.0033087809570133686\n",
      "\n",
      "EPOCH 31 of 50\n",
      "\n",
      "Training loss: 0.005369393051498466\n",
      "Validation loss: 0.0033010893190900483\n",
      "\n",
      "EPOCH 32 of 50\n",
      "\n",
      "Training loss: 0.005356391529656119\n",
      "Validation loss: 0.0033023035308967033\n",
      "\n",
      "EPOCH 33 of 50\n",
      "\n",
      "Training loss: 0.005524382098681397\n",
      "Validation loss: 0.003301497083157301\n",
      "\n",
      "EPOCH 34 of 50\n",
      "\n",
      "Training loss: 0.005198093467495508\n",
      "Validation loss: 0.0033087480502823987\n",
      "\n",
      "EPOCH 35 of 50\n",
      "\n",
      "Training loss: 0.005646399532755216\n",
      "Validation loss: 0.003308978242178758\n",
      "\n",
      "EPOCH 36 of 50\n",
      "\n",
      "Training loss: 0.005471673121468889\n",
      "Validation loss: 0.0033093298940608897\n",
      "\n",
      "EPOCH 37 of 50\n",
      "\n",
      "Training loss: 0.005074831036229928\n",
      "Validation loss: 0.003305940811211864\n",
      "\n",
      "EPOCH 38 of 50\n",
      "\n",
      "Training loss: 0.005248164913306634\n",
      "Validation loss: 0.003302639117464423\n",
      "\n",
      "EPOCH 39 of 50\n",
      "\n",
      "Training loss: 0.0052835897594276406\n",
      "Validation loss: 0.0033094949709872403\n",
      "\n",
      "EPOCH 40 of 50\n",
      "\n",
      "Training loss: 0.005458832304510806\n",
      "Validation loss: 0.0033028045824418464\n",
      "\n",
      "EPOCH 41 of 50\n",
      "\n",
      "Training loss: 0.005606976803392172\n",
      "Validation loss: 0.003306198554734389\n",
      "\n",
      "EPOCH 42 of 50\n",
      "\n",
      "Training loss: 0.005389669806592994\n",
      "Validation loss: 0.003307034494355321\n",
      "\n",
      "EPOCH 43 of 50\n",
      "\n",
      "Training loss: 0.005453161067432827\n",
      "Validation loss: 0.003306346790244182\n",
      "\n",
      "EPOCH 44 of 50\n",
      "\n",
      "Training loss: 0.0056083116214722395\n",
      "Validation loss: 0.003309617517516017\n",
      "\n",
      "EPOCH 45 of 50\n",
      "\n",
      "Training loss: 0.005857203647287356\n",
      "Validation loss: 0.0033137697416047254\n",
      "\n",
      "EPOCH 46 of 50\n",
      "\n",
      "Training loss: 0.0053783196748958696\n",
      "Validation loss: 0.0033124068286269903\n",
      "\n",
      "EPOCH 47 of 50\n",
      "\n",
      "Training loss: 0.005330443304652969\n",
      "Validation loss: 0.003304427877689401\n",
      "\n",
      "EPOCH 48 of 50\n",
      "\n",
      "Training loss: 0.005304448099599944\n",
      "Validation loss: 0.0033112141148497662\n",
      "\n",
      "EPOCH 49 of 50\n",
      "\n",
      "Training loss: 0.005543313558316893\n",
      "Validation loss: 0.003306137785936395\n",
      "\n",
      "EPOCH 50 of 50\n",
      "\n",
      "Training loss: 0.005037840041849349\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-30 05:09:49,920] Trial 23 finished with value: 0.003309766839568814 and parameters: {'lr': 3.516198769306717e-05, 'weight_decay': 9.563087462230968e-05, 'hidden_size': 379, 'dropout_rate': 0.46391549399918924}. Best is trial 16 with value: 0.0032828766076515117.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.003309766839568814\n",
      "learning rate: 0.0003415617158877162\n",
      "weight_decay: 7.654141374107913e-05\n",
      "hidden_size: 418\n",
      "dropout_rate: 0.4148239556079152\n",
      "\n",
      "EPOCH 1 of 50\n",
      "\n",
      "Training loss: 0.005684344201452202\n",
      "Validation loss: 0.003296851102883617\n",
      "\n",
      "EPOCH 2 of 50\n",
      "\n",
      "Training loss: 0.005453199510359102\n",
      "Validation loss: 0.0032991113451619944\n",
      "\n",
      "EPOCH 3 of 50\n",
      "\n",
      "Training loss: 0.005412210296425555\n",
      "Validation loss: 0.0033043664880096912\n",
      "\n",
      "EPOCH 4 of 50\n",
      "\n",
      "Training loss: 0.0054081773074964685\n",
      "Validation loss: 0.003304374016200503\n",
      "\n",
      "EPOCH 5 of 50\n",
      "\n",
      "Training loss: 0.005341744619525141\n",
      "Validation loss: 0.003307479744156202\n",
      "\n",
      "EPOCH 6 of 50\n",
      "\n",
      "Training loss: 0.0056366041406161254\n",
      "Validation loss: 0.00331526598893106\n",
      "\n",
      "EPOCH 7 of 50\n",
      "\n",
      "Training loss: 0.005458982195705175\n",
      "Validation loss: 0.0033099623396992683\n",
      "\n",
      "EPOCH 8 of 50\n",
      "\n",
      "Training loss: 0.005403105868026614\n",
      "Validation loss: 0.003312466200441122\n",
      "\n",
      "EPOCH 9 of 50\n",
      "\n",
      "Training loss: 0.0055010248389508985\n",
      "Validation loss: 0.0033142514682064452\n",
      "\n",
      "EPOCH 10 of 50\n",
      "\n",
      "Training loss: 0.005270721442583535\n",
      "Validation loss: 0.003310051358615359\n",
      "\n",
      "EPOCH 11 of 50\n",
      "\n",
      "Training loss: 0.005678385237438811\n",
      "Validation loss: 0.0033120354637503624\n",
      "\n",
      "EPOCH 12 of 50\n",
      "\n",
      "Training loss: 0.00585860953045388\n",
      "Validation loss: 0.0033137296171238026\n",
      "\n",
      "EPOCH 13 of 50\n",
      "\n",
      "Training loss: 0.005555792789285381\n",
      "Validation loss: 0.0033175724868973098\n",
      "\n",
      "EPOCH 14 of 50\n",
      "\n",
      "Training loss: 0.005453731088588635\n",
      "Validation loss: 0.0033181640319526196\n",
      "\n",
      "EPOCH 15 of 50\n",
      "\n",
      "Training loss: 0.005937437216440837\n",
      "Validation loss: 0.00331229359532396\n",
      "\n",
      "EPOCH 16 of 50\n",
      "\n",
      "Training loss: 0.005584826382497947\n",
      "Validation loss: 0.0033094470078746476\n",
      "\n",
      "EPOCH 17 of 50\n",
      "\n",
      "Training loss: 0.005486104223463271\n",
      "Validation loss: 0.003308541839942336\n",
      "\n",
      "EPOCH 18 of 50\n",
      "\n",
      "Training loss: 0.005588782899495628\n",
      "Validation loss: 0.0033109422462681928\n",
      "\n",
      "EPOCH 19 of 50\n",
      "\n",
      "Training loss: 0.005372531375744277\n",
      "Validation loss: 0.003310882098351916\n",
      "\n",
      "EPOCH 20 of 50\n",
      "\n",
      "Training loss: 0.005357426410127018\n",
      "Validation loss: 0.003313046880066395\n",
      "\n",
      "EPOCH 21 of 50\n",
      "\n",
      "Training loss: 0.005287151059342755\n",
      "Validation loss: 0.003311101502428452\n",
      "\n",
      "EPOCH 22 of 50\n",
      "\n",
      "Training loss: 0.005689850439214044\n",
      "Validation loss: 0.003311285050585866\n",
      "\n",
      "EPOCH 23 of 50\n",
      "\n",
      "Training loss: 0.005337937735021114\n",
      "Validation loss: 0.003313844713071982\n",
      "\n",
      "EPOCH 24 of 50\n",
      "\n",
      "Training loss: 0.005527089919067091\n",
      "Validation loss: 0.0033106248204906783\n",
      "\n",
      "EPOCH 25 of 50\n",
      "\n",
      "Training loss: 0.005393969412479136\n",
      "Validation loss: 0.0033161835744976997\n",
      "\n",
      "EPOCH 26 of 50\n",
      "\n",
      "Training loss: 0.005356507634537088\n",
      "Validation loss: 0.0033178831605861583\n",
      "\n",
      "EPOCH 27 of 50\n",
      "\n",
      "Training loss: 0.005753220524638891\n",
      "Validation loss: 0.0033174508716911077\n",
      "\n",
      "EPOCH 28 of 50\n",
      "\n",
      "Training loss: 0.005511582104696168\n",
      "Validation loss: 0.0033121327869594097\n",
      "\n",
      "EPOCH 29 of 50\n",
      "\n",
      "Training loss: 0.005547273593644301\n",
      "Validation loss: 0.00331362197175622\n",
      "\n",
      "EPOCH 30 of 50\n",
      "\n",
      "Training loss: 0.005435659601870511\n",
      "Validation loss: 0.003312965544561545\n",
      "\n",
      "EPOCH 31 of 50\n",
      "\n",
      "Training loss: 0.005525972901119126\n",
      "Validation loss: 0.0033172600281735263\n",
      "\n",
      "EPOCH 32 of 50\n",
      "\n",
      "Training loss: 0.005579352068404357\n",
      "Validation loss: 0.003314137846852342\n",
      "\n",
      "EPOCH 33 of 50\n",
      "\n",
      "Training loss: 0.005472289838103784\n",
      "Validation loss: 0.0033123710503180823\n",
      "\n",
      "EPOCH 34 of 50\n",
      "\n",
      "Training loss: 0.005607226242621739\n",
      "Validation loss: 0.00330814851137499\n",
      "\n",
      "EPOCH 35 of 50\n",
      "\n",
      "Training loss: 0.0053321742452681065\n",
      "Validation loss: 0.0033091941537956395\n",
      "\n",
      "EPOCH 36 of 50\n",
      "\n",
      "Training loss: 0.005313553562801745\n",
      "Validation loss: 0.003314650927980741\n",
      "\n",
      "EPOCH 37 of 50\n",
      "\n",
      "Training loss: 0.005544869177457359\n",
      "Validation loss: 0.0033115255646407604\n",
      "\n",
      "EPOCH 38 of 50\n",
      "\n",
      "Training loss: 0.0053743610882924665\n",
      "Validation loss: 0.0033166982854406037\n",
      "\n",
      "EPOCH 39 of 50\n",
      "\n",
      "Training loss: 0.005538497534063127\n",
      "Validation loss: 0.0033142322208732367\n",
      "\n",
      "EPOCH 40 of 50\n",
      "\n",
      "Training loss: 0.005351877228046457\n",
      "Validation loss: 0.003314053174108267\n",
      "\n",
      "EPOCH 41 of 50\n",
      "\n",
      "Training loss: 0.005686950197236406\n",
      "Validation loss: 0.0033127968975653252\n",
      "\n",
      "EPOCH 42 of 50\n",
      "\n",
      "Training loss: 0.005441285307622618\n",
      "Validation loss: 0.0033109089514861503\n",
      "\n",
      "EPOCH 43 of 50\n",
      "\n",
      "Training loss: 0.005630178842693567\n",
      "Validation loss: 0.0033112529199570417\n",
      "\n",
      "EPOCH 44 of 50\n",
      "\n",
      "Training loss: 0.005760193078054322\n",
      "Validation loss: 0.0033139746325711408\n",
      "\n",
      "EPOCH 45 of 50\n",
      "\n",
      "Training loss: 0.005421974463388324\n",
      "Validation loss: 0.0033141582583387694\n",
      "\n",
      "EPOCH 46 of 50\n",
      "\n",
      "Training loss: 0.005273444008909994\n",
      "Validation loss: 0.003315113407249252\n",
      "\n",
      "EPOCH 47 of 50\n",
      "\n",
      "Training loss: 0.005660232725656695\n",
      "Validation loss: 0.0033128422995408378\n",
      "\n",
      "EPOCH 48 of 50\n",
      "\n",
      "Training loss: 0.005292214608440797\n",
      "Validation loss: 0.003311722927416364\n",
      "\n",
      "EPOCH 49 of 50\n",
      "\n",
      "Training loss: 0.005617571322040426\n",
      "Validation loss: 0.0033143043207625547\n",
      "\n",
      "EPOCH 50 of 50\n",
      "\n",
      "Training loss: 0.005703747945113314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-30 05:14:01,017] Trial 24 finished with value: 0.0033081918178747096 and parameters: {'lr': 0.0003415617158877162, 'weight_decay': 7.654141374107913e-05, 'hidden_size': 418, 'dropout_rate': 0.4148239556079152}. Best is trial 16 with value: 0.0032828766076515117.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.0033081918178747096\n",
      "learning rate: 0.00010509445977582517\n",
      "weight_decay: 3.5176434113822414e-05\n",
      "hidden_size: 363\n",
      "dropout_rate: 0.4474841450783594\n",
      "\n",
      "EPOCH 1 of 50\n",
      "\n",
      "Training loss: 0.00547642648840944\n",
      "Validation loss: 0.003506376796091596\n",
      "\n",
      "EPOCH 2 of 50\n",
      "\n",
      "Training loss: 0.005212394210199515\n",
      "Validation loss: 0.003832233138382435\n",
      "\n",
      "EPOCH 3 of 50\n",
      "\n",
      "Training loss: 0.0056579869447482955\n",
      "Validation loss: 0.004062741296365857\n",
      "\n",
      "EPOCH 4 of 50\n",
      "\n",
      "Training loss: 0.005427719093859196\n",
      "Validation loss: 0.004141516828288634\n",
      "\n",
      "EPOCH 5 of 50\n",
      "\n",
      "Training loss: 0.005551441210425562\n",
      "Validation loss: 0.004210923099890351\n",
      "\n",
      "EPOCH 6 of 50\n",
      "\n",
      "Training loss: 0.005280625851203998\n",
      "Validation loss: 0.004215662678082784\n",
      "\n",
      "EPOCH 7 of 50\n",
      "\n",
      "Training loss: 0.005459317885753181\n",
      "Validation loss: 0.0041903787447760505\n",
      "\n",
      "EPOCH 8 of 50\n",
      "\n",
      "Training loss: 0.005206986408059795\n",
      "Validation loss: 0.004174870749314626\n",
      "\n",
      "EPOCH 9 of 50\n",
      "\n",
      "Training loss: 0.005363431500477923\n",
      "Validation loss: 0.004199523711577058\n",
      "\n",
      "EPOCH 10 of 50\n",
      "\n",
      "Training loss: 0.005432158553351958\n",
      "Validation loss: 0.00420073924275736\n",
      "\n",
      "EPOCH 11 of 50\n",
      "\n",
      "Training loss: 0.005302191401521365\n",
      "Validation loss: 0.004189058905467391\n",
      "\n",
      "EPOCH 12 of 50\n",
      "\n",
      "Training loss: 0.005408858621699942\n",
      "Validation loss: 0.004238197424759467\n",
      "\n",
      "EPOCH 13 of 50\n",
      "\n",
      "Training loss: 0.005463479397197564\n",
      "Validation loss: 0.004240328911691904\n",
      "\n",
      "EPOCH 14 of 50\n",
      "\n",
      "Training loss: 0.005371561636113458\n",
      "Validation loss: 0.004186881162847082\n",
      "\n",
      "EPOCH 15 of 50\n",
      "\n",
      "Training loss: 0.005563321989029646\n",
      "Validation loss: 0.0042604195574919386\n",
      "\n",
      "EPOCH 16 of 50\n",
      "\n",
      "Training loss: 0.005503689094136159\n",
      "Validation loss: 0.004241585110624631\n",
      "\n",
      "EPOCH 17 of 50\n",
      "\n",
      "Training loss: 0.005283256966827644\n",
      "Validation loss: 0.0042211928715308504\n",
      "\n",
      "EPOCH 18 of 50\n",
      "\n",
      "Training loss: 0.00541752646677196\n",
      "Validation loss: 0.004252281893665592\n",
      "\n",
      "EPOCH 19 of 50\n",
      "\n",
      "Training loss: 0.005418761969647474\n",
      "Validation loss: 0.004226355347782373\n",
      "\n",
      "EPOCH 20 of 50\n",
      "\n",
      "Training loss: 0.005409797757036156\n",
      "Validation loss: 0.004225439081589381\n",
      "\n",
      "EPOCH 21 of 50\n",
      "\n",
      "Training loss: 0.005337380545420779\n",
      "Validation loss: 0.004246414794276158\n",
      "\n",
      "EPOCH 22 of 50\n",
      "\n",
      "Training loss: 0.005399017749975125\n",
      "Validation loss: 0.0042473264038562775\n",
      "\n",
      "EPOCH 23 of 50\n",
      "\n",
      "Training loss: 0.005216775229200721\n",
      "Validation loss: 0.004227025046323736\n",
      "\n",
      "EPOCH 24 of 50\n",
      "\n",
      "Training loss: 0.005262627525048124\n",
      "Validation loss: 0.004226893885061145\n",
      "\n",
      "EPOCH 25 of 50\n",
      "\n",
      "Training loss: 0.005284397604150904\n",
      "Validation loss: 0.004251495391751329\n",
      "\n",
      "EPOCH 26 of 50\n",
      "\n",
      "Training loss: 0.005381304201566511\n",
      "Validation loss: 0.004200214132045706\n",
      "\n",
      "EPOCH 27 of 50\n",
      "\n",
      "Training loss: 0.005599871174328857\n",
      "Validation loss: 0.0042114541089783115\n",
      "\n",
      "EPOCH 28 of 50\n",
      "\n",
      "Training loss: 0.005437651752597756\n",
      "Validation loss: 0.0042413317908843355\n",
      "\n",
      "EPOCH 29 of 50\n",
      "\n",
      "Training loss: 0.0056254454474482275\n",
      "Validation loss: 0.0042692923452705145\n",
      "\n",
      "EPOCH 30 of 50\n",
      "\n",
      "Training loss: 0.005310384660131401\n",
      "Validation loss: 0.0042509029153734446\n",
      "\n",
      "EPOCH 31 of 50\n",
      "\n",
      "Training loss: 0.005475404051442941\n",
      "Validation loss: 0.004274573099489014\n",
      "\n",
      "EPOCH 32 of 50\n",
      "\n",
      "Training loss: 0.005324219819158316\n",
      "Validation loss: 0.0042502726428210735\n",
      "\n",
      "EPOCH 33 of 50\n",
      "\n",
      "Training loss: 0.005259044494272934\n",
      "Validation loss: 0.004286594999333222\n",
      "\n",
      "EPOCH 34 of 50\n",
      "\n",
      "Training loss: 0.005309947016131546\n",
      "Validation loss: 0.004241667377452056\n",
      "\n",
      "EPOCH 35 of 50\n",
      "\n",
      "Training loss: 0.005786719111104806\n",
      "Validation loss: 0.004255199106410146\n",
      "\n",
      "EPOCH 36 of 50\n",
      "\n",
      "Training loss: 0.00525287130019731\n",
      "Validation loss: 0.004274410118038456\n",
      "\n",
      "EPOCH 37 of 50\n",
      "\n",
      "Training loss: 0.005495881403072013\n",
      "Validation loss: 0.004293295710037152\n",
      "\n",
      "EPOCH 38 of 50\n",
      "\n",
      "Training loss: 0.005306079285219312\n",
      "Validation loss: 0.004298466490581632\n",
      "\n",
      "EPOCH 39 of 50\n",
      "\n",
      "Training loss: 0.005614137975499034\n",
      "Validation loss: 0.004309945972636342\n",
      "\n",
      "EPOCH 40 of 50\n",
      "\n",
      "Training loss: 0.005687741769684685\n",
      "Validation loss: 0.004331923245141904\n",
      "\n",
      "EPOCH 41 of 50\n",
      "\n",
      "Training loss: 0.005592257043139802\n",
      "Validation loss: 0.004283662730207046\n",
      "\n",
      "EPOCH 42 of 50\n",
      "\n",
      "Training loss: 0.005358783450598518\n",
      "Validation loss: 0.004332535046463211\n",
      "\n",
      "EPOCH 43 of 50\n",
      "\n",
      "Training loss: 0.005359607308896052\n",
      "Validation loss: 0.004295012913644314\n",
      "\n",
      "EPOCH 44 of 50\n",
      "\n",
      "Training loss: 0.005453772687663634\n",
      "Validation loss: 0.004285679664462805\n",
      "\n",
      "EPOCH 45 of 50\n",
      "\n",
      "Training loss: 0.005738440176679028\n",
      "Validation loss: 0.004311481413121025\n",
      "\n",
      "EPOCH 46 of 50\n",
      "\n",
      "Training loss: 0.005233997737781869\n",
      "Validation loss: 0.0042775780117760105\n",
      "\n",
      "EPOCH 47 of 50\n",
      "\n",
      "Training loss: 0.005402165465056896\n",
      "Validation loss: 0.004261728220929702\n",
      "\n",
      "EPOCH 48 of 50\n",
      "\n",
      "Training loss: 0.005216729335693849\n",
      "Validation loss: 0.004281652082378666\n",
      "\n",
      "EPOCH 49 of 50\n",
      "\n",
      "Training loss: 0.005471330239541001\n",
      "Validation loss: 0.004327218979597092\n",
      "\n",
      "EPOCH 50 of 50\n",
      "\n",
      "Training loss: 0.0055553424172103405\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-30 05:18:16,612] Trial 25 finished with value: 0.0042476180630425615 and parameters: {'lr': 0.00010509445977582517, 'weight_decay': 3.5176434113822414e-05, 'hidden_size': 363, 'dropout_rate': 0.4474841450783594}. Best is trial 16 with value: 0.0032828766076515117.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.0042476180630425615\n",
      "learning rate: 0.00039806284963471504\n",
      "weight_decay: 5.7726456345760316e-05\n",
      "hidden_size: 401\n",
      "dropout_rate: 0.4833483980930048\n",
      "\n",
      "EPOCH 1 of 50\n",
      "\n",
      "Training loss: 0.005254924297332764\n",
      "Validation loss: 0.003303724341094494\n",
      "\n",
      "EPOCH 2 of 50\n",
      "\n",
      "Training loss: 0.005106466123834252\n",
      "Validation loss: 0.003311995416879654\n",
      "\n",
      "EPOCH 3 of 50\n",
      "\n",
      "Training loss: 0.00497379790370663\n",
      "Validation loss: 0.003329215105623007\n",
      "\n",
      "EPOCH 4 of 50\n",
      "\n",
      "Training loss: 0.005241885212146574\n",
      "Validation loss: 0.0033490618225187063\n",
      "\n",
      "EPOCH 5 of 50\n",
      "\n",
      "Training loss: 0.00487524193401138\n",
      "Validation loss: 0.003353589835266272\n",
      "\n",
      "EPOCH 6 of 50\n",
      "\n",
      "Training loss: 0.004995735362172127\n",
      "Validation loss: 0.003354347233350078\n",
      "\n",
      "EPOCH 7 of 50\n",
      "\n",
      "Training loss: 0.005180668991266025\n",
      "Validation loss: 0.0033641315530985594\n",
      "\n",
      "EPOCH 8 of 50\n",
      "\n",
      "Training loss: 0.0053804306209915215\n",
      "Validation loss: 0.0033522941327343383\n",
      "\n",
      "EPOCH 9 of 50\n",
      "\n",
      "Training loss: 0.0051531921037369305\n",
      "Validation loss: 0.0033443388529121876\n",
      "\n",
      "EPOCH 10 of 50\n",
      "\n",
      "Training loss: 0.005353465417606963\n",
      "Validation loss: 0.003356741896520058\n",
      "\n",
      "EPOCH 11 of 50\n",
      "\n",
      "Training loss: 0.005365543507246507\n",
      "Validation loss: 0.0033604169730097055\n",
      "\n",
      "EPOCH 12 of 50\n",
      "\n",
      "Training loss: 0.005318372562113736\n",
      "Validation loss: 0.003344240520770351\n",
      "\n",
      "EPOCH 13 of 50\n",
      "\n",
      "Training loss: 0.005127226235345006\n",
      "Validation loss: 0.0033527023624628782\n",
      "\n",
      "EPOCH 14 of 50\n",
      "\n",
      "Training loss: 0.005175273296319776\n",
      "Validation loss: 0.003344103383521239\n",
      "\n",
      "EPOCH 15 of 50\n",
      "\n",
      "Training loss: 0.005303951911628246\n",
      "Validation loss: 0.003345901301751534\n",
      "\n",
      "EPOCH 16 of 50\n",
      "\n",
      "Training loss: 0.0051904266648408436\n",
      "Validation loss: 0.0033606862804541984\n",
      "\n",
      "EPOCH 17 of 50\n",
      "\n",
      "Training loss: 0.00507120625115931\n",
      "Validation loss: 0.003357100378101071\n",
      "\n",
      "EPOCH 18 of 50\n",
      "\n",
      "Training loss: 0.005029592663049698\n",
      "Validation loss: 0.003352967711786429\n",
      "\n",
      "EPOCH 19 of 50\n",
      "\n",
      "Training loss: 0.005279663484543562\n",
      "Validation loss: 0.003347603293756644\n",
      "\n",
      "EPOCH 20 of 50\n",
      "\n",
      "Training loss: 0.005202755202642745\n",
      "Validation loss: 0.003346098974967996\n",
      "\n",
      "EPOCH 21 of 50\n",
      "\n",
      "Training loss: 0.005025750336547692\n",
      "Validation loss: 0.0033368869529416165\n",
      "\n",
      "EPOCH 22 of 50\n",
      "\n",
      "Training loss: 0.005610539422680934\n",
      "Validation loss: 0.003346546165024241\n",
      "\n",
      "EPOCH 23 of 50\n",
      "\n",
      "Training loss: 0.005046102140719692\n",
      "Validation loss: 0.0033634561114013195\n",
      "\n",
      "EPOCH 24 of 50\n",
      "\n",
      "Training loss: 0.004967279939187897\n",
      "Validation loss: 0.0033614207059144974\n",
      "\n",
      "EPOCH 25 of 50\n",
      "\n",
      "Training loss: 0.00515526463277638\n",
      "Validation loss: 0.0033478386079271636\n",
      "\n",
      "EPOCH 26 of 50\n",
      "\n",
      "Training loss: 0.005535136546111769\n",
      "Validation loss: 0.003361631262426575\n",
      "\n",
      "EPOCH 27 of 50\n",
      "\n",
      "Training loss: 0.004900156054645777\n",
      "Validation loss: 0.003356094860161344\n",
      "\n",
      "EPOCH 28 of 50\n",
      "\n",
      "Training loss: 0.004975781413829989\n",
      "Validation loss: 0.0033627689505616822\n",
      "\n",
      "EPOCH 29 of 50\n",
      "\n",
      "Training loss: 0.004930086718458269\n",
      "Validation loss: 0.0033603495297332606\n",
      "\n",
      "EPOCH 30 of 50\n",
      "\n",
      "Training loss: 0.005142753193568852\n",
      "Validation loss: 0.0033597484386215606\n",
      "\n",
      "EPOCH 31 of 50\n",
      "\n",
      "Training loss: 0.0050768111315038465\n",
      "Validation loss: 0.0033631527330726385\n",
      "\n",
      "EPOCH 32 of 50\n",
      "\n",
      "Training loss: 0.0052844756800267435\n",
      "Validation loss: 0.003353975790863236\n",
      "\n",
      "EPOCH 33 of 50\n",
      "\n",
      "Training loss: 0.005289245112281706\n",
      "Validation loss: 0.003346943762153387\n",
      "\n",
      "EPOCH 34 of 50\n",
      "\n",
      "Training loss: 0.005189238892247279\n",
      "Validation loss: 0.003341483340288202\n",
      "\n",
      "EPOCH 35 of 50\n",
      "\n",
      "Training loss: 0.005042599281296134\n",
      "Validation loss: 0.003342883583779136\n",
      "\n",
      "EPOCH 36 of 50\n",
      "\n",
      "Training loss: 0.005269411123461193\n",
      "Validation loss: 0.0033469577319920063\n",
      "\n",
      "EPOCH 37 of 50\n",
      "\n",
      "Training loss: 0.005181306507438421\n",
      "Validation loss: 0.003358277337004741\n",
      "\n",
      "EPOCH 38 of 50\n",
      "\n",
      "Training loss: 0.005115625914186239\n",
      "Validation loss: 0.0033604939623425403\n",
      "\n",
      "EPOCH 39 of 50\n",
      "\n",
      "Training loss: 0.005213024327531457\n",
      "Validation loss: 0.0033498116148014865\n",
      "\n",
      "EPOCH 40 of 50\n",
      "\n",
      "Training loss: 0.004987045346448819\n",
      "Validation loss: 0.0033528557202468314\n",
      "\n",
      "EPOCH 41 of 50\n",
      "\n",
      "Training loss: 0.0048665140186332995\n",
      "Validation loss: 0.0033514644019305706\n",
      "\n",
      "EPOCH 42 of 50\n",
      "\n",
      "Training loss: 0.005166776995691989\n",
      "Validation loss: 0.003350386939321955\n",
      "\n",
      "EPOCH 43 of 50\n",
      "\n",
      "Training loss: 0.0054112595195571584\n",
      "Validation loss: 0.0033442815765738487\n",
      "\n",
      "EPOCH 44 of 50\n",
      "\n",
      "Training loss: 0.00513611501082778\n",
      "Validation loss: 0.003369554178789258\n",
      "\n",
      "EPOCH 45 of 50\n",
      "\n",
      "Training loss: 0.005359836595339907\n",
      "Validation loss: 0.003359406410406033\n",
      "\n",
      "EPOCH 46 of 50\n",
      "\n",
      "Training loss: 0.005171992609070407\n",
      "Validation loss: 0.0033535749341050782\n",
      "\n",
      "EPOCH 47 of 50\n",
      "\n",
      "Training loss: 0.005284057878371742\n",
      "Validation loss: 0.0033659403367588916\n",
      "\n",
      "EPOCH 48 of 50\n",
      "\n",
      "Training loss: 0.005270487499526805\n",
      "Validation loss: 0.0033532708572844663\n",
      "\n",
      "EPOCH 49 of 50\n",
      "\n",
      "Training loss: 0.005261032738619381\n",
      "Validation loss: 0.0033606168969223895\n",
      "\n",
      "EPOCH 50 of 50\n",
      "\n",
      "Training loss: 0.0050704260098023545\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-30 05:22:26,054] Trial 26 finished with value: 0.003357880205536882 and parameters: {'lr': 0.00039806284963471504, 'weight_decay': 5.7726456345760316e-05, 'hidden_size': 401, 'dropout_rate': 0.4833483980930048}. Best is trial 16 with value: 0.0032828766076515117.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.003357880205536882\n",
      "learning rate: 3.1767424990923905e-05\n",
      "weight_decay: 0.0002718480823143746\n",
      "hidden_size: 318\n",
      "dropout_rate: 0.4989443793254028\n",
      "\n",
      "EPOCH 1 of 50\n",
      "\n",
      "Training loss: 0.006068101618438959\n",
      "Validation loss: 0.003308818986018499\n",
      "\n",
      "EPOCH 2 of 50\n",
      "\n",
      "Training loss: 0.005451931411193477\n",
      "Validation loss: 0.0032990547673155866\n",
      "\n",
      "EPOCH 3 of 50\n",
      "\n",
      "Training loss: 0.0056135679802132975\n",
      "Validation loss: 0.003290643605093161\n",
      "\n",
      "EPOCH 4 of 50\n",
      "\n",
      "Training loss: 0.005902912881639268\n",
      "Validation loss: 0.003294964243347446\n",
      "\n",
      "EPOCH 5 of 50\n",
      "\n",
      "Training loss: 0.00574991422601872\n",
      "Validation loss: 0.0032950551249086857\n",
      "\n",
      "EPOCH 6 of 50\n",
      "\n",
      "Training loss: 0.005841028617901934\n",
      "Validation loss: 0.0032919093500822783\n",
      "\n",
      "EPOCH 7 of 50\n",
      "\n",
      "Training loss: 0.005717865915762054\n",
      "Validation loss: 0.0032904959128548703\n",
      "\n",
      "EPOCH 8 of 50\n",
      "\n",
      "Training loss: 0.005832489838616716\n",
      "Validation loss: 0.0032986176665872335\n",
      "\n",
      "EPOCH 9 of 50\n",
      "\n",
      "Training loss: 0.005643495462006993\n",
      "Validation loss: 0.003295519311601917\n",
      "\n",
      "EPOCH 10 of 50\n",
      "\n",
      "Training loss: 0.0058607489336282015\n",
      "Validation loss: 0.0032919072546064854\n",
      "\n",
      "EPOCH 11 of 50\n",
      "\n",
      "Training loss: 0.00562846153560612\n",
      "Validation loss: 0.003296739887446165\n",
      "\n",
      "EPOCH 12 of 50\n",
      "\n",
      "Training loss: 0.00562175673743089\n",
      "Validation loss: 0.00329502757328252\n",
      "\n",
      "EPOCH 13 of 50\n",
      "\n",
      "Training loss: 0.00560867331094212\n",
      "Validation loss: 0.0032942105705539384\n",
      "\n",
      "EPOCH 14 of 50\n",
      "\n",
      "Training loss: 0.005725119573374589\n",
      "Validation loss: 0.003293927681321899\n",
      "\n",
      "EPOCH 15 of 50\n",
      "\n",
      "Training loss: 0.005648450874206092\n",
      "Validation loss: 0.003290204331278801\n",
      "\n",
      "EPOCH 16 of 50\n",
      "\n",
      "Training loss: 0.005717942646394174\n",
      "Validation loss: 0.003291396346564094\n",
      "\n",
      "EPOCH 17 of 50\n",
      "\n",
      "Training loss: 0.005709946491859025\n",
      "Validation loss: 0.0032893233777334294\n",
      "\n",
      "EPOCH 18 of 50\n",
      "\n",
      "Training loss: 0.005446620388991303\n",
      "Validation loss: 0.0032932052854448557\n",
      "\n",
      "EPOCH 19 of 50\n",
      "\n",
      "Training loss: 0.005582755224572288\n",
      "Validation loss: 0.003294432768598199\n",
      "\n",
      "EPOCH 20 of 50\n",
      "\n",
      "Training loss: 0.005499337799847126\n",
      "Validation loss: 0.003289770878230532\n",
      "\n",
      "EPOCH 21 of 50\n",
      "\n",
      "Training loss: 0.0058801933709118105\n",
      "Validation loss: 0.003293963304410378\n",
      "\n",
      "EPOCH 22 of 50\n",
      "\n",
      "Training loss: 0.00567871689175566\n",
      "Validation loss: 0.0032920762120435634\n",
      "\n",
      "EPOCH 23 of 50\n",
      "\n",
      "Training loss: 0.005568185252034002\n",
      "Validation loss: 0.0032984158800294003\n",
      "\n",
      "EPOCH 24 of 50\n",
      "\n",
      "Training loss: 0.005561724796684252\n",
      "Validation loss: 0.0033022211864590645\n",
      "\n",
      "EPOCH 25 of 50\n",
      "\n",
      "Training loss: 0.0060215820040967726\n",
      "Validation loss: 0.0032943986977140107\n",
      "\n",
      "EPOCH 26 of 50\n",
      "\n",
      "Training loss: 0.00555841660954886\n",
      "Validation loss: 0.0032925665533790984\n",
      "\n",
      "EPOCH 27 of 50\n",
      "\n",
      "Training loss: 0.00594488044993745\n",
      "Validation loss: 0.003292065424223741\n",
      "\n",
      "EPOCH 28 of 50\n",
      "\n",
      "Training loss: 0.005672461094541682\n",
      "Validation loss: 0.003289460359762112\n",
      "\n",
      "EPOCH 29 of 50\n",
      "\n",
      "Training loss: 0.005639607707659404\n",
      "Validation loss: 0.003289488066608707\n",
      "\n",
      "EPOCH 30 of 50\n",
      "\n",
      "Training loss: 0.005547012047221263\n",
      "Validation loss: 0.0032895058393478394\n",
      "\n",
      "EPOCH 31 of 50\n",
      "\n",
      "Training loss: 0.005815684640159209\n",
      "Validation loss: 0.0032900994022687278\n",
      "\n",
      "EPOCH 32 of 50\n",
      "\n",
      "Training loss: 0.0059573725383314825\n",
      "Validation loss: 0.003288351853067676\n",
      "\n",
      "EPOCH 33 of 50\n",
      "\n",
      "Training loss: 0.005756874206579394\n",
      "Validation loss: 0.003288883793478211\n",
      "\n",
      "EPOCH 34 of 50\n",
      "\n",
      "Training loss: 0.00570539597214924\n",
      "Validation loss: 0.003291865655531486\n",
      "\n",
      "EPOCH 35 of 50\n",
      "\n",
      "Training loss: 0.005714591592550278\n",
      "Validation loss: 0.0032922638735423484\n",
      "\n",
      "EPOCH 36 of 50\n",
      "\n",
      "Training loss: 0.005838148502839936\n",
      "Validation loss: 0.003296162001788616\n",
      "\n",
      "EPOCH 37 of 50\n",
      "\n",
      "Training loss: 0.005676586439626085\n",
      "Validation loss: 0.003290486056357622\n",
      "\n",
      "EPOCH 38 of 50\n",
      "\n",
      "Training loss: 0.005579516343358491\n",
      "Validation loss: 0.00329300737939775\n",
      "\n",
      "EPOCH 39 of 50\n",
      "\n",
      "Training loss: 0.005654786765161488\n",
      "Validation loss: 0.003294560049350063\n",
      "\n",
      "EPOCH 40 of 50\n",
      "\n",
      "Training loss: 0.005506525023116006\n",
      "Validation loss: 0.0032938309013843536\n",
      "\n",
      "EPOCH 41 of 50\n",
      "\n",
      "Training loss: 0.005752673734807306\n",
      "Validation loss: 0.0032917979018141827\n",
      "\n",
      "EPOCH 42 of 50\n",
      "\n",
      "Training loss: 0.0058008467054201495\n",
      "Validation loss: 0.0032927635281036296\n",
      "\n",
      "EPOCH 43 of 50\n",
      "\n",
      "Training loss: 0.005671599207239019\n",
      "Validation loss: 0.003288847394287586\n",
      "\n",
      "EPOCH 44 of 50\n",
      "\n",
      "Training loss: 0.005484534556873971\n",
      "Validation loss: 0.003291121373573939\n",
      "\n",
      "EPOCH 45 of 50\n",
      "\n",
      "Training loss: 0.005757396005921894\n",
      "Validation loss: 0.0032959839639564357\n",
      "\n",
      "EPOCH 46 of 50\n",
      "\n",
      "Training loss: 0.005754745513614681\n",
      "Validation loss: 0.0032927419524639845\n",
      "\n",
      "EPOCH 47 of 50\n",
      "\n",
      "Training loss: 0.005706357459227244\n",
      "Validation loss: 0.003294127294793725\n",
      "\n",
      "EPOCH 48 of 50\n",
      "\n",
      "Training loss: 0.00573006800065438\n",
      "Validation loss: 0.003292174699405829\n",
      "\n",
      "EPOCH 49 of 50\n",
      "\n",
      "Training loss: 0.005631526828640037\n",
      "Validation loss: 0.003292840595046679\n",
      "\n",
      "EPOCH 50 of 50\n",
      "\n",
      "Training loss: 0.005672071956925922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-30 05:26:37,907] Trial 27 finished with value: 0.0032910531541953483 and parameters: {'lr': 3.1767424990923905e-05, 'weight_decay': 0.0002718480823143746, 'hidden_size': 318, 'dropout_rate': 0.4989443793254028}. Best is trial 16 with value: 0.0032828766076515117.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.0032910531541953483\n",
      "learning rate: 1.0052879980766115e-05\n",
      "weight_decay: 0.0003842507045187802\n",
      "hidden_size: 313\n",
      "dropout_rate: 0.4970791058413613\n",
      "\n",
      "EPOCH 1 of 50\n",
      "\n",
      "Training loss: 0.005511445148537557\n",
      "Validation loss: 0.003349308700611194\n",
      "\n",
      "EPOCH 2 of 50\n",
      "\n",
      "Training loss: 0.005332245181004207\n",
      "Validation loss: 0.003493847015003363\n",
      "\n",
      "EPOCH 3 of 50\n",
      "\n",
      "Training loss: 0.005454053584900167\n",
      "Validation loss: 0.003600264200940728\n",
      "\n",
      "EPOCH 4 of 50\n",
      "\n",
      "Training loss: 0.005379955594738324\n",
      "Validation loss: 0.0036701963593562445\n",
      "\n",
      "EPOCH 5 of 50\n",
      "\n",
      "Training loss: 0.005345926102664735\n",
      "Validation loss: 0.003750584398706754\n",
      "\n",
      "EPOCH 6 of 50\n",
      "\n",
      "Training loss: 0.005627346975314949\n",
      "Validation loss: 0.003754633323599895\n",
      "\n",
      "EPOCH 7 of 50\n",
      "\n",
      "Training loss: 0.005498191341757774\n",
      "Validation loss: 0.00371531016814212\n",
      "\n",
      "EPOCH 8 of 50\n",
      "\n",
      "Training loss: 0.005561444442719221\n",
      "Validation loss: 0.003725950994218389\n",
      "\n",
      "EPOCH 9 of 50\n",
      "\n",
      "Training loss: 0.005440509619398249\n",
      "Validation loss: 0.0037645366974174976\n",
      "\n",
      "EPOCH 10 of 50\n",
      "\n",
      "Training loss: 0.005682197192476856\n",
      "Validation loss: 0.0037172255106270313\n",
      "\n",
      "EPOCH 11 of 50\n",
      "\n",
      "Training loss: 0.005310975377344423\n",
      "Validation loss: 0.003645901568233967\n",
      "\n",
      "EPOCH 12 of 50\n",
      "\n",
      "Training loss: 0.00569701065412826\n",
      "Validation loss: 0.003673677099868655\n",
      "\n",
      "EPOCH 13 of 50\n",
      "\n",
      "Training loss: 0.005704937037080526\n",
      "Validation loss: 0.0036906315945088863\n",
      "\n",
      "EPOCH 14 of 50\n",
      "\n",
      "Training loss: 0.005616919758419196\n",
      "Validation loss: 0.0037392452359199524\n",
      "\n",
      "EPOCH 15 of 50\n",
      "\n",
      "Training loss: 0.005177219398319721\n",
      "Validation loss: 0.0037940307520329952\n",
      "\n",
      "EPOCH 16 of 50\n",
      "\n",
      "Training loss: 0.00562282284307811\n",
      "Validation loss: 0.003777284330377976\n",
      "\n",
      "EPOCH 17 of 50\n",
      "\n",
      "Training loss: 0.005361826210800145\n",
      "Validation loss: 0.003745125994707147\n",
      "\n",
      "EPOCH 18 of 50\n",
      "\n",
      "Training loss: 0.005259642351625694\n",
      "Validation loss: 0.003681728538746635\n",
      "\n",
      "EPOCH 19 of 50\n",
      "\n",
      "Training loss: 0.005479178520747357\n",
      "Validation loss: 0.003683009029676517\n",
      "\n",
      "EPOCH 20 of 50\n",
      "\n",
      "Training loss: 0.005626135040074587\n",
      "Validation loss: 0.0037465912755578756\n",
      "\n",
      "EPOCH 21 of 50\n",
      "\n",
      "Training loss: 0.005411741220288807\n",
      "Validation loss: 0.0037295559886842966\n",
      "\n",
      "EPOCH 22 of 50\n",
      "\n",
      "Training loss: 0.005247263005003333\n",
      "Validation loss: 0.0037321249643961587\n",
      "\n",
      "EPOCH 23 of 50\n",
      "\n",
      "Training loss: 0.005308146950685316\n",
      "Validation loss: 0.003708256253351768\n",
      "\n",
      "EPOCH 24 of 50\n",
      "\n",
      "Training loss: 0.00561642258738478\n",
      "Validation loss: 0.0037177889607846737\n",
      "\n",
      "EPOCH 25 of 50\n",
      "\n",
      "Training loss: 0.005488852866821819\n",
      "Validation loss: 0.0037437884602695704\n",
      "\n",
      "EPOCH 26 of 50\n",
      "\n",
      "Training loss: 0.005489621104465591\n",
      "Validation loss: 0.003735638844470183\n",
      "\n",
      "EPOCH 27 of 50\n",
      "\n",
      "Training loss: 0.00561208479727308\n",
      "Validation loss: 0.003723568826292952\n",
      "\n",
      "EPOCH 28 of 50\n",
      "\n",
      "Training loss: 0.005802686274465587\n",
      "Validation loss: 0.003729717107489705\n",
      "\n",
      "EPOCH 29 of 50\n",
      "\n",
      "Training loss: 0.005327125519721044\n",
      "Validation loss: 0.003730120680605372\n",
      "\n",
      "EPOCH 30 of 50\n",
      "\n",
      "Training loss: 0.005440609115693305\n",
      "Validation loss: 0.003744235960766673\n",
      "\n",
      "EPOCH 31 of 50\n",
      "\n",
      "Training loss: 0.005683590709749196\n",
      "Validation loss: 0.0037359267783661685\n",
      "\n",
      "EPOCH 32 of 50\n",
      "\n",
      "Training loss: 0.005315300672211581\n",
      "Validation loss: 0.003743982563416163\n",
      "\n",
      "EPOCH 33 of 50\n",
      "\n",
      "Training loss: 0.0056823245249688625\n",
      "Validation loss: 0.0037696503568440676\n",
      "\n",
      "EPOCH 34 of 50\n",
      "\n",
      "Training loss: 0.0054091849985222025\n",
      "Validation loss: 0.0037439309526234865\n",
      "\n",
      "EPOCH 35 of 50\n",
      "\n",
      "Training loss: 0.0054183304827246405\n",
      "Validation loss: 0.003747417979563276\n",
      "\n",
      "EPOCH 36 of 50\n",
      "\n",
      "Training loss: 0.005488911436663734\n",
      "Validation loss: 0.00375936560643216\n",
      "\n",
      "EPOCH 37 of 50\n",
      "\n",
      "Training loss: 0.005353562482115295\n",
      "Validation loss: 0.003740165072182814\n",
      "\n",
      "EPOCH 38 of 50\n",
      "\n",
      "Training loss: 0.005431700575475891\n",
      "Validation loss: 0.003718393466745814\n",
      "\n",
      "EPOCH 39 of 50\n",
      "\n",
      "Training loss: 0.005662471159464783\n",
      "Validation loss: 0.0037159717176109552\n",
      "\n",
      "EPOCH 40 of 50\n",
      "\n",
      "Training loss: 0.005572015316122108\n",
      "Validation loss: 0.0037358305417001247\n",
      "\n",
      "EPOCH 41 of 50\n",
      "\n",
      "Training loss: 0.005430946747461955\n",
      "Validation loss: 0.0037294847425073385\n",
      "\n",
      "EPOCH 42 of 50\n",
      "\n",
      "Training loss: 0.0053103272803127766\n",
      "Validation loss: 0.003706134778137008\n",
      "\n",
      "EPOCH 43 of 50\n",
      "\n",
      "Training loss: 0.005357143210454119\n",
      "Validation loss: 0.0037146759374688068\n",
      "\n",
      "EPOCH 44 of 50\n",
      "\n",
      "Training loss: 0.005413213279098272\n",
      "Validation loss: 0.0037067142936090627\n",
      "\n",
      "EPOCH 45 of 50\n",
      "\n",
      "Training loss: 0.005314366115878026\n",
      "Validation loss: 0.0036710720354070268\n",
      "\n",
      "EPOCH 46 of 50\n",
      "\n",
      "Training loss: 0.0053600818953580326\n",
      "Validation loss: 0.0036868381624420485\n",
      "\n",
      "EPOCH 47 of 50\n",
      "\n",
      "Training loss: 0.005448943469673395\n",
      "Validation loss: 0.0037131710754086575\n",
      "\n",
      "EPOCH 48 of 50\n",
      "\n",
      "Training loss: 0.005482522796632515\n",
      "Validation loss: 0.003741801716387272\n",
      "\n",
      "EPOCH 49 of 50\n",
      "\n",
      "Training loss: 0.0053359319766362505\n",
      "Validation loss: 0.0037303189747035503\n",
      "\n",
      "EPOCH 50 of 50\n",
      "\n",
      "Training loss: 0.005344994314428832\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-30 05:30:53,554] Trial 28 finished with value: 0.0036859439375499883 and parameters: {'lr': 1.0052879980766115e-05, 'weight_decay': 0.0003842507045187802, 'hidden_size': 313, 'dropout_rate': 0.4970791058413613}. Best is trial 16 with value: 0.0032828766076515117.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.0036859439375499883\n",
      "learning rate: 6.722356173589858e-05\n",
      "weight_decay: 0.00012456912567505678\n",
      "hidden_size: 327\n",
      "dropout_rate: 0.4498423192570181\n",
      "\n",
      "EPOCH 1 of 50\n",
      "\n",
      "Training loss: 0.005559450532827113\n",
      "Validation loss: 0.003313058754429221\n",
      "\n",
      "EPOCH 2 of 50\n",
      "\n",
      "Training loss: 0.005581876418242852\n",
      "Validation loss: 0.003303318362062176\n",
      "\n",
      "EPOCH 3 of 50\n",
      "\n",
      "Training loss: 0.005519248182988829\n",
      "Validation loss: 0.0033035746309906244\n",
      "\n",
      "EPOCH 4 of 50\n",
      "\n",
      "Training loss: 0.005784962636729081\n",
      "Validation loss: 0.0033051942785580954\n",
      "\n",
      "EPOCH 5 of 50\n",
      "\n",
      "Training loss: 0.005397404802756177\n",
      "Validation loss: 0.0033039896128078303\n",
      "\n",
      "EPOCH 6 of 50\n",
      "\n",
      "Training loss: 0.005832638539787795\n",
      "Validation loss: 0.003304975184922417\n",
      "\n",
      "EPOCH 7 of 50\n",
      "\n",
      "Training loss: 0.005598289995557732\n",
      "Validation loss: 0.0033054365776479244\n",
      "\n",
      "EPOCH 8 of 50\n",
      "\n",
      "Training loss: 0.005635684795884622\n",
      "Validation loss: 0.003304980772857865\n",
      "\n",
      "EPOCH 9 of 50\n",
      "\n",
      "Training loss: 0.005589731296317445\n",
      "Validation loss: 0.003302131391440829\n",
      "\n",
      "EPOCH 10 of 50\n",
      "\n",
      "Training loss: 0.005681236610851354\n",
      "Validation loss: 0.003300607902929187\n",
      "\n",
      "EPOCH 11 of 50\n",
      "\n",
      "Training loss: 0.005459901876747608\n",
      "Validation loss: 0.0033062022800246873\n",
      "\n",
      "EPOCH 12 of 50\n",
      "\n",
      "Training loss: 0.005504446362869607\n",
      "Validation loss: 0.0033088466928650937\n",
      "\n",
      "EPOCH 13 of 50\n",
      "\n",
      "Training loss: 0.005606856611039903\n",
      "Validation loss: 0.003310405028363069\n",
      "\n",
      "EPOCH 14 of 50\n",
      "\n",
      "Training loss: 0.005932691558781598\n",
      "Validation loss: 0.003309023411323627\n",
      "\n",
      "EPOCH 15 of 50\n",
      "\n",
      "Training loss: 0.005537273000097937\n",
      "Validation loss: 0.003308194146181146\n",
      "\n",
      "EPOCH 16 of 50\n",
      "\n",
      "Training loss: 0.005871242533127467\n",
      "Validation loss: 0.0033083255402743816\n",
      "\n",
      "EPOCH 17 of 50\n",
      "\n",
      "Training loss: 0.005734631636490424\n",
      "Validation loss: 0.0033068270422518253\n",
      "\n",
      "EPOCH 18 of 50\n",
      "\n",
      "Training loss: 0.005680247934328185\n",
      "Validation loss: 0.0033089645051707826\n",
      "\n",
      "EPOCH 19 of 50\n",
      "\n",
      "Training loss: 0.005514960373855299\n",
      "Validation loss: 0.0033098941979308925\n",
      "\n",
      "EPOCH 20 of 50\n",
      "\n",
      "Training loss: 0.005778076748053233\n",
      "Validation loss: 0.0033091582202663026\n",
      "\n",
      "EPOCH 21 of 50\n",
      "\n",
      "Training loss: 0.005690187371025483\n",
      "Validation loss: 0.003311091490710775\n",
      "\n",
      "EPOCH 22 of 50\n",
      "\n",
      "Training loss: 0.005700721560666959\n",
      "Validation loss: 0.0033123744651675224\n",
      "\n",
      "EPOCH 23 of 50\n",
      "\n",
      "Training loss: 0.0054713089226020705\n",
      "Validation loss: 0.003310474722335736\n",
      "\n",
      "EPOCH 24 of 50\n",
      "\n",
      "Training loss: 0.005505705071199272\n",
      "Validation loss: 0.003305455514540275\n",
      "\n",
      "EPOCH 25 of 50\n",
      "\n",
      "Training loss: 0.0055638122786250375\n",
      "Validation loss: 0.003308463841676712\n",
      "\n",
      "EPOCH 26 of 50\n",
      "\n",
      "Training loss: 0.005443839511523644\n",
      "Validation loss: 0.0033137730788439512\n",
      "\n",
      "EPOCH 27 of 50\n",
      "\n",
      "Training loss: 0.005466178525239229\n",
      "Validation loss: 0.0033077035720149675\n",
      "\n",
      "EPOCH 28 of 50\n",
      "\n",
      "Training loss: 0.005853327922523022\n",
      "Validation loss: 0.003309578246747454\n",
      "\n",
      "EPOCH 29 of 50\n",
      "\n",
      "Training loss: 0.005669305644308527\n",
      "Validation loss: 0.0033071481933196387\n",
      "\n",
      "EPOCH 30 of 50\n",
      "\n",
      "Training loss: 0.005598581447783444\n",
      "Validation loss: 0.0033080248006929955\n",
      "\n",
      "EPOCH 31 of 50\n",
      "\n",
      "Training loss: 0.0056514678419464165\n",
      "Validation loss: 0.003306993438551823\n",
      "\n",
      "EPOCH 32 of 50\n",
      "\n",
      "Training loss: 0.005371037508464522\n",
      "Validation loss: 0.0033077409801383815\n",
      "\n",
      "EPOCH 33 of 50\n",
      "\n",
      "Training loss: 0.005159337875536746\n",
      "Validation loss: 0.0033055967651307583\n",
      "\n",
      "EPOCH 34 of 50\n",
      "\n",
      "Training loss: 0.005560607235464785\n",
      "Validation loss: 0.003306204918771982\n",
      "\n",
      "EPOCH 35 of 50\n",
      "\n",
      "Training loss: 0.005422187606907553\n",
      "Validation loss: 0.003307578464349111\n",
      "\n",
      "EPOCH 36 of 50\n",
      "\n",
      "Training loss: 0.005785523189438714\n",
      "Validation loss: 0.0033033565462877354\n",
      "\n",
      "EPOCH 37 of 50\n",
      "\n",
      "Training loss: 0.005662198747611708\n",
      "Validation loss: 0.0033065886236727238\n",
      "\n",
      "EPOCH 38 of 50\n",
      "\n",
      "Training loss: 0.005531242375986444\n",
      "Validation loss: 0.003308782276387016\n",
      "\n",
      "EPOCH 39 of 50\n",
      "\n",
      "Training loss: 0.005741052070839537\n",
      "Validation loss: 0.003305214224383235\n",
      "\n",
      "EPOCH 40 of 50\n",
      "\n",
      "Training loss: 0.005693825065261788\n",
      "Validation loss: 0.0033045208547264338\n",
      "\n",
      "EPOCH 41 of 50\n",
      "\n",
      "Training loss: 0.005609323425839345\n",
      "Validation loss: 0.0033066801261156797\n",
      "\n",
      "EPOCH 42 of 50\n",
      "\n",
      "Training loss: 0.005484723072085116\n",
      "Validation loss: 0.0033080962020903826\n",
      "\n",
      "EPOCH 43 of 50\n",
      "\n",
      "Training loss: 0.0056154913682904505\n",
      "Validation loss: 0.0033065220341086388\n",
      "\n",
      "EPOCH 44 of 50\n",
      "\n",
      "Training loss: 0.005529663215080897\n",
      "Validation loss: 0.00330821688597401\n",
      "\n",
      "EPOCH 45 of 50\n",
      "\n",
      "Training loss: 0.005786051145858235\n",
      "Validation loss: 0.003306361691405376\n",
      "\n",
      "EPOCH 46 of 50\n",
      "\n",
      "Training loss: 0.005551860202103853\n",
      "Validation loss: 0.003306648461148143\n",
      "\n",
      "EPOCH 47 of 50\n",
      "\n",
      "Training loss: 0.00543189648952749\n",
      "Validation loss: 0.0033069263057162366\n",
      "\n",
      "EPOCH 48 of 50\n",
      "\n",
      "Training loss: 0.005396178561366267\n",
      "Validation loss: 0.0033069591348369918\n",
      "\n",
      "EPOCH 49 of 50\n",
      "\n",
      "Training loss: 0.005612470054378112\n",
      "Validation loss: 0.003307503182440996\n",
      "\n",
      "EPOCH 50 of 50\n",
      "\n",
      "Training loss: 0.0053225117218163275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-30 05:34:57,760] Trial 29 finished with value: 0.0033087116510917744 and parameters: {'lr': 6.722356173589858e-05, 'weight_decay': 0.00012456912567505678, 'hidden_size': 327, 'dropout_rate': 0.4498423192570181}. Best is trial 16 with value: 0.0032828766076515117.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.0033087116510917744\n",
      "learning rate: 0.0001538124619654144\n",
      "weight_decay: 0.0002182814092530188\n",
      "hidden_size: 357\n",
      "dropout_rate: 0.41644496896625255\n",
      "\n",
      "EPOCH 1 of 50\n",
      "\n",
      "Training loss: 0.00510796651037203\n",
      "Validation loss: 0.0033448125080515942\n",
      "\n",
      "EPOCH 2 of 50\n",
      "\n",
      "Training loss: 0.005017693309734265\n",
      "Validation loss: 0.0034061980744202933\n",
      "\n",
      "EPOCH 3 of 50\n",
      "\n",
      "Training loss: 0.005332241455713908\n",
      "Validation loss: 0.0034912353536734977\n",
      "\n",
      "EPOCH 4 of 50\n",
      "\n",
      "Training loss: 0.005315119918021891\n",
      "Validation loss: 0.0035249619589497647\n",
      "\n",
      "EPOCH 5 of 50\n",
      "\n",
      "Training loss: 0.005564722129040294\n",
      "Validation loss: 0.003555453848093748\n",
      "\n",
      "EPOCH 6 of 50\n",
      "\n",
      "Training loss: 0.00494409934617579\n",
      "Validation loss: 0.0035245790301511684\n",
      "\n",
      "EPOCH 7 of 50\n",
      "\n",
      "Training loss: 0.005247527500614524\n",
      "Validation loss: 0.003537942422553897\n",
      "\n",
      "EPOCH 8 of 50\n",
      "\n",
      "Training loss: 0.005173962382185791\n",
      "Validation loss: 0.0035628780412177243\n",
      "\n",
      "EPOCH 9 of 50\n",
      "\n",
      "Training loss: 0.005543648575743039\n",
      "Validation loss: 0.0035612810558329024\n",
      "\n",
      "EPOCH 10 of 50\n",
      "\n",
      "Training loss: 0.0055191484021229874\n",
      "Validation loss: 0.003552794922143221\n",
      "\n",
      "EPOCH 11 of 50\n",
      "\n",
      "Training loss: 0.005142285954207182\n",
      "Validation loss: 0.0035220051649957895\n",
      "\n",
      "EPOCH 12 of 50\n",
      "\n",
      "Training loss: 0.005051272196902169\n",
      "Validation loss: 0.003565228544175625\n",
      "\n",
      "EPOCH 13 of 50\n",
      "\n",
      "Training loss: 0.0050814642001771266\n",
      "Validation loss: 0.003562450564155976\n",
      "\n",
      "EPOCH 14 of 50\n",
      "\n",
      "Training loss: 0.005242829935418235\n",
      "Validation loss: 0.003570564246426026\n",
      "\n",
      "EPOCH 15 of 50\n",
      "\n",
      "Training loss: 0.005133128052370416\n",
      "Validation loss: 0.003557224137087663\n",
      "\n",
      "EPOCH 16 of 50\n",
      "\n",
      "Training loss: 0.005499174456215567\n",
      "Validation loss: 0.003559676076595982\n",
      "\n",
      "EPOCH 17 of 50\n",
      "\n",
      "Training loss: 0.005312249840547641\n",
      "Validation loss: 0.0035840262038012347\n",
      "\n",
      "EPOCH 18 of 50\n",
      "\n",
      "Training loss: 0.005203620918715994\n",
      "Validation loss: 0.0035454843503733477\n",
      "\n",
      "EPOCH 19 of 50\n",
      "\n",
      "Training loss: 0.005201017768639658\n",
      "Validation loss: 0.00353146189202865\n",
      "\n",
      "EPOCH 20 of 50\n",
      "\n",
      "Training loss: 0.005098802218627598\n",
      "Validation loss: 0.0035792485966036716\n",
      "\n",
      "EPOCH 21 of 50\n",
      "\n",
      "Training loss: 0.005224921430150668\n",
      "Validation loss: 0.003578841375807921\n",
      "\n",
      "EPOCH 22 of 50\n",
      "\n",
      "Training loss: 0.00504522664575941\n",
      "Validation loss: 0.0035827637960513434\n",
      "\n",
      "EPOCH 23 of 50\n",
      "\n",
      "Training loss: 0.0050767213106155396\n",
      "Validation loss: 0.00357437483035028\n",
      "\n",
      "EPOCH 24 of 50\n",
      "\n",
      "Training loss: 0.005243233741364545\n",
      "Validation loss: 0.003567513000840942\n",
      "\n",
      "EPOCH 25 of 50\n",
      "\n",
      "Training loss: 0.005479999610947238\n",
      "Validation loss: 0.0035561272719254098\n",
      "\n",
      "EPOCH 26 of 50\n",
      "\n",
      "Training loss: 0.0050947172567248344\n",
      "Validation loss: 0.0036007750313729048\n",
      "\n",
      "EPOCH 27 of 50\n",
      "\n",
      "Training loss: 0.005351037097473939\n",
      "Validation loss: 0.0035609587406118712\n",
      "\n",
      "EPOCH 28 of 50\n",
      "\n",
      "Training loss: 0.00508156946549813\n",
      "Validation loss: 0.0035442771234860024\n",
      "\n",
      "EPOCH 29 of 50\n",
      "\n",
      "Training loss: 0.005210148377550973\n",
      "Validation loss: 0.0035831472681214413\n",
      "\n",
      "EPOCH 30 of 50\n",
      "\n",
      "Training loss: 0.005720345976038111\n",
      "Validation loss: 0.0035632598058631024\n",
      "\n",
      "EPOCH 31 of 50\n",
      "\n",
      "Training loss: 0.005300145285824935\n",
      "Validation loss: 0.0035740711415807405\n",
      "\n",
      "EPOCH 32 of 50\n",
      "\n",
      "Training loss: 0.005420469471977817\n",
      "Validation loss: 0.0035630647713939347\n",
      "\n",
      "EPOCH 33 of 50\n",
      "\n",
      "Training loss: 0.005415060402204593\n",
      "Validation loss: 0.0035771793530633054\n",
      "\n",
      "EPOCH 34 of 50\n",
      "\n",
      "Training loss: 0.004976409487426281\n",
      "Validation loss: 0.0035971259543051324\n",
      "\n",
      "EPOCH 35 of 50\n",
      "\n",
      "Training loss: 0.005076453141454194\n",
      "Validation loss: 0.003571067859108249\n",
      "\n",
      "EPOCH 36 of 50\n",
      "\n",
      "Training loss: 0.0054429760202765465\n",
      "Validation loss: 0.003567932561660806\n",
      "\n",
      "EPOCH 37 of 50\n",
      "\n",
      "Training loss: 0.005347517215543323\n",
      "Validation loss: 0.0035581958945840597\n",
      "\n",
      "EPOCH 38 of 50\n",
      "\n",
      "Training loss: 0.005197483477079206\n",
      "Validation loss: 0.003528111226235827\n",
      "\n",
      "EPOCH 39 of 50\n",
      "\n",
      "Training loss: 0.005114838869000475\n",
      "Validation loss: 0.003549494625379642\n",
      "\n",
      "EPOCH 40 of 50\n",
      "\n",
      "Training loss: 0.005179481684333748\n",
      "Validation loss: 0.0035588890314102173\n",
      "\n",
      "EPOCH 41 of 50\n",
      "\n",
      "Training loss: 0.0053917405505975085\n",
      "Validation loss: 0.0035732725324730077\n",
      "\n",
      "EPOCH 42 of 50\n",
      "\n",
      "Training loss: 0.0051626105171938734\n",
      "Validation loss: 0.0035653412342071533\n",
      "\n",
      "EPOCH 43 of 50\n",
      "\n",
      "Training loss: 0.005317375788258182\n",
      "Validation loss: 0.0035734386183321476\n",
      "\n",
      "EPOCH 44 of 50\n",
      "\n",
      "Training loss: 0.00526428921148181\n",
      "Validation loss: 0.0035580214268217483\n",
      "\n",
      "EPOCH 45 of 50\n",
      "\n",
      "Training loss: 0.005045352322566841\n",
      "Validation loss: 0.0035305339843034744\n",
      "\n",
      "EPOCH 46 of 50\n",
      "\n",
      "Training loss: 0.004952798157723414\n",
      "Validation loss: 0.0035236692832161984\n",
      "\n",
      "EPOCH 47 of 50\n",
      "\n",
      "Training loss: 0.005318902820969622\n",
      "Validation loss: 0.0035434449867655835\n",
      "\n",
      "EPOCH 48 of 50\n",
      "\n",
      "Training loss: 0.005291824746463034\n",
      "Validation loss: 0.0035668076016008854\n",
      "\n",
      "EPOCH 49 of 50\n",
      "\n",
      "Training loss: 0.00495857390989032\n",
      "Validation loss: 0.003565878917773565\n",
      "\n",
      "EPOCH 50 of 50\n",
      "\n",
      "Training loss: 0.00508307228382263\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-30 05:39:01,255] Trial 30 finished with value: 0.0035804536504050097 and parameters: {'lr': 0.0001538124619654144, 'weight_decay': 0.0002182814092530188, 'hidden_size': 357, 'dropout_rate': 0.41644496896625255}. Best is trial 16 with value: 0.0032828766076515117.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.0035804536504050097\n",
      "learning rate: 2.72793239518843e-05\n",
      "weight_decay: 7.031683618161533e-05\n",
      "hidden_size: 331\n",
      "dropout_rate: 0.4781868070024953\n",
      "\n",
      "EPOCH 1 of 50\n",
      "\n",
      "Training loss: 0.006011779161377085\n",
      "Validation loss: 0.0035297864427169165\n",
      "\n",
      "EPOCH 2 of 50\n",
      "\n",
      "Training loss: 0.005755716417398717\n",
      "Validation loss: 0.003928494639694691\n",
      "\n",
      "EPOCH 3 of 50\n",
      "\n",
      "Training loss: 0.0057122289047886925\n",
      "Validation loss: 0.004302649293094873\n",
      "\n",
      "EPOCH 4 of 50\n",
      "\n",
      "Training loss: 0.005624658583352963\n",
      "Validation loss: 0.0044224873806039495\n",
      "\n",
      "EPOCH 5 of 50\n",
      "\n",
      "Training loss: 0.0060214583451549215\n",
      "Validation loss: 0.004548098659142852\n",
      "\n",
      "EPOCH 6 of 50\n",
      "\n",
      "Training loss: 0.0055522848334577345\n",
      "Validation loss: 0.00458375954379638\n",
      "\n",
      "EPOCH 7 of 50\n",
      "\n",
      "Training loss: 0.005580406325558822\n",
      "Validation loss: 0.004597218551983436\n",
      "\n",
      "EPOCH 8 of 50\n",
      "\n",
      "Training loss: 0.005514614646219545\n",
      "Validation loss: 0.004496938704202573\n",
      "\n",
      "EPOCH 9 of 50\n",
      "\n",
      "Training loss: 0.005459993456800778\n",
      "Validation loss: 0.004524816060438752\n",
      "\n",
      "EPOCH 10 of 50\n",
      "\n",
      "Training loss: 0.006200382020324469\n",
      "Validation loss: 0.004606239497661591\n",
      "\n",
      "EPOCH 11 of 50\n",
      "\n",
      "Training loss: 0.005304708637090193\n",
      "Validation loss: 0.004560412761444847\n",
      "\n",
      "EPOCH 12 of 50\n",
      "\n",
      "Training loss: 0.005776890087872744\n",
      "Validation loss: 0.004543657414615154\n",
      "\n",
      "EPOCH 13 of 50\n",
      "\n",
      "Training loss: 0.005633654098750817\n",
      "Validation loss: 0.004508967588966091\n",
      "\n",
      "EPOCH 14 of 50\n",
      "\n",
      "Training loss: 0.005434667588108116\n",
      "Validation loss: 0.004594958620145917\n",
      "\n",
      "EPOCH 15 of 50\n",
      "\n",
      "Training loss: 0.005453130049217079\n",
      "Validation loss: 0.00454892326767246\n",
      "\n",
      "EPOCH 16 of 50\n",
      "\n",
      "Training loss: 0.005757448418686788\n",
      "Validation loss: 0.004511241024980943\n",
      "\n",
      "EPOCH 17 of 50\n",
      "\n",
      "Training loss: 0.005755187012255192\n",
      "Validation loss: 0.0045481914809594555\n",
      "\n",
      "EPOCH 18 of 50\n",
      "\n",
      "Training loss: 0.0058574606147077345\n",
      "Validation loss: 0.0046065371328343945\n",
      "\n",
      "EPOCH 19 of 50\n",
      "\n",
      "Training loss: 0.005563280027773645\n",
      "Validation loss: 0.00460932613350451\n",
      "\n",
      "EPOCH 20 of 50\n",
      "\n",
      "Training loss: 0.005745368285311593\n",
      "Validation loss: 0.0045640502745906515\n",
      "\n",
      "EPOCH 21 of 50\n",
      "\n",
      "Training loss: 0.0054467058637075955\n",
      "Validation loss: 0.0045200329429159565\n",
      "\n",
      "EPOCH 22 of 50\n",
      "\n",
      "Training loss: 0.005759240542021062\n",
      "Validation loss: 0.004587059374898672\n",
      "\n",
      "EPOCH 23 of 50\n",
      "\n",
      "Training loss: 0.005621536997043424\n",
      "Validation loss: 0.004545282882948716\n",
      "\n",
      "EPOCH 24 of 50\n",
      "\n",
      "Training loss: 0.00558851742082172\n",
      "Validation loss: 0.004547134352227052\n",
      "\n",
      "EPOCH 25 of 50\n",
      "\n",
      "Training loss: 0.005510570326199134\n",
      "Validation loss: 0.00454228223922352\n",
      "\n",
      "EPOCH 26 of 50\n",
      "\n",
      "Training loss: 0.00587274729170733\n",
      "Validation loss: 0.004584080694864194\n",
      "\n",
      "EPOCH 27 of 50\n",
      "\n",
      "Training loss: 0.005455610833855139\n",
      "Validation loss: 0.004521427831302087\n",
      "\n",
      "EPOCH 28 of 50\n",
      "\n",
      "Training loss: 0.005438114775137769\n",
      "Validation loss: 0.004605061374604702\n",
      "\n",
      "EPOCH 29 of 50\n",
      "\n",
      "Training loss: 0.005535113935669263\n",
      "Validation loss: 0.004618838429450989\n",
      "\n",
      "EPOCH 30 of 50\n",
      "\n",
      "Training loss: 0.0053798947224600446\n",
      "Validation loss: 0.00463402783498168\n",
      "\n",
      "EPOCH 31 of 50\n",
      "\n",
      "Training loss: 0.005499459492663543\n",
      "Validation loss: 0.00458606433433791\n",
      "\n",
      "EPOCH 32 of 50\n",
      "\n",
      "Training loss: 0.005364038489965929\n",
      "Validation loss: 0.004500059488539894\n",
      "\n",
      "EPOCH 33 of 50\n",
      "\n",
      "Training loss: 0.00570515144823326\n",
      "Validation loss: 0.004600527385870616\n",
      "\n",
      "EPOCH 34 of 50\n",
      "\n",
      "Training loss: 0.00549391998598973\n",
      "Validation loss: 0.0045977993092189235\n",
      "\n",
      "EPOCH 35 of 50\n",
      "\n",
      "Training loss: 0.005673318739152617\n",
      "Validation loss: 0.004540096890802185\n",
      "\n",
      "EPOCH 36 of 50\n",
      "\n",
      "Training loss: 0.005358400185488992\n",
      "Validation loss: 0.004612138339628776\n",
      "\n",
      "EPOCH 37 of 50\n",
      "\n",
      "Training loss: 0.00553913414478302\n",
      "Validation loss: 0.004571395382906\n",
      "\n",
      "EPOCH 38 of 50\n",
      "\n",
      "Training loss: 0.005674569143189324\n",
      "Validation loss: 0.004496174398809671\n",
      "\n",
      "EPOCH 39 of 50\n",
      "\n",
      "Training loss: 0.0057918645648492705\n",
      "Validation loss: 0.0044827691745013\n",
      "\n",
      "EPOCH 40 of 50\n",
      "\n",
      "Training loss: 0.00559141223008434\n",
      "Validation loss: 0.004554323386400938\n",
      "\n",
      "EPOCH 41 of 50\n",
      "\n",
      "Training loss: 0.005498098157760169\n",
      "Validation loss: 0.00459728102820615\n",
      "\n",
      "EPOCH 42 of 50\n",
      "\n",
      "Training loss: 0.005381628767483764\n",
      "Validation loss: 0.0045702105077604456\n",
      "\n",
      "EPOCH 43 of 50\n",
      "\n",
      "Training loss: 0.005587430205196142\n",
      "Validation loss: 0.004574980586767197\n",
      "\n",
      "EPOCH 44 of 50\n",
      "\n",
      "Training loss: 0.00564306519097752\n",
      "Validation loss: 0.0045611214979241295\n",
      "\n",
      "EPOCH 45 of 50\n",
      "\n",
      "Training loss: 0.005593850277364254\n",
      "Validation loss: 0.0045477029246588545\n",
      "\n",
      "EPOCH 46 of 50\n",
      "\n",
      "Training loss: 0.0055802132313450175\n",
      "Validation loss: 0.004544382216408849\n",
      "\n",
      "EPOCH 47 of 50\n",
      "\n",
      "Training loss: 0.005498581953967611\n",
      "Validation loss: 0.004597404583667715\n",
      "\n",
      "EPOCH 48 of 50\n",
      "\n",
      "Training loss: 0.0053979255931658875\n",
      "Validation loss: 0.004598693456500769\n",
      "\n",
      "EPOCH 49 of 50\n",
      "\n",
      "Training loss: 0.005588628274078171\n",
      "Validation loss: 0.00456126919016242\n",
      "\n",
      "EPOCH 50 of 50\n",
      "\n",
      "Training loss: 0.00560471938095159\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-30 05:43:12,436] Trial 31 finished with value: 0.004578971148778995 and parameters: {'lr': 2.72793239518843e-05, 'weight_decay': 7.031683618161533e-05, 'hidden_size': 331, 'dropout_rate': 0.4781868070024953}. Best is trial 16 with value: 0.0032828766076515117.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.004578971148778995\n",
      "learning rate: 5.069812824143811e-05\n",
      "weight_decay: 1.88374901862837e-05\n",
      "hidden_size: 300\n",
      "dropout_rate: 0.4698369953468511\n",
      "\n",
      "EPOCH 1 of 50\n",
      "\n",
      "Training loss: 0.00544442127769192\n",
      "Validation loss: 0.003542367989818255\n",
      "\n",
      "EPOCH 2 of 50\n",
      "\n",
      "Training loss: 0.005361333851599031\n",
      "Validation loss: 0.0037344235461205244\n",
      "\n",
      "EPOCH 3 of 50\n",
      "\n",
      "Training loss: 0.005505029319061173\n",
      "Validation loss: 0.003871432039886713\n",
      "\n",
      "EPOCH 4 of 50\n",
      "\n",
      "Training loss: 0.005486928495681948\n",
      "Validation loss: 0.003984680399298668\n",
      "\n",
      "EPOCH 5 of 50\n",
      "\n",
      "Training loss: 0.005427731356273095\n",
      "Validation loss: 0.003946458377564947\n",
      "\n",
      "EPOCH 6 of 50\n",
      "\n",
      "Training loss: 0.0055903468487991225\n",
      "Validation loss: 0.003956968042378624\n",
      "\n",
      "EPOCH 7 of 50\n",
      "\n",
      "Training loss: 0.005439412159224351\n",
      "Validation loss: 0.003968875699987014\n",
      "\n",
      "EPOCH 8 of 50\n",
      "\n",
      "Training loss: 0.005507913030063112\n",
      "Validation loss: 0.003970032945896189\n",
      "\n",
      "EPOCH 9 of 50\n",
      "\n",
      "Training loss: 0.0053799044496069355\n",
      "Validation loss: 0.003978146783386667\n",
      "\n",
      "EPOCH 10 of 50\n",
      "\n",
      "Training loss: 0.005655132234096527\n",
      "Validation loss: 0.003991278198858102\n",
      "\n",
      "EPOCH 11 of 50\n",
      "\n",
      "Training loss: 0.005592969246208668\n",
      "Validation loss: 0.004011052505423625\n",
      "\n",
      "EPOCH 12 of 50\n",
      "\n",
      "Training loss: 0.005924921482801437\n",
      "Validation loss: 0.003978106736515959\n",
      "\n",
      "EPOCH 13 of 50\n",
      "\n",
      "Training loss: 0.005867055099871423\n",
      "Validation loss: 0.003985338844358921\n",
      "\n",
      "EPOCH 14 of 50\n",
      "\n",
      "Training loss: 0.005383042308191459\n",
      "Validation loss: 0.003974311441803972\n",
      "\n",
      "EPOCH 15 of 50\n",
      "\n",
      "Training loss: 0.005313780417458879\n",
      "Validation loss: 0.003990177918846409\n",
      "\n",
      "EPOCH 16 of 50\n",
      "\n",
      "Training loss: 0.0055710457058416474\n",
      "Validation loss: 0.00399863727701207\n",
      "\n",
      "EPOCH 17 of 50\n",
      "\n",
      "Training loss: 0.005422318302508857\n",
      "Validation loss: 0.003967975964769721\n",
      "\n",
      "EPOCH 18 of 50\n",
      "\n",
      "Training loss: 0.005357534676376317\n",
      "Validation loss: 0.004009115354468425\n",
      "\n",
      "EPOCH 19 of 50\n",
      "\n",
      "Training loss: 0.005283789295289252\n",
      "Validation loss: 0.003985440979401271\n",
      "\n",
      "EPOCH 20 of 50\n",
      "\n",
      "Training loss: 0.005977791992740499\n",
      "Validation loss: 0.004025643380979697\n",
      "\n",
      "EPOCH 21 of 50\n",
      "\n",
      "Training loss: 0.005350990427864922\n",
      "Validation loss: 0.004035280862202247\n",
      "\n",
      "EPOCH 22 of 50\n",
      "\n",
      "Training loss: 0.00585387510040568\n",
      "Validation loss: 0.004053272074088454\n",
      "\n",
      "EPOCH 23 of 50\n",
      "\n",
      "Training loss: 0.005637124077313476\n",
      "Validation loss: 0.0040423697015891475\n",
      "\n",
      "EPOCH 24 of 50\n",
      "\n",
      "Training loss: 0.005871821608808305\n",
      "Validation loss: 0.004052001362045606\n",
      "\n",
      "EPOCH 25 of 50\n",
      "\n",
      "Training loss: 0.005420930217951536\n",
      "Validation loss: 0.0039930338971316814\n",
      "\n",
      "EPOCH 26 of 50\n",
      "\n",
      "Training loss: 0.005505725017024411\n",
      "Validation loss: 0.004033332224935293\n",
      "\n",
      "EPOCH 27 of 50\n",
      "\n",
      "Training loss: 0.005395827607976066\n",
      "Validation loss: 0.004045269529645641\n",
      "\n",
      "EPOCH 28 of 50\n",
      "\n",
      "Training loss: 0.005408615029106538\n",
      "Validation loss: 0.003970235586166382\n",
      "\n",
      "EPOCH 29 of 50\n",
      "\n",
      "Training loss: 0.005853159792928232\n",
      "Validation loss: 0.004013042741765578\n",
      "\n",
      "EPOCH 30 of 50\n",
      "\n",
      "Training loss: 0.005439061412794722\n",
      "Validation loss: 0.003998468707626064\n",
      "\n",
      "EPOCH 31 of 50\n",
      "\n",
      "Training loss: 0.005367510150083237\n",
      "Validation loss: 0.004000387232129772\n",
      "\n",
      "EPOCH 32 of 50\n",
      "\n",
      "Training loss: 0.005503140752100282\n",
      "Validation loss: 0.004015692897761862\n",
      "\n",
      "EPOCH 33 of 50\n",
      "\n",
      "Training loss: 0.005320734085722102\n",
      "Validation loss: 0.0040132454596459866\n",
      "\n",
      "EPOCH 34 of 50\n",
      "\n",
      "Training loss: 0.005571647133264277\n",
      "Validation loss: 0.004018526369084914\n",
      "\n",
      "EPOCH 35 of 50\n",
      "\n",
      "Training loss: 0.0057744610951178605\n",
      "Validation loss: 0.004002291476354003\n",
      "\n",
      "EPOCH 36 of 50\n",
      "\n",
      "Training loss: 0.005549389817234542\n",
      "Validation loss: 0.00402720901183784\n",
      "\n",
      "EPOCH 37 of 50\n",
      "\n",
      "Training loss: 0.005354682242290841\n",
      "Validation loss: 0.004045056800047557\n",
      "\n",
      "EPOCH 38 of 50\n",
      "\n",
      "Training loss: 0.005442594747162527\n",
      "Validation loss: 0.003949777223169804\n",
      "\n",
      "EPOCH 39 of 50\n",
      "\n",
      "Training loss: 0.005425251037296321\n",
      "Validation loss: 0.004007206065580249\n",
      "\n",
      "EPOCH 40 of 50\n",
      "\n",
      "Training loss: 0.005277384098412262\n",
      "Validation loss: 0.004014529675866167\n",
      "\n",
      "EPOCH 41 of 50\n",
      "\n",
      "Training loss: 0.00540658924728632\n",
      "Validation loss: 0.004023156128823757\n",
      "\n",
      "EPOCH 42 of 50\n",
      "\n",
      "Training loss: 0.0055980295356776975\n",
      "Validation loss: 0.003981602921461065\n",
      "\n",
      "EPOCH 43 of 50\n",
      "\n",
      "Training loss: 0.005590589354849524\n",
      "Validation loss: 0.00398050993680954\n",
      "\n",
      "EPOCH 44 of 50\n",
      "\n",
      "Training loss: 0.005552019406523969\n",
      "Validation loss: 0.003944747072334091\n",
      "\n",
      "EPOCH 45 of 50\n",
      "\n",
      "Training loss: 0.0053635080758896135\n",
      "Validation loss: 0.0039499712487061816\n",
      "\n",
      "EPOCH 46 of 50\n",
      "\n",
      "Training loss: 0.005397705619947778\n",
      "Validation loss: 0.003972590047245224\n",
      "\n",
      "EPOCH 47 of 50\n",
      "\n",
      "Training loss: 0.005561354414870341\n",
      "Validation loss: 0.003992296289652586\n",
      "\n",
      "EPOCH 48 of 50\n",
      "\n",
      "Training loss: 0.005371803624762429\n",
      "Validation loss: 0.004022831640516718\n",
      "\n",
      "EPOCH 49 of 50\n",
      "\n",
      "Training loss: 0.0056593095262845354\n",
      "Validation loss: 0.004039034945890307\n",
      "\n",
      "EPOCH 50 of 50\n",
      "\n",
      "Training loss: 0.005473092974474032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-30 05:47:21,621] Trial 32 finished with value: 0.004059170450394352 and parameters: {'lr': 5.069812824143811e-05, 'weight_decay': 1.88374901862837e-05, 'hidden_size': 300, 'dropout_rate': 0.4698369953468511}. Best is trial 16 with value: 0.0032828766076515117.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.004059170450394352\n",
      "learning rate: 9.455681759207219e-05\n",
      "weight_decay: 4.116546682877437e-05\n",
      "hidden_size: 435\n",
      "dropout_rate: 0.4537205764327793\n",
      "\n",
      "EPOCH 1 of 50\n",
      "\n",
      "Training loss: 0.005513989780512121\n",
      "Validation loss: 0.0033556349420299134\n",
      "\n",
      "EPOCH 2 of 50\n",
      "\n",
      "Training loss: 0.005392309174769455\n",
      "Validation loss: 0.0033624079854538045\n",
      "\n",
      "EPOCH 3 of 50\n",
      "\n",
      "Training loss: 0.005412722161660592\n",
      "Validation loss: 0.0033640036514649787\n",
      "\n",
      "EPOCH 4 of 50\n",
      "\n",
      "Training loss: 0.005599718721997406\n",
      "Validation loss: 0.0033734052752455077\n",
      "\n",
      "EPOCH 5 of 50\n",
      "\n",
      "Training loss: 0.005489075194216437\n",
      "Validation loss: 0.00336120572562019\n",
      "\n",
      "EPOCH 6 of 50\n",
      "\n",
      "Training loss: 0.005561648868024349\n",
      "Validation loss: 0.0033848448656499386\n",
      "\n",
      "EPOCH 7 of 50\n",
      "\n",
      "Training loss: 0.00534753796334068\n",
      "Validation loss: 0.0033785377939542136\n",
      "\n",
      "EPOCH 8 of 50\n",
      "\n",
      "Training loss: 0.0052090279706236385\n",
      "Validation loss: 0.003383836398522059\n",
      "\n",
      "EPOCH 9 of 50\n",
      "\n",
      "Training loss: 0.005273015807486243\n",
      "Validation loss: 0.0033853251952677965\n",
      "\n",
      "EPOCH 10 of 50\n",
      "\n",
      "Training loss: 0.005620829347107146\n",
      "Validation loss: 0.0033974520241220794\n",
      "\n",
      "EPOCH 11 of 50\n",
      "\n",
      "Training loss: 0.005590917801277505\n",
      "Validation loss: 0.003376510925590992\n",
      "\n",
      "EPOCH 12 of 50\n",
      "\n",
      "Training loss: 0.005448243787719144\n",
      "Validation loss: 0.0033788276681055627\n",
      "\n",
      "EPOCH 13 of 50\n",
      "\n",
      "Training loss: 0.00533230082752804\n",
      "Validation loss: 0.0033627215307205915\n",
      "\n",
      "EPOCH 14 of 50\n",
      "\n",
      "Training loss: 0.005626666359603405\n",
      "Validation loss: 0.0033706987742334604\n",
      "\n",
      "EPOCH 15 of 50\n",
      "\n",
      "Training loss: 0.005302839550293154\n",
      "Validation loss: 0.0033823210590829453\n",
      "\n",
      "EPOCH 16 of 50\n",
      "\n",
      "Training loss: 0.005614883473349942\n",
      "Validation loss: 0.0033817708802719912\n",
      "\n",
      "EPOCH 17 of 50\n",
      "\n",
      "Training loss: 0.005672013387084007\n",
      "Validation loss: 0.0033710515902688107\n",
      "\n",
      "EPOCH 18 of 50\n",
      "\n",
      "Training loss: 0.005308535182848573\n",
      "Validation loss: 0.0033718817867338657\n",
      "\n",
      "EPOCH 19 of 50\n",
      "\n",
      "Training loss: 0.0055871194538970785\n",
      "Validation loss: 0.0033821096488585076\n",
      "\n",
      "EPOCH 20 of 50\n",
      "\n",
      "Training loss: 0.00569902691576216\n",
      "Validation loss: 0.0033620784524828196\n",
      "\n",
      "EPOCH 21 of 50\n",
      "\n",
      "Training loss: 0.005462312398271428\n",
      "Validation loss: 0.0033812991653879485\n",
      "\n",
      "EPOCH 22 of 50\n",
      "\n",
      "Training loss: 0.005561754883577426\n",
      "Validation loss: 0.0033633253381898007\n",
      "\n",
      "EPOCH 23 of 50\n",
      "\n",
      "Training loss: 0.005640728088716666\n",
      "Validation loss: 0.0033648606234540543\n",
      "\n",
      "EPOCH 24 of 50\n",
      "\n",
      "Training loss: 0.005377064769466718\n",
      "Validation loss: 0.0033812158120175204\n",
      "\n",
      "EPOCH 25 of 50\n",
      "\n",
      "Training loss: 0.005180795211344957\n",
      "Validation loss: 0.0033650981107105813\n",
      "\n",
      "EPOCH 26 of 50\n",
      "\n",
      "Training loss: 0.005659216445767217\n",
      "Validation loss: 0.00335907742070655\n",
      "\n",
      "EPOCH 27 of 50\n",
      "\n",
      "Training loss: 0.005862592667755153\n",
      "Validation loss: 0.003353552194312215\n",
      "\n",
      "EPOCH 28 of 50\n",
      "\n",
      "Training loss: 0.005302793812006712\n",
      "Validation loss: 0.003356460714712739\n",
      "\n",
      "EPOCH 29 of 50\n",
      "\n",
      "Training loss: 0.005683740212892492\n",
      "Validation loss: 0.0033664926886558533\n",
      "\n",
      "EPOCH 30 of 50\n",
      "\n",
      "Training loss: 0.005407703497136633\n",
      "Validation loss: 0.0033653633824239173\n",
      "\n",
      "EPOCH 31 of 50\n",
      "\n",
      "Training loss: 0.00537106834558977\n",
      "Validation loss: 0.0033646307419985533\n",
      "\n",
      "EPOCH 32 of 50\n",
      "\n",
      "Training loss: 0.005387101891554064\n",
      "Validation loss: 0.003374230582267046\n",
      "\n",
      "EPOCH 33 of 50\n",
      "\n",
      "Training loss: 0.005555965730713474\n",
      "Validation loss: 0.0033865606722732386\n",
      "\n",
      "EPOCH 34 of 50\n",
      "\n",
      "Training loss: 0.005282333741585414\n",
      "Validation loss: 0.0033786258039375148\n",
      "\n",
      "EPOCH 35 of 50\n",
      "\n",
      "Training loss: 0.005504798920204242\n",
      "Validation loss: 0.0033515066218872866\n",
      "\n",
      "EPOCH 36 of 50\n",
      "\n",
      "Training loss: 0.0057742286783953505\n",
      "Validation loss: 0.0033629663909475007\n",
      "\n",
      "EPOCH 37 of 50\n",
      "\n",
      "Training loss: 0.005337111134496\n",
      "Validation loss: 0.003382551639030377\n",
      "\n",
      "EPOCH 38 of 50\n",
      "\n",
      "Training loss: 0.0054794238983756965\n",
      "Validation loss: 0.0033649326457331576\n",
      "\n",
      "EPOCH 39 of 50\n",
      "\n",
      "Training loss: 0.005611573863360617\n",
      "Validation loss: 0.0033665221805373826\n",
      "\n",
      "EPOCH 40 of 50\n",
      "\n",
      "Training loss: 0.005344510854532321\n",
      "Validation loss: 0.0033693560399115086\n",
      "\n",
      "EPOCH 41 of 50\n",
      "\n",
      "Training loss: 0.0054900206677201725\n",
      "Validation loss: 0.003387515743573507\n",
      "\n",
      "EPOCH 42 of 50\n",
      "\n",
      "Training loss: 0.005281868675309751\n",
      "Validation loss: 0.003377896345530947\n",
      "\n",
      "EPOCH 43 of 50\n",
      "\n",
      "Training loss: 0.005497355893668201\n",
      "Validation loss: 0.0033575460159530244\n",
      "\n",
      "EPOCH 44 of 50\n",
      "\n",
      "Training loss: 0.0052153154328051544\n",
      "Validation loss: 0.003367067857955893\n",
      "\n",
      "EPOCH 45 of 50\n",
      "\n",
      "Training loss: 0.0055439626901514\n",
      "Validation loss: 0.0033633262695123753\n",
      "\n",
      "EPOCH 46 of 50\n",
      "\n",
      "Training loss: 0.005700144554591841\n",
      "Validation loss: 0.0033682596404105425\n",
      "\n",
      "EPOCH 47 of 50\n",
      "\n",
      "Training loss: 0.005171623080968857\n",
      "Validation loss: 0.003376744997998079\n",
      "\n",
      "EPOCH 48 of 50\n",
      "\n",
      "Training loss: 0.005693919491022825\n",
      "Validation loss: 0.0033655724643419185\n",
      "\n",
      "EPOCH 49 of 50\n",
      "\n",
      "Training loss: 0.005410662438306544\n",
      "Validation loss: 0.0033682330201069513\n",
      "\n",
      "EPOCH 50 of 50\n",
      "\n",
      "Training loss: 0.005626701128979524\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-30 05:51:37,811] Trial 33 finished with value: 0.0033831987529993057 and parameters: {'lr': 9.455681759207219e-05, 'weight_decay': 4.116546682877437e-05, 'hidden_size': 435, 'dropout_rate': 0.4537205764327793}. Best is trial 16 with value: 0.0032828766076515117.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.0033831987529993057\n",
      "learning rate: 5.672617065178356e-05\n",
      "weight_decay: 0.00010629260660626887\n",
      "hidden_size: 387\n",
      "dropout_rate: 0.47892829801447445\n",
      "\n",
      "EPOCH 1 of 50\n",
      "\n",
      "Training loss: 0.005624177503503031\n",
      "Validation loss: 0.00329620026362439\n",
      "\n",
      "EPOCH 2 of 50\n",
      "\n",
      "Training loss: 0.005435957573354244\n",
      "Validation loss: 0.0032995184883475304\n",
      "\n",
      "EPOCH 3 of 50\n",
      "\n",
      "Training loss: 0.005464161280542612\n",
      "Validation loss: 0.0033048830615977445\n",
      "\n",
      "EPOCH 4 of 50\n",
      "\n",
      "Training loss: 0.005617471411824226\n",
      "Validation loss: 0.0033089204225689173\n",
      "\n",
      "EPOCH 5 of 50\n",
      "\n",
      "Training loss: 0.005346955653900902\n",
      "Validation loss: 0.003311718193193277\n",
      "\n",
      "EPOCH 6 of 50\n",
      "\n",
      "Training loss: 0.005300069952176677\n",
      "Validation loss: 0.00330950024848183\n",
      "\n",
      "EPOCH 7 of 50\n",
      "\n",
      "Training loss: 0.0052832314848072\n",
      "Validation loss: 0.003310774608204762\n",
      "\n",
      "EPOCH 8 of 50\n",
      "\n",
      "Training loss: 0.005451336140847868\n",
      "Validation loss: 0.003316367588316401\n",
      "\n",
      "EPOCH 9 of 50\n",
      "\n",
      "Training loss: 0.005488909108357297\n",
      "Validation loss: 0.003311151017745336\n",
      "\n",
      "EPOCH 10 of 50\n",
      "\n",
      "Training loss: 0.00582711688346333\n",
      "Validation loss: 0.0033131149442245564\n",
      "\n",
      "EPOCH 11 of 50\n",
      "\n",
      "Training loss: 0.005398120731115341\n",
      "Validation loss: 0.003309637773782015\n",
      "\n",
      "EPOCH 12 of 50\n",
      "\n",
      "Training loss: 0.005300277378410101\n",
      "Validation loss: 0.0033133356676747403\n",
      "\n",
      "EPOCH 13 of 50\n",
      "\n",
      "Training loss: 0.005195133129341735\n",
      "Validation loss: 0.003307920958225926\n",
      "\n",
      "EPOCH 14 of 50\n",
      "\n",
      "Training loss: 0.005549902872492869\n",
      "Validation loss: 0.0033126731092731156\n",
      "\n",
      "EPOCH 15 of 50\n",
      "\n",
      "Training loss: 0.005527352552033133\n",
      "Validation loss: 0.003312221340214213\n",
      "\n",
      "EPOCH 16 of 50\n",
      "\n",
      "Training loss: 0.005008221681540211\n",
      "Validation loss: 0.003310432890430093\n",
      "\n",
      "EPOCH 17 of 50\n",
      "\n",
      "Training loss: 0.005408530692673392\n",
      "Validation loss: 0.0033141188323497772\n",
      "\n",
      "EPOCH 18 of 50\n",
      "\n",
      "Training loss: 0.005414475169446733\n",
      "Validation loss: 0.0033078281364093223\n",
      "\n",
      "EPOCH 19 of 50\n",
      "\n",
      "Training loss: 0.005609079626285368\n",
      "Validation loss: 0.0033082276737938323\n",
      "\n",
      "EPOCH 20 of 50\n",
      "\n",
      "Training loss: 0.00557503326692515\n",
      "Validation loss: 0.0033111420925706625\n",
      "\n",
      "EPOCH 21 of 50\n",
      "\n",
      "Training loss: 0.005316878073952264\n",
      "Validation loss: 0.003308149054646492\n",
      "\n",
      "EPOCH 22 of 50\n",
      "\n",
      "Training loss: 0.005248108671771156\n",
      "Validation loss: 0.0033081562723964453\n",
      "\n",
      "EPOCH 23 of 50\n",
      "\n",
      "Training loss: 0.005517170609285434\n",
      "Validation loss: 0.003309780110915502\n",
      "\n",
      "EPOCH 24 of 50\n",
      "\n",
      "Training loss: 0.005434377067204978\n",
      "Validation loss: 0.003310074176018437\n",
      "\n",
      "EPOCH 25 of 50\n",
      "\n",
      "Training loss: 0.005294919893559482\n",
      "Validation loss: 0.0033094289246946573\n",
      "\n",
      "EPOCH 26 of 50\n",
      "\n",
      "Training loss: 0.00553114784674512\n",
      "Validation loss: 0.003307627203563849\n",
      "\n",
      "EPOCH 27 of 50\n",
      "\n",
      "Training loss: 0.005424470537238651\n",
      "Validation loss: 0.003313383630787333\n",
      "\n",
      "EPOCH 28 of 50\n",
      "\n",
      "Training loss: 0.0054049488260514205\n",
      "Validation loss: 0.0033135609701275826\n",
      "\n",
      "EPOCH 29 of 50\n",
      "\n",
      "Training loss: 0.005169559658194582\n",
      "Validation loss: 0.0033111749216914177\n",
      "\n",
      "EPOCH 30 of 50\n",
      "\n",
      "Training loss: 0.005465454654768109\n",
      "Validation loss: 0.0033098726999014616\n",
      "\n",
      "EPOCH 31 of 50\n",
      "\n",
      "Training loss: 0.0055964720538920825\n",
      "Validation loss: 0.0033183161479731402\n",
      "\n",
      "EPOCH 32 of 50\n",
      "\n",
      "Training loss: 0.00543355481285188\n",
      "Validation loss: 0.003310376157363256\n",
      "\n",
      "EPOCH 33 of 50\n",
      "\n",
      "Training loss: 0.0054786609382265145\n",
      "Validation loss: 0.003310550314684709\n",
      "\n",
      "EPOCH 34 of 50\n",
      "\n",
      "Training loss: 0.00530169697271453\n",
      "Validation loss: 0.00331302173435688\n",
      "\n",
      "EPOCH 35 of 50\n",
      "\n",
      "Training loss: 0.006009962151034011\n",
      "Validation loss: 0.003316469800968965\n",
      "\n",
      "EPOCH 36 of 50\n",
      "\n",
      "Training loss: 0.005822806205186579\n",
      "Validation loss: 0.0033140291925519705\n",
      "\n",
      "EPOCH 37 of 50\n",
      "\n",
      "Training loss: 0.005344752998401721\n",
      "Validation loss: 0.003316085940847794\n",
      "\n",
      "EPOCH 38 of 50\n",
      "\n",
      "Training loss: 0.005425316074656116\n",
      "Validation loss: 0.003317512727032105\n",
      "\n",
      "EPOCH 39 of 50\n",
      "\n",
      "Training loss: 0.005453090131696727\n",
      "Validation loss: 0.003314896486699581\n",
      "\n",
      "EPOCH 40 of 50\n",
      "\n",
      "Training loss: 0.0052614721935242414\n",
      "Validation loss: 0.003311274262766043\n",
      "\n",
      "EPOCH 41 of 50\n",
      "\n",
      "Training loss: 0.005337533592763875\n",
      "Validation loss: 0.0033128551052262387\n",
      "\n",
      "EPOCH 42 of 50\n",
      "\n",
      "Training loss: 0.005296604475006461\n",
      "Validation loss: 0.0033104096073657274\n",
      "\n",
      "EPOCH 43 of 50\n",
      "\n",
      "Training loss: 0.005605641700741317\n",
      "Validation loss: 0.0033155235772331557\n",
      "\n",
      "EPOCH 44 of 50\n",
      "\n",
      "Training loss: 0.005541259164197577\n",
      "Validation loss: 0.0033116179207960763\n",
      "\n",
      "EPOCH 45 of 50\n",
      "\n",
      "Training loss: 0.005499417997068829\n",
      "Validation loss: 0.003314489576344689\n",
      "\n",
      "EPOCH 46 of 50\n",
      "\n",
      "Training loss: 0.005389869833985965\n",
      "Validation loss: 0.003310900259142121\n",
      "\n",
      "EPOCH 47 of 50\n",
      "\n",
      "Training loss: 0.005529091072579225\n",
      "Validation loss: 0.0033078653117020926\n",
      "\n",
      "EPOCH 48 of 50\n",
      "\n",
      "Training loss: 0.005477947648614645\n",
      "Validation loss: 0.003308397407333056\n",
      "\n",
      "EPOCH 49 of 50\n",
      "\n",
      "Training loss: 0.005386876821931865\n",
      "Validation loss: 0.0033124356220165887\n",
      "\n",
      "EPOCH 50 of 50\n",
      "\n",
      "Training loss: 0.005267251515761018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-30 05:55:43,776] Trial 34 finished with value: 0.003315456366787354 and parameters: {'lr': 5.672617065178356e-05, 'weight_decay': 0.00010629260660626887, 'hidden_size': 387, 'dropout_rate': 0.47892829801447445}. Best is trial 16 with value: 0.0032828766076515117.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.003315456366787354\n",
      "learning rate: 7.577783337831243e-05\n",
      "weight_decay: 2.7326888652729394e-05\n",
      "hidden_size: 485\n",
      "dropout_rate: 0.4381306613750493\n",
      "\n",
      "EPOCH 1 of 50\n",
      "\n",
      "Training loss: 0.005310584635784228\n",
      "Validation loss: 0.003301510587334633\n",
      "\n",
      "EPOCH 2 of 50\n",
      "\n",
      "Training loss: 0.0054818191565573215\n",
      "Validation loss: 0.0033091905061155558\n",
      "\n",
      "EPOCH 3 of 50\n",
      "\n",
      "Training loss: 0.005593218737178379\n",
      "Validation loss: 0.00331179634667933\n",
      "\n",
      "EPOCH 4 of 50\n",
      "\n",
      "Training loss: 0.005258984915498231\n",
      "Validation loss: 0.003311377406741182\n",
      "\n",
      "EPOCH 5 of 50\n",
      "\n",
      "Training loss: 0.005222181298045648\n",
      "Validation loss: 0.0033109826035797596\n",
      "\n",
      "EPOCH 6 of 50\n",
      "\n",
      "Training loss: 0.005386089181734456\n",
      "Validation loss: 0.0033119277407725654\n",
      "\n",
      "EPOCH 7 of 50\n",
      "\n",
      "Training loss: 0.005564943007710908\n",
      "Validation loss: 0.003316964100425442\n",
      "\n",
      "EPOCH 8 of 50\n",
      "\n",
      "Training loss: 0.006012850829089682\n",
      "Validation loss: 0.0033092855010181665\n",
      "\n",
      "EPOCH 9 of 50\n",
      "\n",
      "Training loss: 0.005492376318822305\n",
      "Validation loss: 0.0033133088145405054\n",
      "\n",
      "EPOCH 10 of 50\n",
      "\n",
      "Training loss: 0.005393154246525632\n",
      "Validation loss: 0.003309593147908648\n",
      "\n",
      "EPOCH 11 of 50\n",
      "\n",
      "Training loss: 0.0052148146658307975\n",
      "Validation loss: 0.0033131547582646212\n",
      "\n",
      "EPOCH 12 of 50\n",
      "\n",
      "Training loss: 0.005362177371150917\n",
      "Validation loss: 0.003310904993365208\n",
      "\n",
      "EPOCH 13 of 50\n",
      "\n",
      "Training loss: 0.005384232641922103\n",
      "Validation loss: 0.003311310817177097\n",
      "\n",
      "EPOCH 14 of 50\n",
      "\n",
      "Training loss: 0.005500719054705567\n",
      "Validation loss: 0.0033114448500176272\n",
      "\n",
      "EPOCH 15 of 50\n",
      "\n",
      "Training loss: 0.005259607711599933\n",
      "Validation loss: 0.003312160959467292\n",
      "\n",
      "EPOCH 16 of 50\n",
      "\n",
      "Training loss: 0.0055454858682221836\n",
      "Validation loss: 0.0033096324962874255\n",
      "\n",
      "EPOCH 17 of 50\n",
      "\n",
      "Training loss: 0.005208180984482169\n",
      "Validation loss: 0.003306803215915958\n",
      "\n",
      "EPOCH 18 of 50\n",
      "\n",
      "Training loss: 0.005211242836796575\n",
      "Validation loss: 0.003313853327805797\n",
      "\n",
      "EPOCH 19 of 50\n",
      "\n",
      "Training loss: 0.0053101044872568715\n",
      "Validation loss: 0.0033113344106823206\n",
      "\n",
      "EPOCH 20 of 50\n",
      "\n",
      "Training loss: 0.005983358352548546\n",
      "Validation loss: 0.003309620932365457\n",
      "\n",
      "EPOCH 21 of 50\n",
      "\n",
      "Training loss: 0.0051736850291490555\n",
      "Validation loss: 0.003312084823846817\n",
      "\n",
      "EPOCH 22 of 50\n",
      "\n",
      "Training loss: 0.005398015439924266\n",
      "Validation loss: 0.0033148198078076043\n",
      "\n",
      "EPOCH 23 of 50\n",
      "\n",
      "Training loss: 0.005293232414664494\n",
      "Validation loss: 0.0033114441515256963\n",
      "\n",
      "EPOCH 24 of 50\n",
      "\n",
      "Training loss: 0.005417074439012342\n",
      "Validation loss: 0.0033093170107652745\n",
      "\n",
      "EPOCH 25 of 50\n",
      "\n",
      "Training loss: 0.00520420371968713\n",
      "Validation loss: 0.0033165170655896268\n",
      "\n",
      "EPOCH 26 of 50\n",
      "\n",
      "Training loss: 0.005180243247499068\n",
      "Validation loss: 0.0033153436767558255\n",
      "\n",
      "EPOCH 27 of 50\n",
      "\n",
      "Training loss: 0.005286646955129173\n",
      "Validation loss: 0.003312560652072231\n",
      "\n",
      "EPOCH 28 of 50\n",
      "\n",
      "Training loss: 0.005361800418338842\n",
      "Validation loss: 0.003311199446519216\n",
      "\n",
      "EPOCH 29 of 50\n",
      "\n",
      "Training loss: 0.005387542717572715\n",
      "Validation loss: 0.003309679993738731\n",
      "\n",
      "EPOCH 30 of 50\n",
      "\n",
      "Training loss: 0.005260318155503935\n",
      "Validation loss: 0.0033082482405006886\n",
      "\n",
      "EPOCH 31 of 50\n",
      "\n",
      "Training loss: 0.005492907224429978\n",
      "Validation loss: 0.0033123632116864123\n",
      "\n",
      "EPOCH 32 of 50\n",
      "\n",
      "Training loss: 0.005457188416686323\n",
      "Validation loss: 0.003313998614127437\n",
      "\n",
      "EPOCH 33 of 50\n",
      "\n",
      "Training loss: 0.005347784453382094\n",
      "Validation loss: 0.0033124431502074003\n",
      "\n",
      "EPOCH 34 of 50\n",
      "\n",
      "Training loss: 0.005493875334246291\n",
      "Validation loss: 0.003314606457327803\n",
      "\n",
      "EPOCH 35 of 50\n",
      "\n",
      "Training loss: 0.0054565404748751056\n",
      "Validation loss: 0.003312386960412065\n",
      "\n",
      "EPOCH 36 of 50\n",
      "\n",
      "Training loss: 0.005424766542596949\n",
      "Validation loss: 0.0033085171598941088\n",
      "\n",
      "EPOCH 37 of 50\n",
      "\n",
      "Training loss: 0.005323665113084846\n",
      "Validation loss: 0.003310395094255606\n",
      "\n",
      "EPOCH 38 of 50\n",
      "\n",
      "Training loss: 0.005575482914638188\n",
      "Validation loss: 0.0033096338156610727\n",
      "\n",
      "EPOCH 39 of 50\n",
      "\n",
      "Training loss: 0.0056544482294056155\n",
      "Validation loss: 0.003308957675471902\n",
      "\n",
      "EPOCH 40 of 50\n",
      "\n",
      "Training loss: 0.005449565903594096\n",
      "Validation loss: 0.0033118700763831535\n",
      "\n",
      "EPOCH 41 of 50\n",
      "\n",
      "Training loss: 0.005376939196139574\n",
      "Validation loss: 0.0033163136492172876\n",
      "\n",
      "EPOCH 42 of 50\n",
      "\n",
      "Training loss: 0.005359599677224954\n",
      "Validation loss: 0.0033111270361890397\n",
      "\n",
      "EPOCH 43 of 50\n",
      "\n",
      "Training loss: 0.005368740711775091\n",
      "Validation loss: 0.003312118894731005\n",
      "\n",
      "EPOCH 44 of 50\n",
      "\n",
      "Training loss: 0.0056375921703875065\n",
      "Validation loss: 0.0033144585322588682\n",
      "\n",
      "EPOCH 45 of 50\n",
      "\n",
      "Training loss: 0.005310096984936131\n",
      "Validation loss: 0.003312823362648487\n",
      "\n",
      "EPOCH 46 of 50\n",
      "\n",
      "Training loss: 0.005164355867438846\n",
      "Validation loss: 0.0033132288760195174\n",
      "\n",
      "EPOCH 47 of 50\n",
      "\n",
      "Training loss: 0.005223895164413584\n",
      "Validation loss: 0.0033096153444300094\n",
      "\n",
      "EPOCH 48 of 50\n",
      "\n",
      "Training loss: 0.005426587847371896\n",
      "Validation loss: 0.003314359967286388\n",
      "\n",
      "EPOCH 49 of 50\n",
      "\n",
      "Training loss: 0.005841342551219795\n",
      "Validation loss: 0.0033125027548521757\n",
      "\n",
      "EPOCH 50 of 50\n",
      "\n",
      "Training loss: 0.005450247579978572\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-30 05:59:47,588] Trial 35 finished with value: 0.0033096453795830407 and parameters: {'lr': 7.577783337831243e-05, 'weight_decay': 2.7326888652729394e-05, 'hidden_size': 485, 'dropout_rate': 0.4381306613750493}. Best is trial 16 with value: 0.0032828766076515117.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.0033096453795830407\n",
      "learning rate: 3.259939575665951e-05\n",
      "weight_decay: 1.7754732966956772e-05\n",
      "hidden_size: 549\n",
      "dropout_rate: 0.4996571687471073\n",
      "\n",
      "EPOCH 1 of 50\n",
      "\n",
      "Training loss: 0.005621276692383819\n",
      "Validation loss: 0.0035128076560795307\n",
      "\n",
      "EPOCH 2 of 50\n",
      "\n",
      "Training loss: 0.005172118570448624\n",
      "Validation loss: 0.003849105133364598\n",
      "\n",
      "EPOCH 3 of 50\n",
      "\n",
      "Training loss: 0.00551213753513164\n",
      "Validation loss: 0.004137822038804491\n",
      "\n",
      "EPOCH 4 of 50\n",
      "\n",
      "Training loss: 0.005407771431944436\n",
      "Validation loss: 0.00431489300293227\n",
      "\n",
      "EPOCH 5 of 50\n",
      "\n",
      "Training loss: 0.005515097071313196\n",
      "Validation loss: 0.00444939685985446\n",
      "\n",
      "EPOCH 6 of 50\n",
      "\n",
      "Training loss: 0.0056066807980338735\n",
      "Validation loss: 0.004515158866221706\n",
      "\n",
      "EPOCH 7 of 50\n",
      "\n",
      "Training loss: 0.005682367262327009\n",
      "Validation loss: 0.004430953025196989\n",
      "\n",
      "EPOCH 8 of 50\n",
      "\n",
      "Training loss: 0.0053990936527649564\n",
      "Validation loss: 0.0044683855182180805\n",
      "\n",
      "EPOCH 9 of 50\n",
      "\n",
      "Training loss: 0.005249685375019908\n",
      "Validation loss: 0.004377045125390093\n",
      "\n",
      "EPOCH 10 of 50\n",
      "\n",
      "Training loss: 0.005213131015706394\n",
      "Validation loss: 0.004391485204299291\n",
      "\n",
      "EPOCH 11 of 50\n",
      "\n",
      "Training loss: 0.005305121342341105\n",
      "Validation loss: 0.004430870680759351\n",
      "\n",
      "EPOCH 12 of 50\n",
      "\n",
      "Training loss: 0.0054334167960203355\n",
      "Validation loss: 0.004426423227414489\n",
      "\n",
      "EPOCH 13 of 50\n",
      "\n",
      "Training loss: 0.005495624513261848\n",
      "Validation loss: 0.004463486761475603\n",
      "\n",
      "EPOCH 14 of 50\n",
      "\n",
      "Training loss: 0.0054209613137775\n",
      "Validation loss: 0.00447234814055264\n",
      "\n",
      "EPOCH 15 of 50\n",
      "\n",
      "Training loss: 0.005356804415997531\n",
      "Validation loss: 0.004430227214470506\n",
      "\n",
      "EPOCH 16 of 50\n",
      "\n",
      "Training loss: 0.005193518009036779\n",
      "Validation loss: 0.004375784425064921\n",
      "\n",
      "EPOCH 17 of 50\n",
      "\n",
      "Training loss: 0.005724816686577267\n",
      "Validation loss: 0.004390559469660123\n",
      "\n",
      "EPOCH 18 of 50\n",
      "\n",
      "Training loss: 0.005542114118321074\n",
      "Validation loss: 0.004481615886713068\n",
      "\n",
      "EPOCH 19 of 50\n",
      "\n",
      "Training loss: 0.005598954597695006\n",
      "Validation loss: 0.004447006232415636\n",
      "\n",
      "EPOCH 20 of 50\n",
      "\n",
      "Training loss: 0.005559117921317617\n",
      "Validation loss: 0.004458192115028699\n",
      "\n",
      "EPOCH 21 of 50\n",
      "\n",
      "Training loss: 0.005476299776799149\n",
      "Validation loss: 0.0044386677133540315\n",
      "\n",
      "EPOCH 22 of 50\n",
      "\n",
      "Training loss: 0.005496379815869861\n",
      "Validation loss: 0.004384910920634866\n",
      "\n",
      "EPOCH 23 of 50\n",
      "\n",
      "Training loss: 0.0053386797404123675\n",
      "Validation loss: 0.00436726367721955\n",
      "\n",
      "EPOCH 24 of 50\n",
      "\n",
      "Training loss: 0.005666175960666603\n",
      "Validation loss: 0.0043680621311068535\n",
      "\n",
      "EPOCH 25 of 50\n",
      "\n",
      "Training loss: 0.0054518829306794536\n",
      "Validation loss: 0.004361226844290893\n",
      "\n",
      "EPOCH 26 of 50\n",
      "\n",
      "Training loss: 0.005235613272007968\n",
      "Validation loss: 0.004403181451683243\n",
      "\n",
      "EPOCH 27 of 50\n",
      "\n",
      "Training loss: 0.0054409084841609\n",
      "Validation loss: 0.004411068744957447\n",
      "\n",
      "EPOCH 28 of 50\n",
      "\n",
      "Training loss: 0.005385295565550526\n",
      "Validation loss: 0.004428303490082423\n",
      "\n",
      "EPOCH 29 of 50\n",
      "\n",
      "Training loss: 0.005484169193853934\n",
      "Validation loss: 0.004420783681174119\n",
      "\n",
      "EPOCH 30 of 50\n",
      "\n",
      "Training loss: 0.005393614216397206\n",
      "Validation loss: 0.004439374043916662\n",
      "\n",
      "EPOCH 31 of 50\n",
      "\n",
      "Training loss: 0.005540733613694708\n",
      "Validation loss: 0.004422150164221724\n",
      "\n",
      "EPOCH 32 of 50\n",
      "\n",
      "Training loss: 0.005767832924094465\n",
      "Validation loss: 0.004430071838820974\n",
      "\n",
      "EPOCH 33 of 50\n",
      "\n",
      "Training loss: 0.005469617019924853\n",
      "Validation loss: 0.004433699417859316\n",
      "\n",
      "EPOCH 34 of 50\n",
      "\n",
      "Training loss: 0.005464882098345293\n",
      "Validation loss: 0.004432408294330041\n",
      "\n",
      "EPOCH 35 of 50\n",
      "\n",
      "Training loss: 0.005631224976645576\n",
      "Validation loss: 0.00441357190720737\n",
      "\n",
      "EPOCH 36 of 50\n",
      "\n",
      "Training loss: 0.005581052042543888\n",
      "Validation loss: 0.004372773924842477\n",
      "\n",
      "EPOCH 37 of 50\n",
      "\n",
      "Training loss: 0.00534540999473797\n",
      "Validation loss: 0.004388243580857913\n",
      "\n",
      "EPOCH 38 of 50\n",
      "\n",
      "Training loss: 0.0057696619381507235\n",
      "Validation loss: 0.0044115933900078135\n",
      "\n",
      "EPOCH 39 of 50\n",
      "\n",
      "Training loss: 0.005530995989425315\n",
      "Validation loss: 0.004434683204938968\n",
      "\n",
      "EPOCH 40 of 50\n",
      "\n",
      "Training loss: 0.005463851719266838\n",
      "Validation loss: 0.0044619714220364886\n",
      "\n",
      "EPOCH 41 of 50\n",
      "\n",
      "Training loss: 0.005640480098211103\n",
      "Validation loss: 0.004504543806736668\n",
      "\n",
      "EPOCH 42 of 50\n",
      "\n",
      "Training loss: 0.00544611933744616\n",
      "Validation loss: 0.004406512094040711\n",
      "\n",
      "EPOCH 43 of 50\n",
      "\n",
      "Training loss: 0.005621357070696022\n",
      "Validation loss: 0.004418524991100033\n",
      "\n",
      "EPOCH 44 of 50\n",
      "\n",
      "Training loss: 0.005474552977830172\n",
      "Validation loss: 0.004451976468165715\n",
      "\n",
      "EPOCH 45 of 50\n",
      "\n",
      "Training loss: 0.0055747550084359115\n",
      "Validation loss: 0.004436982795596123\n",
      "\n",
      "EPOCH 46 of 50\n",
      "\n",
      "Training loss: 0.00527628555169536\n",
      "Validation loss: 0.004453291262810429\n",
      "\n",
      "EPOCH 47 of 50\n",
      "\n",
      "Training loss: 0.005214458227985435\n",
      "Validation loss: 0.004464910676081975\n",
      "\n",
      "EPOCH 48 of 50\n",
      "\n",
      "Training loss: 0.005357315608610709\n",
      "Validation loss: 0.00449492239082853\n",
      "\n",
      "EPOCH 49 of 50\n",
      "\n",
      "Training loss: 0.005370106703291337\n",
      "Validation loss: 0.004443537599096696\n",
      "\n",
      "EPOCH 50 of 50\n",
      "\n",
      "Training loss: 0.005388988233688805\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-30 06:03:53,236] Trial 36 finished with value: 0.0044653695076704025 and parameters: {'lr': 3.259939575665951e-05, 'weight_decay': 1.7754732966956772e-05, 'hidden_size': 549, 'dropout_rate': 0.4996571687471073}. Best is trial 16 with value: 0.0032828766076515117.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.0044653695076704025\n",
      "learning rate: 0.00010399989209040471\n",
      "weight_decay: 0.0001562825043185793\n",
      "hidden_size: 356\n",
      "dropout_rate: 0.4618641581819325\n",
      "\n",
      "EPOCH 1 of 50\n",
      "\n",
      "Training loss: 0.005414749754385816\n",
      "Validation loss: 0.0033590206876397133\n",
      "\n",
      "EPOCH 2 of 50\n",
      "\n",
      "Training loss: 0.005274002103962832\n",
      "Validation loss: 0.0033504550034801164\n",
      "\n",
      "EPOCH 3 of 50\n",
      "\n",
      "Training loss: 0.0056843507207102245\n",
      "Validation loss: 0.003350013867020607\n",
      "\n",
      "EPOCH 4 of 50\n",
      "\n",
      "Training loss: 0.005298774611825745\n",
      "Validation loss: 0.0033388372200230756\n",
      "\n",
      "EPOCH 5 of 50\n",
      "\n",
      "Training loss: 0.005522600530336301\n",
      "Validation loss: 0.003351909341290593\n",
      "\n",
      "EPOCH 6 of 50\n",
      "\n",
      "Training loss: 0.005578685758842362\n",
      "Validation loss: 0.003345673360551397\n",
      "\n",
      "EPOCH 7 of 50\n",
      "\n",
      "Training loss: 0.005467630250172483\n",
      "Validation loss: 0.003336475153143207\n",
      "\n",
      "EPOCH 8 of 50\n",
      "\n",
      "Training loss: 0.005235263767341773\n",
      "Validation loss: 0.003343974705785513\n",
      "\n",
      "EPOCH 9 of 50\n",
      "\n",
      "Training loss: 0.005492435406065649\n",
      "Validation loss: 0.003348353784531355\n",
      "\n",
      "EPOCH 10 of 50\n",
      "\n",
      "Training loss: 0.005366510271819102\n",
      "Validation loss: 0.003340605335930983\n",
      "\n",
      "EPOCH 11 of 50\n",
      "\n",
      "Training loss: 0.00537805962893698\n",
      "Validation loss: 0.0033403814304620028\n",
      "\n",
      "EPOCH 12 of 50\n",
      "\n",
      "Training loss: 0.005499494210299518\n",
      "Validation loss: 0.003336439918105801\n",
      "\n",
      "EPOCH 13 of 50\n",
      "\n",
      "Training loss: 0.00538040842447016\n",
      "Validation loss: 0.00334386364556849\n",
      "\n",
      "EPOCH 14 of 50\n",
      "\n",
      "Training loss: 0.005348076164308522\n",
      "Validation loss: 0.0033388795175900063\n",
      "\n",
      "EPOCH 15 of 50\n",
      "\n",
      "Training loss: 0.0053691670505536925\n",
      "Validation loss: 0.0033343061804771423\n",
      "\n",
      "EPOCH 16 of 50\n",
      "\n",
      "Training loss: 0.005668577298315035\n",
      "Validation loss: 0.00334788152637581\n",
      "\n",
      "EPOCH 17 of 50\n",
      "\n",
      "Training loss: 0.005551933362666104\n",
      "Validation loss: 0.0033388060983270407\n",
      "\n",
      "EPOCH 18 of 50\n",
      "\n",
      "Training loss: 0.005678390928854545\n",
      "Validation loss: 0.003350385387117664\n",
      "\n",
      "EPOCH 19 of 50\n",
      "\n",
      "Training loss: 0.00519293756224215\n",
      "Validation loss: 0.003341912136723598\n",
      "\n",
      "EPOCH 20 of 50\n",
      "\n",
      "Training loss: 0.005459540834029515\n",
      "Validation loss: 0.003335583566998442\n",
      "\n",
      "EPOCH 21 of 50\n",
      "\n",
      "Training loss: 0.005430336834655868\n",
      "Validation loss: 0.003342668293043971\n",
      "\n",
      "EPOCH 22 of 50\n",
      "\n",
      "Training loss: 0.00530799372225172\n",
      "Validation loss: 0.0033298744820058346\n",
      "\n",
      "EPOCH 23 of 50\n",
      "\n",
      "Training loss: 0.005804803946779834\n",
      "Validation loss: 0.003333915723487735\n",
      "\n",
      "EPOCH 24 of 50\n",
      "\n",
      "Training loss: 0.005576463364478614\n",
      "Validation loss: 0.003337427508085966\n",
      "\n",
      "EPOCH 25 of 50\n",
      "\n",
      "Training loss: 0.005411807292451461\n",
      "Validation loss: 0.003334161282206575\n",
      "\n",
      "EPOCH 26 of 50\n",
      "\n",
      "Training loss: 0.005377608299669292\n",
      "Validation loss: 0.00334939236442248\n",
      "\n",
      "EPOCH 27 of 50\n",
      "\n",
      "Training loss: 0.0054528188581267996\n",
      "Validation loss: 0.0033511975004027286\n",
      "\n",
      "EPOCH 28 of 50\n",
      "\n",
      "Training loss: 0.005347162692083253\n",
      "Validation loss: 0.0033464114336917796\n",
      "\n",
      "EPOCH 29 of 50\n",
      "\n",
      "Training loss: 0.005515187875264221\n",
      "Validation loss: 0.0033499719575047493\n",
      "\n",
      "EPOCH 30 of 50\n",
      "\n",
      "Training loss: 0.005365207118706571\n",
      "Validation loss: 0.003336218884214759\n",
      "\n",
      "EPOCH 31 of 50\n",
      "\n",
      "Training loss: 0.005539915938344266\n",
      "Validation loss: 0.0033430777645359435\n",
      "\n",
      "EPOCH 32 of 50\n",
      "\n",
      "Training loss: 0.00570899879352914\n",
      "Validation loss: 0.003324547472099463\n",
      "\n",
      "EPOCH 33 of 50\n",
      "\n",
      "Training loss: 0.0053818925387329524\n",
      "Validation loss: 0.0033332506039490304\n",
      "\n",
      "EPOCH 34 of 50\n",
      "\n",
      "Training loss: 0.00533432016770045\n",
      "Validation loss: 0.0033340605441480875\n",
      "\n",
      "EPOCH 35 of 50\n",
      "\n",
      "Training loss: 0.0053153062860171\n",
      "Validation loss: 0.0033238292671740055\n",
      "\n",
      "EPOCH 36 of 50\n",
      "\n",
      "Training loss: 0.005332427565008402\n",
      "Validation loss: 0.003338688751682639\n",
      "\n",
      "EPOCH 37 of 50\n",
      "\n",
      "Training loss: 0.005410565270317925\n",
      "Validation loss: 0.00333681881117324\n",
      "\n",
      "EPOCH 38 of 50\n",
      "\n",
      "Training loss: 0.005283192162298494\n",
      "Validation loss: 0.0033412297101070485\n",
      "\n",
      "EPOCH 39 of 50\n",
      "\n",
      "Training loss: 0.00541781277085344\n",
      "Validation loss: 0.0033250198854754367\n",
      "\n",
      "EPOCH 40 of 50\n",
      "\n",
      "Training loss: 0.005552277279396852\n",
      "Validation loss: 0.0033394142519682646\n",
      "\n",
      "EPOCH 41 of 50\n",
      "\n",
      "Training loss: 0.005361156952049997\n",
      "Validation loss: 0.0033519677041719356\n",
      "\n",
      "EPOCH 42 of 50\n",
      "\n",
      "Training loss: 0.00526668814321359\n",
      "Validation loss: 0.003339133302991589\n",
      "\n",
      "EPOCH 43 of 50\n",
      "\n",
      "Training loss: 0.0053600105457007885\n",
      "Validation loss: 0.0033361926519622407\n",
      "\n",
      "EPOCH 44 of 50\n",
      "\n",
      "Training loss: 0.005584409977826808\n",
      "Validation loss: 0.0033351872116327286\n",
      "\n",
      "EPOCH 45 of 50\n",
      "\n",
      "Training loss: 0.005410111147082514\n",
      "Validation loss: 0.0033440254628658295\n",
      "\n",
      "EPOCH 46 of 50\n",
      "\n",
      "Training loss: 0.005559472625868188\n",
      "Validation loss: 0.0033444894943386316\n",
      "\n",
      "EPOCH 47 of 50\n",
      "\n",
      "Training loss: 0.005638620919651455\n",
      "Validation loss: 0.003333266436432799\n",
      "\n",
      "EPOCH 48 of 50\n",
      "\n",
      "Training loss: 0.005449977884483006\n",
      "Validation loss: 0.003340081855033835\n",
      "\n",
      "EPOCH 49 of 50\n",
      "\n",
      "Training loss: 0.005922325679825412\n",
      "Validation loss: 0.003350342313448588\n",
      "\n",
      "EPOCH 50 of 50\n",
      "\n",
      "Training loss: 0.005463884677737951\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-30 06:08:01,768] Trial 37 finished with value: 0.003340318566188216 and parameters: {'lr': 0.00010399989209040471, 'weight_decay': 0.0001562825043185793, 'hidden_size': 356, 'dropout_rate': 0.4618641581819325}. Best is trial 16 with value: 0.0032828766076515117.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.003340318566188216\n",
      "learning rate: 4.232968377905399e-05\n",
      "weight_decay: 1.4416232839027517e-05\n",
      "hidden_size: 323\n",
      "dropout_rate: 0.4781835259644503\n",
      "\n",
      "EPOCH 1 of 50\n",
      "\n",
      "Training loss: 0.005452922959294584\n",
      "Validation loss: 0.0033065210251758495\n",
      "\n",
      "EPOCH 2 of 50\n",
      "\n",
      "Training loss: 0.005772392006797923\n",
      "Validation loss: 0.0033090709087749324\n",
      "\n",
      "EPOCH 3 of 50\n",
      "\n",
      "Training loss: 0.005473350226465199\n",
      "Validation loss: 0.0033023436553776264\n",
      "\n",
      "EPOCH 4 of 50\n",
      "\n",
      "Training loss: 0.005557785172843271\n",
      "Validation loss: 0.0033131520419071117\n",
      "\n",
      "EPOCH 5 of 50\n",
      "\n",
      "Training loss: 0.005370207596570253\n",
      "Validation loss: 0.0033053087536245584\n",
      "\n",
      "EPOCH 6 of 50\n",
      "\n",
      "Training loss: 0.005371390065799157\n",
      "Validation loss: 0.0033057387142131725\n",
      "\n",
      "EPOCH 7 of 50\n",
      "\n",
      "Training loss: 0.005402465557886494\n",
      "Validation loss: 0.0033071580498168864\n",
      "\n",
      "EPOCH 8 of 50\n",
      "\n",
      "Training loss: 0.0053948296958373654\n",
      "Validation loss: 0.0033003761588285365\n",
      "\n",
      "EPOCH 9 of 50\n",
      "\n",
      "Training loss: 0.0051455179539819556\n",
      "Validation loss: 0.0033024849059681096\n",
      "\n",
      "EPOCH 10 of 50\n",
      "\n",
      "Training loss: 0.005361066432669759\n",
      "Validation loss: 0.0033037870501478515\n",
      "\n",
      "EPOCH 11 of 50\n",
      "\n",
      "Training loss: 0.005711122053778834\n",
      "Validation loss: 0.0033031885977834463\n",
      "\n",
      "EPOCH 12 of 50\n",
      "\n",
      "Training loss: 0.005274870407043232\n",
      "Validation loss: 0.003305780701339245\n",
      "\n",
      "EPOCH 13 of 50\n",
      "\n",
      "Training loss: 0.005314154239992301\n",
      "Validation loss: 0.003306141976887981\n",
      "\n",
      "EPOCH 14 of 50\n",
      "\n",
      "Training loss: 0.005457005153099696\n",
      "Validation loss: 0.0033047526764372983\n",
      "\n",
      "EPOCH 15 of 50\n",
      "\n",
      "Training loss: 0.005782260300798548\n",
      "Validation loss: 0.003303742269054055\n",
      "\n",
      "EPOCH 16 of 50\n",
      "\n",
      "Training loss: 0.005385140784912639\n",
      "Validation loss: 0.00330200232565403\n",
      "\n",
      "EPOCH 17 of 50\n",
      "\n",
      "Training loss: 0.005383983254432678\n",
      "Validation loss: 0.003301304066553712\n",
      "\n",
      "EPOCH 18 of 50\n",
      "\n",
      "Training loss: 0.005396730783912871\n",
      "Validation loss: 0.003301795804873109\n",
      "\n",
      "EPOCH 19 of 50\n",
      "\n",
      "Training loss: 0.005447496349612872\n",
      "Validation loss: 0.0032995345536619425\n",
      "\n",
      "EPOCH 20 of 50\n",
      "\n",
      "Training loss: 0.005169295576504535\n",
      "Validation loss: 0.003299030940979719\n",
      "\n",
      "EPOCH 21 of 50\n",
      "\n",
      "Training loss: 0.0054582330243041115\n",
      "Validation loss: 0.0033050791826099157\n",
      "\n",
      "EPOCH 22 of 50\n",
      "\n",
      "Training loss: 0.00532233373572429\n",
      "Validation loss: 0.003302981456120809\n",
      "\n",
      "EPOCH 23 of 50\n",
      "\n",
      "Training loss: 0.005576060360504521\n",
      "Validation loss: 0.003300206425289313\n",
      "\n",
      "EPOCH 24 of 50\n",
      "\n",
      "Training loss: 0.005283998894608683\n",
      "Validation loss: 0.003303281031548977\n",
      "\n",
      "EPOCH 25 of 50\n",
      "\n",
      "Training loss: 0.005726961108545463\n",
      "Validation loss: 0.0033054685530563197\n",
      "\n",
      "EPOCH 26 of 50\n",
      "\n",
      "Training loss: 0.0053213543724268675\n",
      "Validation loss: 0.003299465480570992\n",
      "\n",
      "EPOCH 27 of 50\n",
      "\n",
      "Training loss: 0.005333784243298901\n",
      "Validation loss: 0.0033005676232278347\n",
      "\n",
      "EPOCH 28 of 50\n",
      "\n",
      "Training loss: 0.00523665552544925\n",
      "Validation loss: 0.0033001143019646406\n",
      "\n",
      "EPOCH 29 of 50\n",
      "\n",
      "Training loss: 0.005357390450727608\n",
      "Validation loss: 0.003307235815251867\n",
      "\n",
      "EPOCH 30 of 50\n",
      "\n",
      "Training loss: 0.0052325911819934845\n",
      "Validation loss: 0.0033024613900731006\n",
      "\n",
      "EPOCH 31 of 50\n",
      "\n",
      "Training loss: 0.005599085396776597\n",
      "Validation loss: 0.0033040416116515794\n",
      "\n",
      "EPOCH 32 of 50\n",
      "\n",
      "Training loss: 0.005491019485311376\n",
      "Validation loss: 0.003304588453223308\n",
      "\n",
      "EPOCH 33 of 50\n",
      "\n",
      "Training loss: 0.0054355294236706365\n",
      "Validation loss: 0.0033066872662554183\n",
      "\n",
      "EPOCH 34 of 50\n",
      "\n",
      "Training loss: 0.00512321047588355\n",
      "Validation loss: 0.003305455669760704\n",
      "\n",
      "EPOCH 35 of 50\n",
      "\n",
      "Training loss: 0.005447061189139883\n",
      "Validation loss: 0.0033057404992481074\n",
      "\n",
      "EPOCH 36 of 50\n",
      "\n",
      "Training loss: 0.005388974651901258\n",
      "Validation loss: 0.0033037460719545684\n",
      "\n",
      "EPOCH 37 of 50\n",
      "\n",
      "Training loss: 0.005612774415769511\n",
      "Validation loss: 0.0033030599976579347\n",
      "\n",
      "EPOCH 38 of 50\n",
      "\n",
      "Training loss: 0.005321513757937484\n",
      "Validation loss: 0.003302240356182059\n",
      "\n",
      "EPOCH 39 of 50\n",
      "\n",
      "Training loss: 0.005436543168293105\n",
      "Validation loss: 0.0033048149198293686\n",
      "\n",
      "EPOCH 40 of 50\n",
      "\n",
      "Training loss: 0.0054443761085470515\n",
      "Validation loss: 0.0033055117819458246\n",
      "\n",
      "EPOCH 41 of 50\n",
      "\n",
      "Training loss: 0.0054965915882753\n",
      "Validation loss: 0.0032997861659775176\n",
      "\n",
      "EPOCH 42 of 50\n",
      "\n",
      "Training loss: 0.005446090130135417\n",
      "Validation loss: 0.0032995368043581643\n",
      "\n",
      "EPOCH 43 of 50\n",
      "\n",
      "Training loss: 0.0054337864017321\n",
      "Validation loss: 0.003308155961955587\n",
      "\n",
      "EPOCH 44 of 50\n",
      "\n",
      "Training loss: 0.005595382173649139\n",
      "Validation loss: 0.003303599233428637\n",
      "\n",
      "EPOCH 45 of 50\n",
      "\n",
      "Training loss: 0.005507413090931045\n",
      "Validation loss: 0.003300646785646677\n",
      "\n",
      "EPOCH 46 of 50\n",
      "\n",
      "Training loss: 0.005768031192322572\n",
      "Validation loss: 0.0033006652568777404\n",
      "\n",
      "EPOCH 47 of 50\n",
      "\n",
      "Training loss: 0.005262261101355155\n",
      "Validation loss: 0.003299008666848143\n",
      "\n",
      "EPOCH 48 of 50\n",
      "\n",
      "Training loss: 0.005315635534417298\n",
      "Validation loss: 0.003299197337279717\n",
      "\n",
      "EPOCH 49 of 50\n",
      "\n",
      "Training loss: 0.00520894852363401\n",
      "Validation loss: 0.003300161644195517\n",
      "\n",
      "EPOCH 50 of 50\n",
      "\n",
      "Training loss: 0.0058173429117434556\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-30 06:12:14,047] Trial 38 finished with value: 0.0032997251643488803 and parameters: {'lr': 4.232968377905399e-05, 'weight_decay': 1.4416232839027517e-05, 'hidden_size': 323, 'dropout_rate': 0.4781835259644503}. Best is trial 16 with value: 0.0032828766076515117.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.0032997251643488803\n",
      "learning rate: 0.00048575878929875025\n",
      "weight_decay: 7.594268791607016e-05\n",
      "hidden_size: 405\n",
      "dropout_rate: 0.43469279703408564\n",
      "\n",
      "EPOCH 1 of 50\n",
      "\n",
      "Training loss: 0.0052465941860444015\n",
      "Validation loss: 0.003393014737715324\n",
      "\n",
      "EPOCH 2 of 50\n",
      "\n",
      "Training loss: 0.005313242708022396\n",
      "Validation loss: 0.0034603822665909925\n",
      "\n",
      "EPOCH 3 of 50\n",
      "\n",
      "Training loss: 0.00511676758631236\n",
      "Validation loss: 0.0034937733629097543\n",
      "\n",
      "EPOCH 4 of 50\n",
      "\n",
      "Training loss: 0.0050555044371220804\n",
      "Validation loss: 0.0035159955732524395\n",
      "\n",
      "EPOCH 5 of 50\n",
      "\n",
      "Training loss: 0.004913261263734764\n",
      "Validation loss: 0.0035250003760059676\n",
      "\n",
      "EPOCH 6 of 50\n",
      "\n",
      "Training loss: 0.005074785504904058\n",
      "Validation loss: 0.003530488427107533\n",
      "\n",
      "EPOCH 7 of 50\n",
      "\n",
      "Training loss: 0.005117353388211793\n",
      "Validation loss: 0.0035259281285107136\n",
      "\n",
      "EPOCH 8 of 50\n",
      "\n",
      "Training loss: 0.005286893859091733\n",
      "Validation loss: 0.0035080964056154094\n",
      "\n",
      "EPOCH 9 of 50\n",
      "\n",
      "Training loss: 0.005102927175660928\n",
      "Validation loss: 0.003503897304957112\n",
      "\n",
      "EPOCH 10 of 50\n",
      "\n",
      "Training loss: 0.005217504946308004\n",
      "Validation loss: 0.0035048784532894692\n",
      "\n",
      "EPOCH 11 of 50\n",
      "\n",
      "Training loss: 0.004985596312003003\n",
      "Validation loss: 0.0035371362852553525\n",
      "\n",
      "EPOCH 12 of 50\n",
      "\n",
      "Training loss: 0.005215179744280047\n",
      "Validation loss: 0.003560056754698356\n",
      "\n",
      "EPOCH 13 of 50\n",
      "\n",
      "Training loss: 0.005033607827499509\n",
      "Validation loss: 0.0035277020651847124\n",
      "\n",
      "EPOCH 14 of 50\n",
      "\n",
      "Training loss: 0.005237706264273988\n",
      "Validation loss: 0.0035299664984146753\n",
      "\n",
      "EPOCH 15 of 50\n",
      "\n",
      "Training loss: 0.005189622493667735\n",
      "Validation loss: 0.00350580713711679\n",
      "\n",
      "EPOCH 16 of 50\n",
      "\n",
      "Training loss: 0.005034522308657567\n",
      "Validation loss: 0.0035194027392814555\n",
      "\n",
      "EPOCH 17 of 50\n",
      "\n",
      "Training loss: 0.005051890077690284\n",
      "Validation loss: 0.003505141163865725\n",
      "\n",
      "EPOCH 18 of 50\n",
      "\n",
      "Training loss: 0.00538355800219708\n",
      "Validation loss: 0.0035470496707906327\n",
      "\n",
      "EPOCH 19 of 50\n",
      "\n",
      "Training loss: 0.0050987292391558485\n",
      "Validation loss: 0.0035250596702098846\n",
      "\n",
      "EPOCH 20 of 50\n",
      "\n",
      "Training loss: 0.005097664944413636\n",
      "Validation loss: 0.0035292561321208873\n",
      "\n",
      "EPOCH 21 of 50\n",
      "\n",
      "Training loss: 0.00537632985247506\n",
      "Validation loss: 0.003538643398011724\n",
      "\n",
      "EPOCH 22 of 50\n",
      "\n",
      "Training loss: 0.004960712174781495\n",
      "Validation loss: 0.003560098120942712\n",
      "\n",
      "EPOCH 23 of 50\n",
      "\n",
      "Training loss: 0.00536494603794482\n",
      "Validation loss: 0.0035367871169000864\n",
      "\n",
      "EPOCH 24 of 50\n",
      "\n",
      "Training loss: 0.005187829335530599\n",
      "Validation loss: 0.003509894867117206\n",
      "\n",
      "EPOCH 25 of 50\n",
      "\n",
      "Training loss: 0.00502743473690417\n",
      "Validation loss: 0.0034814372969170413\n",
      "\n",
      "EPOCH 26 of 50\n",
      "\n",
      "Training loss: 0.004972834631593691\n",
      "Validation loss: 0.003511450020596385\n",
      "\n",
      "EPOCH 27 of 50\n",
      "\n",
      "Training loss: 0.0050674014621310765\n",
      "Validation loss: 0.00352996742973725\n",
      "\n",
      "EPOCH 28 of 50\n",
      "\n",
      "Training loss: 0.0049610090338521535\n",
      "Validation loss: 0.003559546157096823\n",
      "\n",
      "EPOCH 29 of 50\n",
      "\n",
      "Training loss: 0.005230421303874916\n",
      "Validation loss: 0.003527141564215223\n",
      "\n",
      "EPOCH 30 of 50\n",
      "\n",
      "Training loss: 0.005229506382925643\n",
      "Validation loss: 0.0035597652507325015\n",
      "\n",
      "EPOCH 31 of 50\n",
      "\n",
      "Training loss: 0.0051332515560918385\n",
      "Validation loss: 0.003539545306315025\n",
      "\n",
      "EPOCH 32 of 50\n",
      "\n",
      "Training loss: 0.00492685430476235\n",
      "Validation loss: 0.003493621557330092\n",
      "\n",
      "EPOCH 33 of 50\n",
      "\n",
      "Training loss: 0.005153764401459032\n",
      "Validation loss: 0.0035190220611790815\n",
      "\n",
      "EPOCH 34 of 50\n",
      "\n",
      "Training loss: 0.0051964036085539395\n",
      "Validation loss: 0.0035318870407839618\n",
      "\n",
      "EPOCH 35 of 50\n",
      "\n",
      "Training loss: 0.005041477529125081\n",
      "Validation loss: 0.003490733215585351\n",
      "\n",
      "EPOCH 36 of 50\n",
      "\n",
      "Training loss: 0.005193090350884531\n",
      "Validation loss: 0.0035194209000716605\n",
      "\n",
      "EPOCH 37 of 50\n",
      "\n",
      "Training loss: 0.0051581403240561485\n",
      "Validation loss: 0.003511752157161633\n",
      "\n",
      "EPOCH 38 of 50\n",
      "\n",
      "Training loss: 0.005301165704925855\n",
      "Validation loss: 0.0035255396893868842\n",
      "\n",
      "EPOCH 39 of 50\n",
      "\n",
      "Training loss: 0.005065747743679417\n",
      "Validation loss: 0.0034844237379729748\n",
      "\n",
      "EPOCH 40 of 50\n",
      "\n",
      "Training loss: 0.004991176150118311\n",
      "Validation loss: 0.0034696386816600957\n",
      "\n",
      "EPOCH 41 of 50\n",
      "\n",
      "Training loss: 0.005248900192479293\n",
      "Validation loss: 0.003487531871845325\n",
      "\n",
      "EPOCH 42 of 50\n",
      "\n",
      "Training loss: 0.004970122930697269\n",
      "Validation loss: 0.0035206477623432875\n",
      "\n",
      "EPOCH 43 of 50\n",
      "\n",
      "Training loss: 0.005029723022340072\n",
      "Validation loss: 0.003524605417624116\n",
      "\n",
      "EPOCH 44 of 50\n",
      "\n",
      "Training loss: 0.0052230944857001305\n",
      "Validation loss: 0.0035346982379754386\n",
      "\n",
      "EPOCH 45 of 50\n",
      "\n",
      "Training loss: 0.005032592763503392\n",
      "Validation loss: 0.003514215194930633\n",
      "\n",
      "EPOCH 46 of 50\n",
      "\n",
      "Training loss: 0.005073215993535187\n",
      "Validation loss: 0.0034890288952738047\n",
      "\n",
      "EPOCH 47 of 50\n",
      "\n",
      "Training loss: 0.005098237035175164\n",
      "Validation loss: 0.0035458664254595837\n",
      "\n",
      "EPOCH 48 of 50\n",
      "\n",
      "Training loss: 0.005113610583874915\n",
      "Validation loss: 0.003533353563398123\n",
      "\n",
      "EPOCH 49 of 50\n",
      "\n",
      "Training loss: 0.0051940333408614\n",
      "Validation loss: 0.003532398802538713\n",
      "\n",
      "EPOCH 50 of 50\n",
      "\n",
      "Training loss: 0.005038565179953973\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-30 06:16:18,108] Trial 39 finished with value: 0.003549878718331456 and parameters: {'lr': 0.00048575878929875025, 'weight_decay': 7.594268791607016e-05, 'hidden_size': 405, 'dropout_rate': 0.43469279703408564}. Best is trial 16 with value: 0.0032828766076515117.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.003549878718331456\n",
      "learning rate: 0.0001842227793844272\n",
      "weight_decay: 5.175266640675241e-05\n",
      "hidden_size: 335\n",
      "dropout_rate: 0.45201535983694485\n",
      "\n",
      "EPOCH 1 of 50\n",
      "\n",
      "Training loss: 0.005675247663425075\n",
      "Validation loss: 0.0033927567613621554\n",
      "\n",
      "EPOCH 2 of 50\n",
      "\n",
      "Training loss: 0.005254998622048233\n",
      "Validation loss: 0.003450237214565277\n",
      "\n",
      "EPOCH 3 of 50\n",
      "\n",
      "Training loss: 0.005422433993468682\n",
      "Validation loss: 0.0034975920959065356\n",
      "\n",
      "EPOCH 4 of 50\n",
      "\n",
      "Training loss: 0.005311050297071536\n",
      "Validation loss: 0.0035443405310312905\n",
      "\n",
      "EPOCH 5 of 50\n",
      "\n",
      "Training loss: 0.005288443942036893\n",
      "Validation loss: 0.003543629233414928\n",
      "\n",
      "EPOCH 6 of 50\n",
      "\n",
      "Training loss: 0.005569967648221387\n",
      "Validation loss: 0.0035137055286516747\n",
      "\n",
      "EPOCH 7 of 50\n",
      "\n",
      "Training loss: 0.004963347093305654\n",
      "Validation loss: 0.003559454266602794\n",
      "\n",
      "EPOCH 8 of 50\n",
      "\n",
      "Training loss: 0.0051852380856871605\n",
      "Validation loss: 0.0035384738196929297\n",
      "\n",
      "EPOCH 9 of 50\n",
      "\n",
      "Training loss: 0.005304099112335179\n",
      "Validation loss: 0.003542915297051271\n",
      "\n",
      "EPOCH 10 of 50\n",
      "\n",
      "Training loss: 0.005358679789221949\n",
      "Validation loss: 0.003577529530351361\n",
      "\n",
      "EPOCH 11 of 50\n",
      "\n",
      "Training loss: 0.0051463439336253535\n",
      "Validation loss: 0.0035506224570175013\n",
      "\n",
      "EPOCH 12 of 50\n",
      "\n",
      "Training loss: 0.005820263332376878\n",
      "Validation loss: 0.0035566517617553473\n",
      "\n",
      "EPOCH 13 of 50\n",
      "\n",
      "Training loss: 0.005256407868324054\n",
      "Validation loss: 0.0035539579888184867\n",
      "\n",
      "EPOCH 14 of 50\n",
      "\n",
      "Training loss: 0.005420610256907012\n",
      "Validation loss: 0.003545612950498859\n",
      "\n",
      "EPOCH 15 of 50\n",
      "\n",
      "Training loss: 0.005241729319095612\n",
      "Validation loss: 0.003543102958550056\n",
      "\n",
      "EPOCH 16 of 50\n",
      "\n",
      "Training loss: 0.005175483024989565\n",
      "Validation loss: 0.0035270014001677432\n",
      "\n",
      "EPOCH 17 of 50\n",
      "\n",
      "Training loss: 0.005234779505473044\n",
      "Validation loss: 0.003557676449418068\n",
      "\n",
      "EPOCH 18 of 50\n",
      "\n",
      "Training loss: 0.005212166553570164\n",
      "Validation loss: 0.0035299055743962526\n",
      "\n",
      "EPOCH 19 of 50\n",
      "\n",
      "Training loss: 0.005286628664988611\n",
      "Validation loss: 0.0035381920946141085\n",
      "\n",
      "EPOCH 20 of 50\n",
      "\n",
      "Training loss: 0.005363044070286883\n",
      "Validation loss: 0.003544841039304932\n",
      "\n",
      "EPOCH 21 of 50\n",
      "\n",
      "Training loss: 0.005267609971471959\n",
      "Validation loss: 0.003523981353888909\n",
      "\n",
      "EPOCH 22 of 50\n",
      "\n",
      "Training loss: 0.00535301517488228\n",
      "Validation loss: 0.0035616623548169932\n",
      "\n",
      "EPOCH 23 of 50\n",
      "\n",
      "Training loss: 0.0051741281317340005\n",
      "Validation loss: 0.003578478004783392\n",
      "\n",
      "EPOCH 24 of 50\n",
      "\n",
      "Training loss: 0.005247722069422404\n",
      "Validation loss: 0.003590129315853119\n",
      "\n",
      "EPOCH 25 of 50\n",
      "\n",
      "Training loss: 0.005363907302833266\n",
      "Validation loss: 0.003549204828838507\n",
      "\n",
      "EPOCH 26 of 50\n",
      "\n",
      "Training loss: 0.00517197201649348\n",
      "Validation loss: 0.0035487653221935034\n",
      "\n",
      "EPOCH 27 of 50\n",
      "\n",
      "Training loss: 0.005302983180930217\n",
      "Validation loss: 0.003572270351772507\n",
      "\n",
      "EPOCH 28 of 50\n",
      "\n",
      "Training loss: 0.005107961336357726\n",
      "Validation loss: 0.003583685572569569\n",
      "\n",
      "EPOCH 29 of 50\n",
      "\n",
      "Training loss: 0.0053240083571937345\n",
      "Validation loss: 0.0035483178993066153\n",
      "\n",
      "EPOCH 30 of 50\n",
      "\n",
      "Training loss: 0.005559423990133736\n",
      "Validation loss: 0.003556498559191823\n",
      "\n",
      "EPOCH 31 of 50\n",
      "\n",
      "Training loss: 0.005197821909354793\n",
      "Validation loss: 0.0035539076973994574\n",
      "\n",
      "EPOCH 32 of 50\n",
      "\n",
      "Training loss: 0.005301575150547756\n",
      "Validation loss: 0.003515041433274746\n",
      "\n",
      "EPOCH 33 of 50\n",
      "\n",
      "Training loss: 0.005655977202372419\n",
      "Validation loss: 0.00351445353589952\n",
      "\n",
      "EPOCH 34 of 50\n",
      "\n",
      "Training loss: 0.005422632365177075\n",
      "Validation loss: 0.0035256710834801197\n",
      "\n",
      "EPOCH 35 of 50\n",
      "\n",
      "Training loss: 0.0052030267090433175\n",
      "Validation loss: 0.0035336231036732593\n",
      "\n",
      "EPOCH 36 of 50\n",
      "\n",
      "Training loss: 0.005205742057619823\n",
      "Validation loss: 0.0035305203249057135\n",
      "\n",
      "EPOCH 37 of 50\n",
      "\n",
      "Training loss: 0.005376404099580314\n",
      "Validation loss: 0.0035246359184384346\n",
      "\n",
      "EPOCH 38 of 50\n",
      "\n",
      "Training loss: 0.005505873200794061\n",
      "Validation loss: 0.0035447836853563786\n",
      "\n",
      "EPOCH 39 of 50\n",
      "\n",
      "Training loss: 0.005520391433189313\n",
      "Validation loss: 0.003528531019886335\n",
      "\n",
      "EPOCH 40 of 50\n",
      "\n",
      "Training loss: 0.005086822383519676\n",
      "Validation loss: 0.0035647190331170955\n",
      "\n",
      "EPOCH 41 of 50\n",
      "\n",
      "Training loss: 0.005448835850175884\n",
      "Validation loss: 0.0035781565432747207\n",
      "\n",
      "EPOCH 42 of 50\n",
      "\n",
      "Training loss: 0.005273119287772311\n",
      "Validation loss: 0.0035569237855573497\n",
      "\n",
      "EPOCH 43 of 50\n",
      "\n",
      "Training loss: 0.0053373412746522166\n",
      "Validation loss: 0.003543657328312596\n",
      "\n",
      "EPOCH 44 of 50\n",
      "\n",
      "Training loss: 0.005186973812265528\n",
      "Validation loss: 0.0035315383380899825\n",
      "\n",
      "EPOCH 45 of 50\n",
      "\n",
      "Training loss: 0.005161189370685154\n",
      "Validation loss: 0.003538004277894894\n",
      "\n",
      "EPOCH 46 of 50\n",
      "\n",
      "Training loss: 0.005348499502158827\n",
      "Validation loss: 0.0035298064661522708\n",
      "\n",
      "EPOCH 47 of 50\n",
      "\n",
      "Training loss: 0.005447726024107801\n",
      "Validation loss: 0.0035582076913366714\n",
      "\n",
      "EPOCH 48 of 50\n",
      "\n",
      "Training loss: 0.005103721076415645\n",
      "Validation loss: 0.003540033164123694\n",
      "\n",
      "EPOCH 49 of 50\n",
      "\n",
      "Training loss: 0.005310729456444581\n",
      "Validation loss: 0.0035479878230641284\n",
      "\n",
      "EPOCH 50 of 50\n",
      "\n",
      "Training loss: 0.005135370056248373\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-30 06:20:16,035] Trial 40 finished with value: 0.0035680632572621107 and parameters: {'lr': 0.0001842227793844272, 'weight_decay': 5.175266640675241e-05, 'hidden_size': 335, 'dropout_rate': 0.45201535983694485}. Best is trial 16 with value: 0.0032828766076515117.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.0035680632572621107\n",
      "learning rate: 0.000295932904688298\n",
      "weight_decay: 4.808719303736067e-05\n",
      "hidden_size: 449\n",
      "dropout_rate: 0.43144151419422033\n",
      "\n",
      "EPOCH 1 of 50\n",
      "\n",
      "Training loss: 0.005304044371263849\n",
      "Validation loss: 0.003320570414264997\n",
      "\n",
      "EPOCH 2 of 50\n",
      "\n",
      "Training loss: 0.005271008238196373\n",
      "Validation loss: 0.0033630153629928827\n",
      "\n",
      "EPOCH 3 of 50\n",
      "\n",
      "Training loss: 0.004970557987689972\n",
      "Validation loss: 0.0034141449723392725\n",
      "\n",
      "EPOCH 4 of 50\n",
      "\n",
      "Training loss: 0.005225195782259107\n",
      "Validation loss: 0.003422837626809875\n",
      "\n",
      "EPOCH 5 of 50\n",
      "\n",
      "Training loss: 0.005454830618368255\n",
      "Validation loss: 0.003418705891817808\n",
      "\n",
      "EPOCH 6 of 50\n",
      "\n",
      "Training loss: 0.005274200242840582\n",
      "Validation loss: 0.003435314322511355\n",
      "\n",
      "EPOCH 7 of 50\n",
      "\n",
      "Training loss: 0.0055693697391284835\n",
      "Validation loss: 0.0034282195071379342\n",
      "\n",
      "EPOCH 8 of 50\n",
      "\n",
      "Training loss: 0.005263312823242611\n",
      "Validation loss: 0.0034205911215394735\n",
      "\n",
      "EPOCH 9 of 50\n",
      "\n",
      "Training loss: 0.005089727359720402\n",
      "Validation loss: 0.0034256549552083015\n",
      "\n",
      "EPOCH 10 of 50\n",
      "\n",
      "Training loss: 0.005380560385270251\n",
      "Validation loss: 0.003456278471276164\n",
      "\n",
      "EPOCH 11 of 50\n",
      "\n",
      "Training loss: 0.005412195188303788\n",
      "Validation loss: 0.003428015081832806\n",
      "\n",
      "EPOCH 12 of 50\n",
      "\n",
      "Training loss: 0.005209552434583505\n",
      "Validation loss: 0.003449147567152977\n",
      "\n",
      "EPOCH 13 of 50\n",
      "\n",
      "Training loss: 0.005281186714354489\n",
      "Validation loss: 0.0034413861576467752\n",
      "\n",
      "EPOCH 14 of 50\n",
      "\n",
      "Training loss: 0.0053939609529657494\n",
      "Validation loss: 0.0034339583944529295\n",
      "\n",
      "EPOCH 15 of 50\n",
      "\n",
      "Training loss: 0.005096522806626227\n",
      "Validation loss: 0.003431231171513597\n",
      "\n",
      "EPOCH 16 of 50\n",
      "\n",
      "Training loss: 0.005334099651210838\n",
      "Validation loss: 0.003443864251797398\n",
      "\n",
      "EPOCH 17 of 50\n",
      "\n",
      "Training loss: 0.005138574788967769\n",
      "Validation loss: 0.003424868763734897\n",
      "\n",
      "EPOCH 18 of 50\n",
      "\n",
      "Training loss: 0.005482090016206105\n",
      "Validation loss: 0.0034318265970796347\n",
      "\n",
      "EPOCH 19 of 50\n",
      "\n",
      "Training loss: 0.005426227347925305\n",
      "Validation loss: 0.0034416453757633767\n",
      "\n",
      "EPOCH 20 of 50\n",
      "\n",
      "Training loss: 0.005256632084233893\n",
      "Validation loss: 0.003453334871058663\n",
      "\n",
      "EPOCH 21 of 50\n",
      "\n",
      "Training loss: 0.005350557673308585\n",
      "Validation loss: 0.003435450606048107\n",
      "\n",
      "EPOCH 22 of 50\n",
      "\n",
      "Training loss: 0.005321481523828374\n",
      "Validation loss: 0.003436461789533496\n",
      "\n",
      "EPOCH 23 of 50\n",
      "\n",
      "Training loss: 0.005129528387139241\n",
      "Validation loss: 0.003413523624961575\n",
      "\n",
      "EPOCH 24 of 50\n",
      "\n",
      "Training loss: 0.00504822392637531\n",
      "Validation loss: 0.003410961013287306\n",
      "\n",
      "EPOCH 25 of 50\n",
      "\n",
      "Training loss: 0.0051067497374282945\n",
      "Validation loss: 0.003422750858590007\n",
      "\n",
      "EPOCH 26 of 50\n",
      "\n",
      "Training loss: 0.0052498822203940814\n",
      "Validation loss: 0.0034168726609398923\n",
      "\n",
      "EPOCH 27 of 50\n",
      "\n",
      "Training loss: 0.005452470077822606\n",
      "Validation loss: 0.0034345507932205996\n",
      "\n",
      "EPOCH 28 of 50\n",
      "\n",
      "Training loss: 0.0054484582506120205\n",
      "Validation loss: 0.0034337083343416452\n",
      "\n",
      "EPOCH 29 of 50\n",
      "\n",
      "Training loss: 0.005365468095988035\n",
      "Validation loss: 0.0034554065205156803\n",
      "\n",
      "EPOCH 30 of 50\n",
      "\n",
      "Training loss: 0.0053163628714780016\n",
      "Validation loss: 0.003430359996855259\n",
      "\n",
      "EPOCH 31 of 50\n",
      "\n",
      "Training loss: 0.0054274760704073645\n",
      "Validation loss: 0.003454706243549784\n",
      "\n",
      "EPOCH 32 of 50\n",
      "\n",
      "Training loss: 0.005158796777120895\n",
      "Validation loss: 0.0034395971645911536\n",
      "\n",
      "EPOCH 33 of 50\n",
      "\n",
      "Training loss: 0.005269802227202389\n",
      "Validation loss: 0.0034648511403550706\n",
      "\n",
      "EPOCH 34 of 50\n",
      "\n",
      "Training loss: 0.00554538042181068\n",
      "Validation loss: 0.0034592190446952977\n",
      "\n",
      "EPOCH 35 of 50\n",
      "\n",
      "Training loss: 0.0051169113462997805\n",
      "Validation loss: 0.0034443022838483253\n",
      "\n",
      "EPOCH 36 of 50\n",
      "\n",
      "Training loss: 0.005387587265835868\n",
      "Validation loss: 0.0034095407463610172\n",
      "\n",
      "EPOCH 37 of 50\n",
      "\n",
      "Training loss: 0.005016747655140029\n",
      "Validation loss: 0.0034207019489258528\n",
      "\n",
      "EPOCH 38 of 50\n",
      "\n",
      "Training loss: 0.005547414326833354\n",
      "Validation loss: 0.003447528462857008\n",
      "\n",
      "EPOCH 39 of 50\n",
      "\n",
      "Training loss: 0.005206932624181111\n",
      "Validation loss: 0.003449235111474991\n",
      "\n",
      "EPOCH 40 of 50\n",
      "\n",
      "Training loss: 0.005166557617485523\n",
      "Validation loss: 0.003471182038386663\n",
      "\n",
      "EPOCH 41 of 50\n",
      "\n",
      "Training loss: 0.005096763553511765\n",
      "Validation loss: 0.0034612595724562802\n",
      "\n",
      "EPOCH 42 of 50\n",
      "\n",
      "Training loss: 0.005213526957150962\n",
      "Validation loss: 0.00345443623761336\n",
      "\n",
      "EPOCH 43 of 50\n",
      "\n",
      "Training loss: 0.00507539003673527\n",
      "Validation loss: 0.0034412782018383345\n",
      "\n",
      "EPOCH 44 of 50\n",
      "\n",
      "Training loss: 0.005202276632189751\n",
      "Validation loss: 0.003433672711253166\n",
      "\n",
      "EPOCH 45 of 50\n",
      "\n",
      "Training loss: 0.005281242697189252\n",
      "Validation loss: 0.003434708807617426\n",
      "\n",
      "EPOCH 46 of 50\n",
      "\n",
      "Training loss: 0.0050511895161536\n",
      "Validation loss: 0.0034404242566476264\n",
      "\n",
      "EPOCH 47 of 50\n",
      "\n",
      "Training loss: 0.005050999215907521\n",
      "Validation loss: 0.003454663402711352\n",
      "\n",
      "EPOCH 48 of 50\n",
      "\n",
      "Training loss: 0.0053108312293059295\n",
      "Validation loss: 0.0034387524550159774\n",
      "\n",
      "EPOCH 49 of 50\n",
      "\n",
      "Training loss: 0.005403773083041112\n",
      "Validation loss: 0.0034383637830615044\n",
      "\n",
      "EPOCH 50 of 50\n",
      "\n",
      "Training loss: 0.005494005874627166\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-30 06:24:12,812] Trial 41 finished with value: 0.003453387257953485 and parameters: {'lr': 0.000295932904688298, 'weight_decay': 4.808719303736067e-05, 'hidden_size': 449, 'dropout_rate': 0.43144151419422033}. Best is trial 16 with value: 0.0032828766076515117.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.003453387257953485\n",
      "learning rate: 0.0005218397773669649\n",
      "weight_decay: 3.07611755832189e-05\n",
      "hidden_size: 402\n",
      "dropout_rate: 0.4659541453466881\n",
      "\n",
      "EPOCH 1 of 50\n",
      "\n",
      "Training loss: 0.0052273218106064535\n",
      "Validation loss: 0.003327229836334785\n",
      "\n",
      "EPOCH 2 of 50\n",
      "\n",
      "Training loss: 0.005012765475031402\n",
      "Validation loss: 0.0033552125096321106\n",
      "\n",
      "EPOCH 3 of 50\n",
      "\n",
      "Training loss: 0.005089252359337277\n",
      "Validation loss: 0.0033588013611733913\n",
      "\n",
      "EPOCH 4 of 50\n",
      "\n",
      "Training loss: 0.005099833942949772\n",
      "Validation loss: 0.0033590377618869147\n",
      "\n",
      "EPOCH 5 of 50\n",
      "\n",
      "Training loss: 0.0048132525601734715\n",
      "Validation loss: 0.003374038729816675\n",
      "\n",
      "EPOCH 6 of 50\n",
      "\n",
      "Training loss: 0.004975032630479998\n",
      "Validation loss: 0.003368514978016416\n",
      "\n",
      "EPOCH 7 of 50\n",
      "\n",
      "Training loss: 0.005060648209311896\n",
      "Validation loss: 0.003350387793034315\n",
      "\n",
      "EPOCH 8 of 50\n",
      "\n",
      "Training loss: 0.005185559469585617\n",
      "Validation loss: 0.003357159594694773\n",
      "\n",
      "EPOCH 9 of 50\n",
      "\n",
      "Training loss: 0.00500374013144109\n",
      "Validation loss: 0.0033563031659772\n",
      "\n",
      "EPOCH 10 of 50\n",
      "\n",
      "Training loss: 0.005168642667639587\n",
      "Validation loss: 0.0033548882541557155\n",
      "\n",
      "EPOCH 11 of 50\n",
      "\n",
      "Training loss: 0.005048988180028068\n",
      "Validation loss: 0.0033704955130815506\n",
      "\n",
      "EPOCH 12 of 50\n",
      "\n",
      "Training loss: 0.005379371707224184\n",
      "Validation loss: 0.0033628166808436313\n",
      "\n",
      "EPOCH 13 of 50\n",
      "\n",
      "Training loss: 0.005085898278695013\n",
      "Validation loss: 0.0033622041810303926\n",
      "\n",
      "EPOCH 14 of 50\n",
      "\n",
      "Training loss: 0.005281614812297953\n",
      "Validation loss: 0.0033569039466480413\n",
      "\n",
      "EPOCH 15 of 50\n",
      "\n",
      "Training loss: 0.005160265214120348\n",
      "Validation loss: 0.0033749189072599015\n",
      "\n",
      "EPOCH 16 of 50\n",
      "\n",
      "Training loss: 0.005215938487607572\n",
      "Validation loss: 0.0033600248862057924\n",
      "\n",
      "EPOCH 17 of 50\n",
      "\n",
      "Training loss: 0.0050411572576397\n",
      "Validation loss: 0.0033638352372994027\n",
      "\n",
      "EPOCH 18 of 50\n",
      "\n",
      "Training loss: 0.005051007623680764\n",
      "Validation loss: 0.003357722812021772\n",
      "\n",
      "EPOCH 19 of 50\n",
      "\n",
      "Training loss: 0.005185517120278544\n",
      "Validation loss: 0.0033687314329048\n",
      "\n",
      "EPOCH 20 of 50\n",
      "\n",
      "Training loss: 0.005213004614536961\n",
      "Validation loss: 0.0033529425660769143\n",
      "\n",
      "EPOCH 21 of 50\n",
      "\n",
      "Training loss: 0.005344864446669817\n",
      "Validation loss: 0.003351567235464851\n",
      "\n",
      "EPOCH 22 of 50\n",
      "\n",
      "Training loss: 0.00509816687554121\n",
      "Validation loss: 0.0033572797353068986\n",
      "\n",
      "EPOCH 23 of 50\n",
      "\n",
      "Training loss: 0.005425110718028413\n",
      "Validation loss: 0.003368513581032554\n",
      "\n",
      "EPOCH 24 of 50\n",
      "\n",
      "Training loss: 0.005337425455864932\n",
      "Validation loss: 0.0033730793899546065\n",
      "\n",
      "EPOCH 25 of 50\n",
      "\n",
      "Training loss: 0.005140282368908326\n",
      "Validation loss: 0.003370601994295915\n",
      "\n",
      "EPOCH 26 of 50\n",
      "\n",
      "Training loss: 0.005163195387770732\n",
      "Validation loss: 0.003369997488334775\n",
      "\n",
      "EPOCH 27 of 50\n",
      "\n",
      "Training loss: 0.005015880154031847\n",
      "Validation loss: 0.0033647979920109115\n",
      "\n",
      "EPOCH 28 of 50\n",
      "\n",
      "Training loss: 0.004975032061338425\n",
      "Validation loss: 0.0033538509160280228\n",
      "\n",
      "EPOCH 29 of 50\n",
      "\n",
      "Training loss: 0.005199677776545286\n",
      "Validation loss: 0.003354905638843775\n",
      "\n",
      "EPOCH 30 of 50\n",
      "\n",
      "Training loss: 0.005273719473431508\n",
      "Validation loss: 0.0033595628725985685\n",
      "\n",
      "EPOCH 31 of 50\n",
      "\n",
      "Training loss: 0.005047093326639798\n",
      "Validation loss: 0.003374386734018723\n",
      "\n",
      "EPOCH 32 of 50\n",
      "\n",
      "Training loss: 0.005429491659419404\n",
      "Validation loss: 0.0033570548985153437\n",
      "\n",
      "EPOCH 33 of 50\n",
      "\n",
      "Training loss: 0.00550741914452778\n",
      "Validation loss: 0.003352904925122857\n",
      "\n",
      "EPOCH 34 of 50\n",
      "\n",
      "Training loss: 0.005065982980239723\n",
      "Validation loss: 0.003359626435364286\n",
      "\n",
      "EPOCH 35 of 50\n",
      "\n",
      "Training loss: 0.005064191167346305\n",
      "Validation loss: 0.003349202840278546\n",
      "\n",
      "EPOCH 36 of 50\n",
      "\n",
      "Training loss: 0.005070549203082919\n",
      "Validation loss: 0.0033649542989830175\n",
      "\n",
      "EPOCH 37 of 50\n",
      "\n",
      "Training loss: 0.0054421652522352\n",
      "Validation loss: 0.003379155183210969\n",
      "\n",
      "EPOCH 38 of 50\n",
      "\n",
      "Training loss: 0.0051291695175071554\n",
      "Validation loss: 0.0033625843934714794\n",
      "\n",
      "EPOCH 39 of 50\n",
      "\n",
      "Training loss: 0.005386600426087777\n",
      "Validation loss: 0.003362856572493911\n",
      "\n",
      "EPOCH 40 of 50\n",
      "\n",
      "Training loss: 0.005314767722868257\n",
      "Validation loss: 0.0033730691454062858\n",
      "\n",
      "EPOCH 41 of 50\n",
      "\n",
      "Training loss: 0.004859455964631504\n",
      "Validation loss: 0.0033718758107473454\n",
      "\n",
      "EPOCH 42 of 50\n",
      "\n",
      "Training loss: 0.005212863855477836\n",
      "Validation loss: 0.003364169659713904\n",
      "\n",
      "EPOCH 43 of 50\n",
      "\n",
      "Training loss: 0.005105523987569743\n",
      "Validation loss: 0.0033730079885572195\n",
      "\n",
      "EPOCH 44 of 50\n",
      "\n",
      "Training loss: 0.005020206509571936\n",
      "Validation loss: 0.0033715264095614352\n",
      "\n",
      "EPOCH 45 of 50\n",
      "\n",
      "Training loss: 0.005303185743590196\n",
      "Validation loss: 0.0033570032101124525\n",
      "\n",
      "EPOCH 46 of 50\n",
      "\n",
      "Training loss: 0.005115448471365703\n",
      "Validation loss: 0.0033509524073451757\n",
      "\n",
      "EPOCH 47 of 50\n",
      "\n",
      "Training loss: 0.005007249872303671\n",
      "Validation loss: 0.0033547806863983474\n",
      "\n",
      "EPOCH 48 of 50\n",
      "\n",
      "Training loss: 0.005056439329766565\n",
      "Validation loss: 0.003377848071977496\n",
      "\n",
      "EPOCH 49 of 50\n",
      "\n",
      "Training loss: 0.005180738943939407\n",
      "Validation loss: 0.003363947927330931\n",
      "\n",
      "EPOCH 50 of 50\n",
      "\n",
      "Training loss: 0.005402131678743495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-30 06:28:14,052] Trial 42 finished with value: 0.0033642133542646966 and parameters: {'lr': 0.0005218397773669649, 'weight_decay': 3.07611755832189e-05, 'hidden_size': 402, 'dropout_rate': 0.4659541453466881}. Best is trial 16 with value: 0.0032828766076515117.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.0033642133542646966\n",
      "learning rate: 0.0002460938047035121\n",
      "weight_decay: 8.178055899816013e-05\n",
      "hidden_size: 442\n",
      "dropout_rate: 0.4840980529244474\n",
      "\n",
      "EPOCH 1 of 50\n",
      "\n",
      "Training loss: 0.0055767638194892145\n",
      "Validation loss: 0.0032946888046960035\n",
      "\n",
      "EPOCH 2 of 50\n",
      "\n",
      "Training loss: 0.005610093914179338\n",
      "Validation loss: 0.0032926946878433228\n",
      "\n",
      "EPOCH 3 of 50\n",
      "\n",
      "Training loss: 0.0052416278049349785\n",
      "Validation loss: 0.0032945387065410614\n",
      "\n",
      "EPOCH 4 of 50\n",
      "\n",
      "Training loss: 0.005138564156368375\n",
      "Validation loss: 0.0032953099192430577\n",
      "\n",
      "EPOCH 5 of 50\n",
      "\n",
      "Training loss: 0.00522619304764602\n",
      "Validation loss: 0.0032946482921640077\n",
      "\n",
      "EPOCH 6 of 50\n",
      "\n",
      "Training loss: 0.005323430057615042\n",
      "Validation loss: 0.003297802604114016\n",
      "\n",
      "EPOCH 7 of 50\n",
      "\n",
      "Training loss: 0.005416012912367781\n",
      "Validation loss: 0.003295200333620111\n",
      "\n",
      "EPOCH 8 of 50\n",
      "\n",
      "Training loss: 0.005455156788229942\n",
      "Validation loss: 0.0032950245464841523\n",
      "\n",
      "EPOCH 9 of 50\n",
      "\n",
      "Training loss: 0.005153860793345504\n",
      "Validation loss: 0.0033009216034164033\n",
      "\n",
      "EPOCH 10 of 50\n",
      "\n",
      "Training loss: 0.005232093571167853\n",
      "Validation loss: 0.003303004273523887\n",
      "\n",
      "EPOCH 11 of 50\n",
      "\n",
      "Training loss: 0.005220008367258642\n",
      "Validation loss: 0.003305323033904036\n",
      "\n",
      "EPOCH 12 of 50\n",
      "\n",
      "Training loss: 0.0054539414640102125\n",
      "Validation loss: 0.0032987042795866728\n",
      "\n",
      "EPOCH 13 of 50\n",
      "\n",
      "Training loss: 0.00516927894204855\n",
      "Validation loss: 0.0032981972520550094\n",
      "\n",
      "EPOCH 14 of 50\n",
      "\n",
      "Training loss: 0.005363987241354253\n",
      "Validation loss: 0.003293151268735528\n",
      "\n",
      "EPOCH 15 of 50\n",
      "\n",
      "Training loss: 0.005325837397120065\n",
      "Validation loss: 0.0032947811608513198\n",
      "\n",
      "EPOCH 16 of 50\n",
      "\n",
      "Training loss: 0.005628327632115947\n",
      "Validation loss: 0.0033025704324245453\n",
      "\n",
      "EPOCH 17 of 50\n",
      "\n",
      "Training loss: 0.005309301376756694\n",
      "Validation loss: 0.0032956701858590045\n",
      "\n",
      "EPOCH 18 of 50\n",
      "\n",
      "Training loss: 0.00517788203433156\n",
      "Validation loss: 0.0032963726359109082\n",
      "\n",
      "EPOCH 19 of 50\n",
      "\n",
      "Training loss: 0.005327036656025384\n",
      "Validation loss: 0.003299492411315441\n",
      "\n",
      "EPOCH 20 of 50\n",
      "\n",
      "Training loss: 0.005357090745949083\n",
      "Validation loss: 0.003295990095163385\n",
      "\n",
      "EPOCH 21 of 50\n",
      "\n",
      "Training loss: 0.005745194024509854\n",
      "Validation loss: 0.0032972038413087526\n",
      "\n",
      "EPOCH 22 of 50\n",
      "\n",
      "Training loss: 0.005322885751310322\n",
      "Validation loss: 0.003299825514356295\n",
      "\n",
      "EPOCH 23 of 50\n",
      "\n",
      "Training loss: 0.005049751450618108\n",
      "Validation loss: 0.0032953030119339624\n",
      "\n",
      "EPOCH 24 of 50\n",
      "\n",
      "Training loss: 0.005073760662020909\n",
      "Validation loss: 0.0032959835759053626\n",
      "\n",
      "EPOCH 25 of 50\n",
      "\n",
      "Training loss: 0.005495467740628455\n",
      "Validation loss: 0.0033015940183152757\n",
      "\n",
      "EPOCH 26 of 50\n",
      "\n",
      "Training loss: 0.005385979803072082\n",
      "Validation loss: 0.0033030204164485135\n",
      "\n",
      "EPOCH 27 of 50\n",
      "\n",
      "Training loss: 0.005206806119531393\n",
      "Validation loss: 0.0032981238327920437\n",
      "\n",
      "EPOCH 28 of 50\n",
      "\n",
      "Training loss: 0.005416283021784491\n",
      "Validation loss: 0.003292252697671453\n",
      "\n",
      "EPOCH 29 of 50\n",
      "\n",
      "Training loss: 0.0051471879705786705\n",
      "Validation loss: 0.00329548337807258\n",
      "\n",
      "EPOCH 30 of 50\n",
      "\n",
      "Training loss: 0.005426781562467416\n",
      "Validation loss: 0.003296035729969541\n",
      "\n",
      "EPOCH 31 of 50\n",
      "\n",
      "Training loss: 0.005238933876777689\n",
      "Validation loss: 0.0032962027471512556\n",
      "\n",
      "EPOCH 32 of 50\n",
      "\n",
      "Training loss: 0.005277552693668339\n",
      "Validation loss: 0.0033032373369981847\n",
      "\n",
      "EPOCH 33 of 50\n",
      "\n",
      "Training loss: 0.005401294419748915\n",
      "Validation loss: 0.003302629959459106\n",
      "\n",
      "EPOCH 34 of 50\n",
      "\n",
      "Training loss: 0.005354937528156572\n",
      "Validation loss: 0.0033023434225469828\n",
      "\n",
      "EPOCH 35 of 50\n",
      "\n",
      "Training loss: 0.0052843783309476245\n",
      "Validation loss: 0.0032981548768778643\n",
      "\n",
      "EPOCH 36 of 50\n",
      "\n",
      "Training loss: 0.0054451571260061525\n",
      "Validation loss: 0.003293684838960568\n",
      "\n",
      "EPOCH 37 of 50\n",
      "\n",
      "Training loss: 0.005181014899992281\n",
      "Validation loss: 0.003299862379208207\n",
      "\n",
      "EPOCH 38 of 50\n",
      "\n",
      "Training loss: 0.005575082782242034\n",
      "Validation loss: 0.0032958966524650655\n",
      "\n",
      "EPOCH 39 of 50\n",
      "\n",
      "Training loss: 0.005301765580144193\n",
      "Validation loss: 0.0032994349021464586\n",
      "\n",
      "EPOCH 40 of 50\n",
      "\n",
      "Training loss: 0.005260532307955954\n",
      "Validation loss: 0.003295669797807932\n",
      "\n",
      "EPOCH 41 of 50\n",
      "\n",
      "Training loss: 0.0051844303703142535\n",
      "Validation loss: 0.003297437603274981\n",
      "\n",
      "EPOCH 42 of 50\n",
      "\n",
      "Training loss: 0.00501981488842931\n",
      "Validation loss: 0.0032947214785963297\n",
      "\n",
      "EPOCH 43 of 50\n",
      "\n",
      "Training loss: 0.00512741063721478\n",
      "Validation loss: 0.003293863652894894\n",
      "\n",
      "EPOCH 44 of 50\n",
      "\n",
      "Training loss: 0.00509910373431113\n",
      "Validation loss: 0.00329560445000728\n",
      "\n",
      "EPOCH 45 of 50\n",
      "\n",
      "Training loss: 0.005250167101621628\n",
      "Validation loss: 0.003297786694020033\n",
      "\n",
      "EPOCH 46 of 50\n",
      "\n",
      "Training loss: 0.00499917419316868\n",
      "Validation loss: 0.0033004386350512505\n",
      "\n",
      "EPOCH 47 of 50\n",
      "\n",
      "Training loss: 0.00525707896384928\n",
      "Validation loss: 0.0032984267454594374\n",
      "\n",
      "EPOCH 48 of 50\n",
      "\n",
      "Training loss: 0.00541903047511975\n",
      "Validation loss: 0.003297474312906464\n",
      "\n",
      "EPOCH 49 of 50\n",
      "\n",
      "Training loss: 0.005454662669863965\n",
      "Validation loss: 0.0032961430648962655\n",
      "\n",
      "EPOCH 50 of 50\n",
      "\n",
      "Training loss: 0.005156376580190327\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-30 06:32:11,167] Trial 43 finished with value: 0.003295280427361528 and parameters: {'lr': 0.0002460938047035121, 'weight_decay': 8.178055899816013e-05, 'hidden_size': 442, 'dropout_rate': 0.4840980529244474}. Best is trial 16 with value: 0.0032828766076515117.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.003295280427361528\n",
      "learning rate: 0.0002297900719187275\n",
      "weight_decay: 9.585739172784814e-05\n",
      "hidden_size: 483\n",
      "dropout_rate: 0.471276236551651\n",
      "\n",
      "EPOCH 1 of 50\n",
      "\n",
      "Training loss: 0.00524465514657398\n",
      "Validation loss: 0.003312059212476015\n",
      "\n",
      "EPOCH 2 of 50\n",
      "\n",
      "Training loss: 0.005228758505028155\n",
      "Validation loss: 0.0033237688864270845\n",
      "\n",
      "EPOCH 3 of 50\n",
      "\n",
      "Training loss: 0.005194169624398152\n",
      "Validation loss: 0.0033465190014491477\n",
      "\n",
      "EPOCH 4 of 50\n",
      "\n",
      "Training loss: 0.005561969346470303\n",
      "Validation loss: 0.0033499596950908503\n",
      "\n",
      "EPOCH 5 of 50\n",
      "\n",
      "Training loss: 0.005323001907931434\n",
      "Validation loss: 0.003361542864392201\n",
      "\n",
      "EPOCH 6 of 50\n",
      "\n",
      "Training loss: 0.005321716734518607\n",
      "Validation loss: 0.0033540550308922925\n",
      "\n",
      "EPOCH 7 of 50\n",
      "\n",
      "Training loss: 0.004940312407496903\n",
      "Validation loss: 0.003358449243629972\n",
      "\n",
      "EPOCH 8 of 50\n",
      "\n",
      "Training loss: 0.005540448913557662\n",
      "Validation loss: 0.0033627839293330908\n",
      "\n",
      "EPOCH 9 of 50\n",
      "\n",
      "Training loss: 0.00525901083730989\n",
      "Validation loss: 0.003353963683669766\n",
      "\n",
      "EPOCH 10 of 50\n",
      "\n",
      "Training loss: 0.005262694373312924\n",
      "Validation loss: 0.00336351222358644\n",
      "\n",
      "EPOCH 11 of 50\n",
      "\n",
      "Training loss: 0.005525678396224976\n",
      "Validation loss: 0.0033589897211641073\n",
      "\n",
      "EPOCH 12 of 50\n",
      "\n",
      "Training loss: 0.0055161443952884935\n",
      "Validation loss: 0.003358622780069709\n",
      "\n",
      "EPOCH 13 of 50\n",
      "\n",
      "Training loss: 0.005261092136303584\n",
      "Validation loss: 0.003351835378756126\n",
      "\n",
      "EPOCH 14 of 50\n",
      "\n",
      "Training loss: 0.0053239550648464095\n",
      "Validation loss: 0.0033607104948411384\n",
      "\n",
      "EPOCH 15 of 50\n",
      "\n",
      "Training loss: 0.005235524149611592\n",
      "Validation loss: 0.003370609444876512\n",
      "\n",
      "EPOCH 16 of 50\n",
      "\n",
      "Training loss: 0.005321566998544667\n",
      "Validation loss: 0.003357754942650596\n",
      "\n",
      "EPOCH 17 of 50\n",
      "\n",
      "Training loss: 0.0057078433326549\n",
      "Validation loss: 0.0033605107261488834\n",
      "\n",
      "EPOCH 18 of 50\n",
      "\n",
      "Training loss: 0.005228249149190055\n",
      "Validation loss: 0.0033559277653694153\n",
      "\n",
      "EPOCH 19 of 50\n",
      "\n",
      "Training loss: 0.005542026522258918\n",
      "Validation loss: 0.0033653308637440205\n",
      "\n",
      "EPOCH 20 of 50\n",
      "\n",
      "Training loss: 0.005014039524313476\n",
      "Validation loss: 0.003363001005103191\n",
      "\n",
      "EPOCH 21 of 50\n",
      "\n",
      "Training loss: 0.005521386292659574\n",
      "Validation loss: 0.0033710780553519726\n",
      "\n",
      "EPOCH 22 of 50\n",
      "\n",
      "Training loss: 0.005701649959923493\n",
      "Validation loss: 0.003354471797744433\n",
      "\n",
      "EPOCH 23 of 50\n",
      "\n",
      "Training loss: 0.005285669635567401\n",
      "Validation loss: 0.0033621805099149546\n",
      "\n",
      "EPOCH 24 of 50\n",
      "\n",
      "Training loss: 0.00537132805523773\n",
      "Validation loss: 0.0033564775561292968\n",
      "\n",
      "EPOCH 25 of 50\n",
      "\n",
      "Training loss: 0.005464687167356412\n",
      "Validation loss: 0.0033616633154451847\n",
      "\n",
      "EPOCH 26 of 50\n",
      "\n",
      "Training loss: 0.00559922920850416\n",
      "Validation loss: 0.0033532206434756517\n",
      "\n",
      "EPOCH 27 of 50\n",
      "\n",
      "Training loss: 0.005249787742892901\n",
      "Validation loss: 0.003353825925538937\n",
      "\n",
      "EPOCH 28 of 50\n",
      "\n",
      "Training loss: 0.005234807962551713\n",
      "Validation loss: 0.003342168560872475\n",
      "\n",
      "EPOCH 29 of 50\n",
      "\n",
      "Training loss: 0.005492264974034495\n",
      "Validation loss: 0.0033590694268544516\n",
      "\n",
      "EPOCH 30 of 50\n",
      "\n",
      "Training loss: 0.00522463515193926\n",
      "Validation loss: 0.0033524358489861092\n",
      "\n",
      "EPOCH 31 of 50\n",
      "\n",
      "Training loss: 0.005269751444252001\n",
      "Validation loss: 0.00336508018275102\n",
      "\n",
      "EPOCH 32 of 50\n",
      "\n",
      "Training loss: 0.005248655720303456\n",
      "Validation loss: 0.0033545812281469503\n",
      "\n",
      "EPOCH 33 of 50\n",
      "\n",
      "Training loss: 0.005207829668910967\n",
      "Validation loss: 0.0033534230509152016\n",
      "\n",
      "EPOCH 34 of 50\n",
      "\n",
      "Training loss: 0.004917552591198021\n",
      "Validation loss: 0.0033475044183433056\n",
      "\n",
      "EPOCH 35 of 50\n",
      "\n",
      "Training loss: 0.005088473049302896\n",
      "Validation loss: 0.0033666830665121474\n",
      "\n",
      "EPOCH 36 of 50\n",
      "\n",
      "Training loss: 0.005221349083715015\n",
      "Validation loss: 0.0033685395028442144\n",
      "\n",
      "EPOCH 37 of 50\n",
      "\n",
      "Training loss: 0.0056462873083849745\n",
      "Validation loss: 0.0033689777677257857\n",
      "\n",
      "EPOCH 38 of 50\n",
      "\n",
      "Training loss: 0.005169945044649972\n",
      "Validation loss: 0.0033763254371782145\n",
      "\n",
      "EPOCH 39 of 50\n",
      "\n",
      "Training loss: 0.0052767303875750965\n",
      "Validation loss: 0.0033719494628409543\n",
      "\n",
      "EPOCH 40 of 50\n",
      "\n",
      "Training loss: 0.005226393877011206\n",
      "Validation loss: 0.0033614070465167365\n",
      "\n",
      "EPOCH 41 of 50\n",
      "\n",
      "Training loss: 0.0051803624050484765\n",
      "Validation loss: 0.0033507068486263356\n",
      "\n",
      "EPOCH 42 of 50\n",
      "\n",
      "Training loss: 0.005329276745518048\n",
      "Validation loss: 0.003357368210951487\n",
      "\n",
      "EPOCH 43 of 50\n",
      "\n",
      "Training loss: 0.005476587865915563\n",
      "Validation loss: 0.0033615753830720982\n",
      "\n",
      "EPOCH 44 of 50\n",
      "\n",
      "Training loss: 0.005321187070674366\n",
      "Validation loss: 0.003378118465964993\n",
      "\n",
      "EPOCH 45 of 50\n",
      "\n",
      "Training loss: 0.005409048559765021\n",
      "Validation loss: 0.0033612707629799843\n",
      "\n",
      "EPOCH 46 of 50\n",
      "\n",
      "Training loss: 0.0053030468213061495\n",
      "Validation loss: 0.00335283693857491\n",
      "\n",
      "EPOCH 47 of 50\n",
      "\n",
      "Training loss: 0.005419702476097478\n",
      "Validation loss: 0.0033647171997775636\n",
      "\n",
      "EPOCH 48 of 50\n",
      "\n",
      "Training loss: 0.00549254080073701\n",
      "Validation loss: 0.0033548354792098203\n",
      "\n",
      "EPOCH 49 of 50\n",
      "\n",
      "Training loss: 0.005026758493234714\n",
      "Validation loss: 0.0033676488480220237\n",
      "\n",
      "EPOCH 50 of 50\n",
      "\n",
      "Training loss: 0.005109783468974961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-30 06:36:16,094] Trial 44 finished with value: 0.00335627025924623 and parameters: {'lr': 0.0002297900719187275, 'weight_decay': 9.585739172784814e-05, 'hidden_size': 483, 'dropout_rate': 0.471276236551651}. Best is trial 16 with value: 0.0032828766076515117.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.00335627025924623\n",
      "learning rate: 0.001674041893157779\n",
      "weight_decay: 7.674626468244613e-05\n",
      "hidden_size: 468\n",
      "dropout_rate: 0.4881828261385519\n",
      "\n",
      "EPOCH 1 of 50\n",
      "\n",
      "Training loss: 0.005318682175129652\n",
      "Validation loss: 0.00334637681953609\n",
      "\n",
      "EPOCH 2 of 50\n",
      "\n",
      "Training loss: 0.005101261970897515\n",
      "Validation loss: 0.0034131803549826145\n",
      "\n",
      "EPOCH 3 of 50\n",
      "\n",
      "Training loss: 0.004633218903715412\n",
      "Validation loss: 0.003446777273590366\n",
      "\n",
      "EPOCH 4 of 50\n",
      "\n",
      "Training loss: 0.004783136485558417\n",
      "Validation loss: 0.003460624177629749\n",
      "\n",
      "EPOCH 5 of 50\n",
      "\n",
      "Training loss: 0.0049576965781549616\n",
      "Validation loss: 0.003475736128166318\n",
      "\n",
      "EPOCH 6 of 50\n",
      "\n",
      "Training loss: 0.004858691633368532\n",
      "Validation loss: 0.003470050791899363\n",
      "\n",
      "EPOCH 7 of 50\n",
      "\n",
      "Training loss: 0.004862654178092877\n",
      "Validation loss: 0.003471375753482183\n",
      "\n",
      "EPOCH 8 of 50\n",
      "\n",
      "Training loss: 0.0048485583780954284\n",
      "Validation loss: 0.0035000150091946125\n",
      "\n",
      "EPOCH 9 of 50\n",
      "\n",
      "Training loss: 0.00479971943423152\n",
      "Validation loss: 0.0035056087654083967\n",
      "\n",
      "EPOCH 10 of 50\n",
      "\n",
      "Training loss: 0.004900791837523381\n",
      "Validation loss: 0.0034737443396200738\n",
      "\n",
      "EPOCH 11 of 50\n",
      "\n",
      "Training loss: 0.004950524204307132\n",
      "Validation loss: 0.003471972964083155\n",
      "\n",
      "EPOCH 12 of 50\n",
      "\n",
      "Training loss: 0.00487183471624222\n",
      "Validation loss: 0.003464329056441784\n",
      "\n",
      "EPOCH 13 of 50\n",
      "\n",
      "Training loss: 0.004878717578119702\n",
      "Validation loss: 0.003483155199016134\n",
      "\n",
      "EPOCH 14 of 50\n",
      "\n",
      "Training loss: 0.00476243975572288\n",
      "Validation loss: 0.0034550579730421305\n",
      "\n",
      "EPOCH 15 of 50\n",
      "\n",
      "Training loss: 0.004850410778696339\n",
      "Validation loss: 0.0034689335928608975\n",
      "\n",
      "EPOCH 16 of 50\n",
      "\n",
      "Training loss: 0.004984898596174187\n",
      "Validation loss: 0.003473966848105192\n",
      "\n",
      "EPOCH 17 of 50\n",
      "\n",
      "Training loss: 0.004960651354243358\n",
      "Validation loss: 0.003448545311888059\n",
      "\n",
      "EPOCH 18 of 50\n",
      "\n",
      "Training loss: 0.005027136325629221\n",
      "Validation loss: 0.0034670685417950153\n",
      "\n",
      "EPOCH 19 of 50\n",
      "\n",
      "Training loss: 0.00482366810966697\n",
      "Validation loss: 0.0034808777272701263\n",
      "\n",
      "EPOCH 20 of 50\n",
      "\n",
      "Training loss: 0.004750256711203191\n",
      "Validation loss: 0.0034762497525662184\n",
      "\n",
      "EPOCH 21 of 50\n",
      "\n",
      "Training loss: 0.004922197355578343\n",
      "Validation loss: 0.0034895415883511305\n",
      "\n",
      "EPOCH 22 of 50\n",
      "\n",
      "Training loss: 0.00492143915552232\n",
      "Validation loss: 0.0034748640221854052\n",
      "\n",
      "EPOCH 23 of 50\n",
      "\n",
      "Training loss: 0.005155985191878345\n",
      "Validation loss: 0.003477906653036674\n",
      "\n",
      "EPOCH 24 of 50\n",
      "\n",
      "Training loss: 0.004924509570830398\n",
      "Validation loss: 0.0034599896365155778\n",
      "\n",
      "EPOCH 25 of 50\n",
      "\n",
      "Training loss: 0.004925363878202107\n",
      "Validation loss: 0.003464433985451857\n",
      "\n",
      "EPOCH 26 of 50\n",
      "\n",
      "Training loss: 0.004893901706155803\n",
      "Validation loss: 0.0034695493523031473\n",
      "\n",
      "EPOCH 27 of 50\n",
      "\n",
      "Training loss: 0.005046511456991236\n",
      "Validation loss: 0.0034575699052462974\n",
      "\n",
      "EPOCH 28 of 50\n",
      "\n",
      "Training loss: 0.004782076795688934\n",
      "Validation loss: 0.0034767905405412116\n",
      "\n",
      "EPOCH 29 of 50\n",
      "\n",
      "Training loss: 0.004721402811507384\n",
      "Validation loss: 0.003471461357548833\n",
      "\n",
      "EPOCH 30 of 50\n",
      "\n",
      "Training loss: 0.004650470335036516\n",
      "Validation loss: 0.003475181059911847\n",
      "\n",
      "EPOCH 31 of 50\n",
      "\n",
      "Training loss: 0.004770539675114883\n",
      "Validation loss: 0.0034645093449701867\n",
      "\n",
      "EPOCH 32 of 50\n",
      "\n",
      "Training loss: 0.004868868583192428\n",
      "Validation loss: 0.0034685341330866017\n",
      "\n",
      "EPOCH 33 of 50\n",
      "\n",
      "Training loss: 0.0047710311806036364\n",
      "Validation loss: 0.003492093567426006\n",
      "\n",
      "EPOCH 34 of 50\n",
      "\n",
      "Training loss: 0.005051089812897974\n",
      "Validation loss: 0.0034893788397312164\n",
      "\n",
      "EPOCH 35 of 50\n",
      "\n",
      "Training loss: 0.004875937062833045\n",
      "Validation loss: 0.003488884074613452\n",
      "\n",
      "EPOCH 36 of 50\n",
      "\n",
      "Training loss: 0.005300519082488285\n",
      "Validation loss: 0.0034711832801500955\n",
      "\n",
      "EPOCH 37 of 50\n",
      "\n",
      "Training loss: 0.004732608148414228\n",
      "Validation loss: 0.003472145413979888\n",
      "\n",
      "EPOCH 38 of 50\n",
      "\n",
      "Training loss: 0.0047797823790460825\n",
      "Validation loss: 0.003480668490131696\n",
      "\n",
      "EPOCH 39 of 50\n",
      "\n",
      "Training loss: 0.004925648940520154\n",
      "Validation loss: 0.0034658548732598624\n",
      "\n",
      "EPOCH 40 of 50\n",
      "\n",
      "Training loss: 0.00481323752966192\n",
      "Validation loss: 0.003471957053989172\n",
      "\n",
      "EPOCH 41 of 50\n",
      "\n",
      "Training loss: 0.0049002207815647125\n",
      "Validation loss: 0.003460537719850739\n",
      "\n",
      "EPOCH 42 of 50\n",
      "\n",
      "Training loss: 0.00497688011576732\n",
      "Validation loss: 0.0034778468931714692\n",
      "\n",
      "EPOCH 43 of 50\n",
      "\n",
      "Training loss: 0.00493886342479123\n",
      "Validation loss: 0.0034604090421150127\n",
      "\n",
      "EPOCH 44 of 50\n",
      "\n",
      "Training loss: 0.0049733818094763495\n",
      "Validation loss: 0.0034820931032299995\n",
      "\n",
      "EPOCH 45 of 50\n",
      "\n",
      "Training loss: 0.0049469444072908824\n",
      "Validation loss: 0.0034817439348747334\n",
      "\n",
      "EPOCH 46 of 50\n",
      "\n",
      "Training loss: 0.004713479869274629\n",
      "Validation loss: 0.0034887909423559904\n",
      "\n",
      "EPOCH 47 of 50\n",
      "\n",
      "Training loss: 0.0051044003210133975\n",
      "Validation loss: 0.0034816289165367684\n",
      "\n",
      "EPOCH 48 of 50\n",
      "\n",
      "Training loss: 0.00497633443834881\n",
      "Validation loss: 0.0034794077121963105\n",
      "\n",
      "EPOCH 49 of 50\n",
      "\n",
      "Training loss: 0.004807662632730272\n",
      "Validation loss: 0.0034778643554697433\n",
      "\n",
      "EPOCH 50 of 50\n",
      "\n",
      "Training loss: 0.004997981117210454\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-30 06:40:15,873] Trial 45 finished with value: 0.0034811293395857015 and parameters: {'lr': 0.001674041893157779, 'weight_decay': 7.674626468244613e-05, 'hidden_size': 468, 'dropout_rate': 0.4881828261385519}. Best is trial 16 with value: 0.0032828766076515117.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.0034811293395857015\n",
      "learning rate: 6.476414274811693e-05\n",
      "weight_decay: 0.00018045146276443922\n",
      "hidden_size: 446\n",
      "dropout_rate: 0.4788988986251944\n",
      "\n",
      "EPOCH 1 of 50\n",
      "\n",
      "Training loss: 0.005145652504223917\n",
      "Validation loss: 0.003310538517932097\n",
      "\n",
      "EPOCH 2 of 50\n",
      "\n",
      "Training loss: 0.005279780003345675\n",
      "Validation loss: 0.0033546486714233956\n",
      "\n",
      "EPOCH 3 of 50\n",
      "\n",
      "Training loss: 0.0054323918496568995\n",
      "Validation loss: 0.003374798921868205\n",
      "\n",
      "EPOCH 4 of 50\n",
      "\n",
      "Training loss: 0.005287743302889996\n",
      "Validation loss: 0.003389415874456366\n",
      "\n",
      "EPOCH 5 of 50\n",
      "\n",
      "Training loss: 0.0051775652294357615\n",
      "Validation loss: 0.003394867138316234\n",
      "\n",
      "EPOCH 6 of 50\n",
      "\n",
      "Training loss: 0.0052112061012950205\n",
      "Validation loss: 0.0033987502877910933\n",
      "\n",
      "EPOCH 7 of 50\n",
      "\n",
      "Training loss: 0.005382785625341866\n",
      "Validation loss: 0.0033767425920814276\n",
      "\n",
      "EPOCH 8 of 50\n",
      "\n",
      "Training loss: 0.005286377026802964\n",
      "Validation loss: 0.0033969655632972717\n",
      "\n",
      "EPOCH 9 of 50\n",
      "\n",
      "Training loss: 0.005068430211395025\n",
      "Validation loss: 0.0034033228488018117\n",
      "\n",
      "EPOCH 10 of 50\n",
      "\n",
      "Training loss: 0.0053418099673257936\n",
      "Validation loss: 0.0033734216510007777\n",
      "\n",
      "EPOCH 11 of 50\n",
      "\n",
      "Training loss: 0.005351074557337496\n",
      "Validation loss: 0.0033867379340032735\n",
      "\n",
      "EPOCH 12 of 50\n",
      "\n",
      "Training loss: 0.0054911925043496825\n",
      "Validation loss: 0.0033803138261040053\n",
      "\n",
      "EPOCH 13 of 50\n",
      "\n",
      "Training loss: 0.00521931704133749\n",
      "Validation loss: 0.00339006291081508\n",
      "\n",
      "EPOCH 14 of 50\n",
      "\n",
      "Training loss: 0.005343850184645917\n",
      "Validation loss: 0.0033828602948536477\n",
      "\n",
      "EPOCH 15 of 50\n",
      "\n",
      "Training loss: 0.005512152074111832\n",
      "Validation loss: 0.00337225377249221\n",
      "\n",
      "EPOCH 16 of 50\n",
      "\n",
      "Training loss: 0.005321789895080858\n",
      "Validation loss: 0.003375548946981629\n",
      "\n",
      "EPOCH 17 of 50\n",
      "\n",
      "Training loss: 0.005269027108119594\n",
      "Validation loss: 0.0033914307908465466\n",
      "\n",
      "EPOCH 18 of 50\n",
      "\n",
      "Training loss: 0.0052722719135797685\n",
      "Validation loss: 0.0034068420063704252\n",
      "\n",
      "EPOCH 19 of 50\n",
      "\n",
      "Training loss: 0.005369747781919109\n",
      "Validation loss: 0.003387184658398231\n",
      "\n",
      "EPOCH 20 of 50\n",
      "\n",
      "Training loss: 0.004987150482419465\n",
      "Validation loss: 0.0033968323841691017\n",
      "\n",
      "EPOCH 21 of 50\n",
      "\n",
      "Training loss: 0.005375905428081751\n",
      "Validation loss: 0.003407465604444345\n",
      "\n",
      "EPOCH 22 of 50\n",
      "\n",
      "Training loss: 0.005124859537722336\n",
      "Validation loss: 0.003399117927377423\n",
      "\n",
      "EPOCH 23 of 50\n",
      "\n",
      "Training loss: 0.0054100674525317215\n",
      "Validation loss: 0.003374484290058414\n",
      "\n",
      "EPOCH 24 of 50\n",
      "\n",
      "Training loss: 0.00545489345677197\n",
      "Validation loss: 0.003378862515091896\n",
      "\n",
      "EPOCH 25 of 50\n",
      "\n",
      "Training loss: 0.005261293819381131\n",
      "Validation loss: 0.003389506135135889\n",
      "\n",
      "EPOCH 26 of 50\n",
      "\n",
      "Training loss: 0.005179141648113728\n",
      "Validation loss: 0.003407937784989675\n",
      "\n",
      "EPOCH 27 of 50\n",
      "\n",
      "Training loss: 0.005678494176309969\n",
      "Validation loss: 0.0034024950582534075\n",
      "\n",
      "EPOCH 28 of 50\n",
      "\n",
      "Training loss: 0.005233410952819718\n",
      "Validation loss: 0.0033984935532013574\n",
      "\n",
      "EPOCH 29 of 50\n",
      "\n",
      "Training loss: 0.0055697741400864385\n",
      "Validation loss: 0.003379691314573089\n",
      "\n",
      "EPOCH 30 of 50\n",
      "\n",
      "Training loss: 0.005493898203389512\n",
      "Validation loss: 0.0034051448261986175\n",
      "\n",
      "EPOCH 31 of 50\n",
      "\n",
      "Training loss: 0.005438603305568297\n",
      "Validation loss: 0.0033896594929198423\n",
      "\n",
      "EPOCH 32 of 50\n",
      "\n",
      "Training loss: 0.0053100744262337685\n",
      "Validation loss: 0.0033991640278448663\n",
      "\n",
      "EPOCH 33 of 50\n",
      "\n",
      "Training loss: 0.0052385906585388714\n",
      "Validation loss: 0.003395629193012913\n",
      "\n",
      "EPOCH 34 of 50\n",
      "\n",
      "Training loss: 0.005108377171887292\n",
      "Validation loss: 0.0034094388441493115\n",
      "\n",
      "EPOCH 35 of 50\n",
      "\n",
      "Training loss: 0.00526289910905891\n",
      "Validation loss: 0.003415848594158888\n",
      "\n",
      "EPOCH 36 of 50\n",
      "\n",
      "Training loss: 0.005171986995264888\n",
      "Validation loss: 0.00339502344528834\n",
      "\n",
      "EPOCH 37 of 50\n",
      "\n",
      "Training loss: 0.005595477660083109\n",
      "Validation loss: 0.00337077584117651\n",
      "\n",
      "EPOCH 38 of 50\n",
      "\n",
      "Training loss: 0.005267747626122501\n",
      "Validation loss: 0.003422463002304236\n",
      "\n",
      "EPOCH 39 of 50\n",
      "\n",
      "Training loss: 0.00558366859331727\n",
      "Validation loss: 0.0034102676436305046\n",
      "\n",
      "EPOCH 40 of 50\n",
      "\n",
      "Training loss: 0.005377688859071996\n",
      "Validation loss: 0.0034112843374411264\n",
      "\n",
      "EPOCH 41 of 50\n",
      "\n",
      "Training loss: 0.005363288283762004\n",
      "Validation loss: 0.0034014970685044923\n",
      "\n",
      "EPOCH 42 of 50\n",
      "\n",
      "Training loss: 0.00519396990744604\n",
      "Validation loss: 0.0034210256611307464\n",
      "\n",
      "EPOCH 43 of 50\n",
      "\n",
      "Training loss: 0.005273788029121028\n",
      "Validation loss: 0.003409526078030467\n",
      "\n",
      "EPOCH 44 of 50\n",
      "\n",
      "Training loss: 0.00528986247566839\n",
      "Validation loss: 0.003386672275761763\n",
      "\n",
      "EPOCH 45 of 50\n",
      "\n",
      "Training loss: 0.005377749317429132\n",
      "Validation loss: 0.0033795415268590054\n",
      "\n",
      "EPOCH 46 of 50\n",
      "\n",
      "Training loss: 0.005226997813830773\n",
      "Validation loss: 0.003398927549521128\n",
      "\n",
      "EPOCH 47 of 50\n",
      "\n",
      "Training loss: 0.005398032307210896\n",
      "Validation loss: 0.0034015707205981016\n",
      "\n",
      "EPOCH 48 of 50\n",
      "\n",
      "Training loss: 0.004988967829073469\n",
      "Validation loss: 0.0033903942288209996\n",
      "\n",
      "EPOCH 49 of 50\n",
      "\n",
      "Training loss: 0.005196100204355187\n",
      "Validation loss: 0.0033973549337436757\n",
      "\n",
      "EPOCH 50 of 50\n",
      "\n",
      "Training loss: 0.005302384081814025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-30 06:44:11,769] Trial 46 finished with value: 0.003417458152398467 and parameters: {'lr': 6.476414274811693e-05, 'weight_decay': 0.00018045146276443922, 'hidden_size': 446, 'dropout_rate': 0.4788988986251944}. Best is trial 16 with value: 0.0032828766076515117.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.003417458152398467\n",
      "learning rate: 2.465164429248147e-05\n",
      "weight_decay: 0.00012324787546192558\n",
      "hidden_size: 434\n",
      "dropout_rate: 0.4996852788535499\n",
      "\n",
      "EPOCH 1 of 50\n",
      "\n",
      "Training loss: 0.005546177737414837\n",
      "Validation loss: 0.0033013885064671435\n",
      "\n",
      "EPOCH 2 of 50\n",
      "\n",
      "Training loss: 0.005791998675300015\n",
      "Validation loss: 0.0033222377145042024\n",
      "\n",
      "EPOCH 3 of 50\n",
      "\n",
      "Training loss: 0.005457403500460916\n",
      "Validation loss: 0.0033370406987766423\n",
      "\n",
      "EPOCH 4 of 50\n",
      "\n",
      "Training loss: 0.005325489108347231\n",
      "Validation loss: 0.0033451936518152556\n",
      "\n",
      "EPOCH 5 of 50\n",
      "\n",
      "Training loss: 0.005689662415534258\n",
      "Validation loss: 0.0033567374727378287\n",
      "\n",
      "EPOCH 6 of 50\n",
      "\n",
      "Training loss: 0.005539062484684918\n",
      "Validation loss: 0.003358597711970409\n",
      "\n",
      "EPOCH 7 of 50\n",
      "\n",
      "Training loss: 0.00559409252471394\n",
      "Validation loss: 0.0033641325620313487\n",
      "\n",
      "EPOCH 8 of 50\n",
      "\n",
      "Training loss: 0.00550568885066443\n",
      "Validation loss: 0.003372349931548039\n",
      "\n",
      "EPOCH 9 of 50\n",
      "\n",
      "Training loss: 0.005456038285046816\n",
      "Validation loss: 0.0033931839279830456\n",
      "\n",
      "EPOCH 10 of 50\n",
      "\n",
      "Training loss: 0.005821680236193869\n",
      "Validation loss: 0.003393537054459254\n",
      "\n",
      "EPOCH 11 of 50\n",
      "\n",
      "Training loss: 0.00554501141111056\n",
      "Validation loss: 0.003386148950085044\n",
      "\n",
      "EPOCH 12 of 50\n",
      "\n",
      "Training loss: 0.00546314663046764\n",
      "Validation loss: 0.003377472050487995\n",
      "\n",
      "EPOCH 13 of 50\n",
      "\n",
      "Training loss: 0.00529478684378167\n",
      "Validation loss: 0.003362446092069149\n",
      "\n",
      "EPOCH 14 of 50\n",
      "\n",
      "Training loss: 0.005439740269341403\n",
      "Validation loss: 0.0033839689567685127\n",
      "\n",
      "EPOCH 15 of 50\n",
      "\n",
      "Training loss: 0.005736232735216618\n",
      "Validation loss: 0.0033582205263276896\n",
      "\n",
      "EPOCH 16 of 50\n",
      "\n",
      "Training loss: 0.005646479523016347\n",
      "Validation loss: 0.0033743775760134063\n",
      "\n",
      "EPOCH 17 of 50\n",
      "\n",
      "Training loss: 0.00569052854552865\n",
      "Validation loss: 0.003384300973266363\n",
      "\n",
      "EPOCH 18 of 50\n",
      "\n",
      "Training loss: 0.005412655158175362\n",
      "Validation loss: 0.003375561675056815\n",
      "\n",
      "EPOCH 19 of 50\n",
      "\n",
      "Training loss: 0.006020816974341869\n",
      "Validation loss: 0.0033810902386903763\n",
      "\n",
      "EPOCH 20 of 50\n",
      "\n",
      "Training loss: 0.005895374549759759\n",
      "Validation loss: 0.0033713302109390497\n",
      "\n",
      "EPOCH 21 of 50\n",
      "\n",
      "Training loss: 0.00522856486754285\n",
      "Validation loss: 0.0033765363041311502\n",
      "\n",
      "EPOCH 22 of 50\n",
      "\n",
      "Training loss: 0.005442207161751058\n",
      "Validation loss: 0.0033755943489571414\n",
      "\n",
      "EPOCH 23 of 50\n",
      "\n",
      "Training loss: 0.005466370791610744\n",
      "Validation loss: 0.0033685939852148294\n",
      "\n",
      "EPOCH 24 of 50\n",
      "\n",
      "Training loss: 0.005403915653005242\n",
      "Validation loss: 0.0033785395013789334\n",
      "\n",
      "EPOCH 25 of 50\n",
      "\n",
      "Training loss: 0.005481026316475537\n",
      "Validation loss: 0.0033783488906919956\n",
      "\n",
      "EPOCH 26 of 50\n",
      "\n",
      "Training loss: 0.0056803282091601025\n",
      "Validation loss: 0.003393560958405336\n",
      "\n",
      "EPOCH 27 of 50\n",
      "\n",
      "Training loss: 0.00548846460878849\n",
      "Validation loss: 0.003383134181300799\n",
      "\n",
      "EPOCH 28 of 50\n",
      "\n",
      "Training loss: 0.005571730797075563\n",
      "Validation loss: 0.0033949598825226226\n",
      "\n",
      "EPOCH 29 of 50\n",
      "\n",
      "Training loss: 0.005726277931696839\n",
      "Validation loss: 0.0033897673711180687\n",
      "\n",
      "EPOCH 30 of 50\n",
      "\n",
      "Training loss: 0.005533595414211352\n",
      "Validation loss: 0.0033954758352289596\n",
      "\n",
      "EPOCH 31 of 50\n",
      "\n",
      "Training loss: 0.005636427758468522\n",
      "Validation loss: 0.0033952691592276096\n",
      "\n",
      "EPOCH 32 of 50\n",
      "\n",
      "Training loss: 0.005637363375475009\n",
      "Validation loss: 0.003385930166890224\n",
      "\n",
      "EPOCH 33 of 50\n",
      "\n",
      "Training loss: 0.005526242312043905\n",
      "Validation loss: 0.003359963263695439\n",
      "\n",
      "EPOCH 34 of 50\n",
      "\n",
      "Training loss: 0.005984199078132709\n",
      "Validation loss: 0.0033739008164654174\n",
      "\n",
      "EPOCH 35 of 50\n",
      "\n",
      "Training loss: 0.005656396142310566\n",
      "Validation loss: 0.0033658045964936414\n",
      "\n",
      "EPOCH 36 of 50\n",
      "\n",
      "Training loss: 0.005367222759458754\n",
      "Validation loss: 0.003375613441069921\n",
      "\n",
      "EPOCH 37 of 50\n",
      "\n",
      "Training loss: 0.0055378693828566205\n",
      "Validation loss: 0.003366444492712617\n",
      "\n",
      "EPOCH 38 of 50\n",
      "\n",
      "Training loss: 0.0054509577134417165\n",
      "Validation loss: 0.003359378936390082\n",
      "\n",
      "EPOCH 39 of 50\n",
      "\n",
      "Training loss: 0.005912555071214835\n",
      "Validation loss: 0.0033590023716290793\n",
      "\n",
      "EPOCH 40 of 50\n",
      "\n",
      "Training loss: 0.005443906980670161\n",
      "Validation loss: 0.0033684256486594677\n",
      "\n",
      "EPOCH 41 of 50\n",
      "\n",
      "Training loss: 0.005412228715916474\n",
      "Validation loss: 0.003380308859050274\n",
      "\n",
      "EPOCH 42 of 50\n",
      "\n",
      "Training loss: 0.005435406282130215\n",
      "Validation loss: 0.0033744622487574816\n",
      "\n",
      "EPOCH 43 of 50\n",
      "\n",
      "Training loss: 0.005892549072288805\n",
      "Validation loss: 0.0033920983939121165\n",
      "\n",
      "EPOCH 44 of 50\n",
      "\n",
      "Training loss: 0.005511710161550177\n",
      "Validation loss: 0.003390649255986015\n",
      "\n",
      "EPOCH 45 of 50\n",
      "\n",
      "Training loss: 0.005785603283180131\n",
      "Validation loss: 0.0033743594928334155\n",
      "\n",
      "EPOCH 46 of 50\n",
      "\n",
      "Training loss: 0.005702329075170888\n",
      "Validation loss: 0.0033870840755601725\n",
      "\n",
      "EPOCH 47 of 50\n",
      "\n",
      "Training loss: 0.005392847065296438\n",
      "Validation loss: 0.003370364739870032\n",
      "\n",
      "EPOCH 48 of 50\n",
      "\n",
      "Training loss: 0.005799002376281553\n",
      "Validation loss: 0.0033554667606949806\n",
      "\n",
      "EPOCH 49 of 50\n",
      "\n",
      "Training loss: 0.00546639578209983\n",
      "Validation loss: 0.003369561784590284\n",
      "\n",
      "EPOCH 50 of 50\n",
      "\n",
      "Training loss: 0.005327840775458349\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-30 06:48:16,117] Trial 47 finished with value: 0.0033735747759540877 and parameters: {'lr': 2.465164429248147e-05, 'weight_decay': 0.00012324787546192558, 'hidden_size': 434, 'dropout_rate': 0.4996852788535499}. Best is trial 16 with value: 0.0032828766076515117.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.0033735747759540877\n",
      "learning rate: 0.0001348845812356841\n",
      "weight_decay: 4.1513019516844074e-05\n",
      "hidden_size: 315\n",
      "dropout_rate: 0.48675193334635625\n",
      "\n",
      "EPOCH 1 of 50\n",
      "\n",
      "Training loss: 0.005432782617087166\n",
      "Validation loss: 0.003418881446123123\n",
      "\n",
      "EPOCH 2 of 50\n",
      "\n",
      "Training loss: 0.005673334209455384\n",
      "Validation loss: 0.003504018997773528\n",
      "\n",
      "EPOCH 3 of 50\n",
      "\n",
      "Training loss: 0.005335816803077857\n",
      "Validation loss: 0.003561601818849643\n",
      "\n",
      "EPOCH 4 of 50\n",
      "\n",
      "Training loss: 0.005544333304795954\n",
      "Validation loss: 0.003580440611888965\n",
      "\n",
      "EPOCH 5 of 50\n",
      "\n",
      "Training loss: 0.005643016555243068\n",
      "Validation loss: 0.0036290721812595925\n",
      "\n",
      "EPOCH 6 of 50\n",
      "\n",
      "Training loss: 0.005350824290265639\n",
      "Validation loss: 0.0036069457419216633\n",
      "\n",
      "EPOCH 7 of 50\n",
      "\n",
      "Training loss: 0.005466588772833347\n",
      "Validation loss: 0.0036176247522234917\n",
      "\n",
      "EPOCH 8 of 50\n",
      "\n",
      "Training loss: 0.00531235792570644\n",
      "Validation loss: 0.0036692882422357798\n",
      "\n",
      "EPOCH 9 of 50\n",
      "\n",
      "Training loss: 0.005561589522080289\n",
      "Validation loss: 0.003630140796303749\n",
      "\n",
      "EPOCH 10 of 50\n",
      "\n",
      "Training loss: 0.0056874688404301805\n",
      "Validation loss: 0.003652812292178472\n",
      "\n",
      "EPOCH 11 of 50\n",
      "\n",
      "Training loss: 0.005788550246506929\n",
      "Validation loss: 0.0036372911805907884\n",
      "\n",
      "EPOCH 12 of 50\n",
      "\n",
      "Training loss: 0.005766494044413169\n",
      "Validation loss: 0.0036016300631066165\n",
      "\n",
      "EPOCH 13 of 50\n",
      "\n",
      "Training loss: 0.005485544317505426\n",
      "Validation loss: 0.0036426426377147436\n",
      "\n",
      "EPOCH 14 of 50\n",
      "\n",
      "Training loss: 0.005346475324283044\n",
      "Validation loss: 0.003642344226439794\n",
      "\n",
      "EPOCH 15 of 50\n",
      "\n",
      "Training loss: 0.0057381885643634535\n",
      "Validation loss: 0.00364986271597445\n",
      "\n",
      "EPOCH 16 of 50\n",
      "\n",
      "Training loss: 0.005473146215081215\n",
      "Validation loss: 0.0036336831593265138\n",
      "\n",
      "EPOCH 17 of 50\n",
      "\n",
      "Training loss: 0.00560660282563832\n",
      "Validation loss: 0.0036136386139939227\n",
      "\n",
      "EPOCH 18 of 50\n",
      "\n",
      "Training loss: 0.005404171068221331\n",
      "Validation loss: 0.0036217369294414916\n",
      "\n",
      "EPOCH 19 of 50\n",
      "\n",
      "Training loss: 0.00550223250562946\n",
      "Validation loss: 0.003647413027162353\n",
      "\n",
      "EPOCH 20 of 50\n",
      "\n",
      "Training loss: 0.005496892612427473\n",
      "Validation loss: 0.0036411279191573462\n",
      "\n",
      "EPOCH 21 of 50\n",
      "\n",
      "Training loss: 0.0056404657661914825\n",
      "Validation loss: 0.0036315142642706633\n",
      "\n",
      "EPOCH 22 of 50\n",
      "\n",
      "Training loss: 0.006037250109430816\n",
      "Validation loss: 0.0036250438230733075\n",
      "\n",
      "EPOCH 23 of 50\n",
      "\n",
      "Training loss: 0.005424311022377676\n",
      "Validation loss: 0.003604465707515677\n",
      "\n",
      "EPOCH 24 of 50\n",
      "\n",
      "Training loss: 0.005454201665189531\n",
      "Validation loss: 0.0035974090763678155\n",
      "\n",
      "EPOCH 25 of 50\n",
      "\n",
      "Training loss: 0.005599984640462531\n",
      "Validation loss: 0.0036059787962585688\n",
      "\n",
      "EPOCH 26 of 50\n",
      "\n",
      "Training loss: 0.005550574356069167\n",
      "Validation loss: 0.003607276128605008\n",
      "\n",
      "EPOCH 27 of 50\n",
      "\n",
      "Training loss: 0.0054283602059715325\n",
      "Validation loss: 0.003636163737004002\n",
      "\n",
      "EPOCH 28 of 50\n",
      "\n",
      "Training loss: 0.005632414094482859\n",
      "Validation loss: 0.0036241556517779827\n",
      "\n",
      "EPOCH 29 of 50\n",
      "\n",
      "Training loss: 0.005666750327994426\n",
      "Validation loss: 0.0035964841954410076\n",
      "\n",
      "EPOCH 30 of 50\n",
      "\n",
      "Training loss: 0.005849699179331462\n",
      "Validation loss: 0.003593280135343472\n",
      "\n",
      "EPOCH 31 of 50\n",
      "\n",
      "Training loss: 0.005283339725186427\n",
      "Validation loss: 0.0036131133480618396\n",
      "\n",
      "EPOCH 32 of 50\n",
      "\n",
      "Training loss: 0.005633435625996854\n",
      "Validation loss: 0.0036202987345556417\n",
      "\n",
      "EPOCH 33 of 50\n",
      "\n",
      "Training loss: 0.005592541562186347\n",
      "Validation loss: 0.0036268893939753375\n",
      "\n",
      "EPOCH 34 of 50\n",
      "\n",
      "Training loss: 0.005700796739094787\n",
      "Validation loss: 0.003630970216666659\n",
      "\n",
      "EPOCH 35 of 50\n",
      "\n",
      "Training loss: 0.005388431432139542\n",
      "Validation loss: 0.003632602592309316\n",
      "\n",
      "EPOCH 36 of 50\n",
      "\n",
      "Training loss: 0.0055261351064675385\n",
      "Validation loss: 0.003613505124424895\n",
      "\n",
      "EPOCH 37 of 50\n",
      "\n",
      "Training loss: 0.005537780907212032\n",
      "Validation loss: 0.0035900098737329245\n",
      "\n",
      "EPOCH 38 of 50\n",
      "\n",
      "Training loss: 0.005436481080121464\n",
      "Validation loss: 0.0036173046100884676\n",
      "\n",
      "EPOCH 39 of 50\n",
      "\n",
      "Training loss: 0.005502389252392782\n",
      "Validation loss: 0.003636096060896913\n",
      "\n",
      "EPOCH 40 of 50\n",
      "\n",
      "Training loss: 0.0055304166291736895\n",
      "Validation loss: 0.003642284699405233\n",
      "\n",
      "EPOCH 41 of 50\n",
      "\n",
      "Training loss: 0.005435493929932515\n",
      "Validation loss: 0.0036424164039393267\n",
      "\n",
      "EPOCH 42 of 50\n",
      "\n",
      "Training loss: 0.005232324694386787\n",
      "Validation loss: 0.0036358299354712167\n",
      "\n",
      "EPOCH 43 of 50\n",
      "\n",
      "Training loss: 0.0055616794981890256\n",
      "Validation loss: 0.0035911652569969497\n",
      "\n",
      "EPOCH 44 of 50\n",
      "\n",
      "Training loss: 0.005229098153197103\n",
      "Validation loss: 0.0036171815202881894\n",
      "\n",
      "EPOCH 45 of 50\n",
      "\n",
      "Training loss: 0.005507782825993167\n",
      "Validation loss: 0.0035942756415655217\n",
      "\n",
      "EPOCH 46 of 50\n",
      "\n",
      "Training loss: 0.005342759709391329\n",
      "Validation loss: 0.0035940999320397773\n",
      "\n",
      "EPOCH 47 of 50\n",
      "\n",
      "Training loss: 0.005558599821395344\n",
      "Validation loss: 0.003589729700858394\n",
      "\n",
      "EPOCH 48 of 50\n",
      "\n",
      "Training loss: 0.005695030429503984\n",
      "Validation loss: 0.00360408549507459\n",
      "\n",
      "EPOCH 49 of 50\n",
      "\n",
      "Training loss: 0.005644303332600329\n",
      "Validation loss: 0.0036199266711870828\n",
      "\n",
      "EPOCH 50 of 50\n",
      "\n",
      "Training loss: 0.005290331112013923\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-30 06:52:23,323] Trial 48 finished with value: 0.0035941048990935087 and parameters: {'lr': 0.0001348845812356841, 'weight_decay': 4.1513019516844074e-05, 'hidden_size': 315, 'dropout_rate': 0.48675193334635625}. Best is trial 16 with value: 0.0032828766076515117.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.0035941048990935087\n",
      "learning rate: 1.958415748336568e-05\n",
      "weight_decay: 0.0002497017971682417\n",
      "hidden_size: 388\n",
      "dropout_rate: 0.45871954394896586\n",
      "\n",
      "EPOCH 1 of 50\n",
      "\n",
      "Training loss: 0.005350134231977993\n",
      "Validation loss: 0.0032970663936187825\n",
      "\n",
      "EPOCH 2 of 50\n",
      "\n",
      "Training loss: 0.005428922776546743\n",
      "Validation loss: 0.003314737152929107\n",
      "\n",
      "EPOCH 3 of 50\n",
      "\n",
      "Training loss: 0.005383187879083885\n",
      "Validation loss: 0.003333374004190167\n",
      "\n",
      "EPOCH 4 of 50\n",
      "\n",
      "Training loss: 0.00523840374727216\n",
      "Validation loss: 0.0033469763584434986\n",
      "\n",
      "EPOCH 5 of 50\n",
      "\n",
      "Training loss: 0.005256647347576088\n",
      "Validation loss: 0.0033555838745087385\n",
      "\n",
      "EPOCH 6 of 50\n",
      "\n",
      "Training loss: 0.005349032632592652\n",
      "Validation loss: 0.0033658319928993783\n",
      "\n",
      "EPOCH 7 of 50\n",
      "\n",
      "Training loss: 0.00533243571408093\n",
      "Validation loss: 0.003363782074302435\n",
      "\n",
      "EPOCH 8 of 50\n",
      "\n",
      "Training loss: 0.005259212080596222\n",
      "Validation loss: 0.003356638985375563\n",
      "\n",
      "EPOCH 9 of 50\n",
      "\n",
      "Training loss: 0.005223029707041051\n",
      "Validation loss: 0.003351263701915741\n",
      "\n",
      "EPOCH 10 of 50\n",
      "\n",
      "Training loss: 0.005280507418016593\n",
      "Validation loss: 0.0033529868815094233\n",
      "\n",
      "EPOCH 11 of 50\n",
      "\n",
      "Training loss: 0.005278683371014065\n",
      "Validation loss: 0.0033574167949457965\n",
      "\n",
      "EPOCH 12 of 50\n",
      "\n",
      "Training loss: 0.005318707269099023\n",
      "Validation loss: 0.00337452736372749\n",
      "\n",
      "EPOCH 13 of 50\n",
      "\n",
      "Training loss: 0.00553463795222342\n",
      "Validation loss: 0.0033679048841198287\n",
      "\n",
      "EPOCH 14 of 50\n",
      "\n",
      "Training loss: 0.005229658705906736\n",
      "Validation loss: 0.003350687368462483\n",
      "\n",
      "EPOCH 15 of 50\n",
      "\n",
      "Training loss: 0.005694733001291752\n",
      "Validation loss: 0.0033475564947972694\n",
      "\n",
      "EPOCH 16 of 50\n",
      "\n",
      "Training loss: 0.005177379896243413\n",
      "Validation loss: 0.0033668785666426024\n",
      "\n",
      "EPOCH 17 of 50\n",
      "\n",
      "Training loss: 0.005383531149062846\n",
      "Validation loss: 0.0033608194595823684\n",
      "\n",
      "EPOCH 18 of 50\n",
      "\n",
      "Training loss: 0.005431492761191394\n",
      "Validation loss: 0.0033597589936107397\n",
      "\n",
      "EPOCH 19 of 50\n",
      "\n",
      "Training loss: 0.005318622829185592\n",
      "Validation loss: 0.003358238454287251\n",
      "\n",
      "EPOCH 20 of 50\n",
      "\n",
      "Training loss: 0.005229730262524552\n",
      "Validation loss: 0.0033550492177406945\n",
      "\n",
      "EPOCH 21 of 50\n",
      "\n",
      "Training loss: 0.005247765686362982\n",
      "Validation loss: 0.003350385775168737\n",
      "\n",
      "EPOCH 22 of 50\n",
      "\n",
      "Training loss: 0.005491314145425956\n",
      "Validation loss: 0.0033516640154023967\n",
      "\n",
      "EPOCH 23 of 50\n",
      "\n",
      "Training loss: 0.005588485523023539\n",
      "Validation loss: 0.0033574077921609082\n",
      "\n",
      "EPOCH 24 of 50\n",
      "\n",
      "Training loss: 0.005004286947142746\n",
      "Validation loss: 0.0033544370283683143\n",
      "\n",
      "EPOCH 25 of 50\n",
      "\n",
      "Training loss: 0.005188607300321261\n",
      "Validation loss: 0.0033644266271342835\n",
      "\n",
      "EPOCH 26 of 50\n",
      "\n",
      "Training loss: 0.005284656925747792\n",
      "Validation loss: 0.003362727894758185\n",
      "\n",
      "EPOCH 27 of 50\n",
      "\n",
      "Training loss: 0.005392480900304185\n",
      "Validation loss: 0.0033504487170527377\n",
      "\n",
      "EPOCH 28 of 50\n",
      "\n",
      "Training loss: 0.005028963477040331\n",
      "Validation loss: 0.003358670510351658\n",
      "\n",
      "EPOCH 29 of 50\n",
      "\n",
      "Training loss: 0.005215765908360481\n",
      "Validation loss: 0.003367133904248476\n",
      "\n",
      "EPOCH 30 of 50\n",
      "\n",
      "Training loss: 0.005375506537449028\n",
      "Validation loss: 0.0033586383797228336\n",
      "\n",
      "EPOCH 31 of 50\n",
      "\n",
      "Training loss: 0.005244130915444758\n",
      "Validation loss: 0.0033675748854875565\n",
      "\n",
      "EPOCH 32 of 50\n",
      "\n",
      "Training loss: 0.005259162332448695\n",
      "Validation loss: 0.0033623562194406986\n",
      "\n",
      "EPOCH 33 of 50\n",
      "\n",
      "Training loss: 0.005615011064542664\n",
      "Validation loss: 0.003360408591106534\n",
      "\n",
      "EPOCH 34 of 50\n",
      "\n",
      "Training loss: 0.005370859832813342\n",
      "Validation loss: 0.0033520866806308427\n",
      "\n",
      "EPOCH 35 of 50\n",
      "\n",
      "Training loss: 0.005269802563513319\n",
      "Validation loss: 0.0033590293023735285\n",
      "\n",
      "EPOCH 36 of 50\n",
      "\n",
      "Training loss: 0.005160192363998956\n",
      "Validation loss: 0.0033642250734070935\n",
      "\n",
      "EPOCH 37 of 50\n",
      "\n",
      "Training loss: 0.005278718347350757\n",
      "Validation loss: 0.00335353659465909\n",
      "\n",
      "EPOCH 38 of 50\n",
      "\n",
      "Training loss: 0.005211592626033558\n",
      "Validation loss: 0.00336536547789971\n",
      "\n",
      "EPOCH 39 of 50\n",
      "\n",
      "Training loss: 0.005354100320902135\n",
      "Validation loss: 0.003359986391539375\n",
      "\n",
      "EPOCH 40 of 50\n",
      "\n",
      "Training loss: 0.005312561730129851\n",
      "Validation loss: 0.003352190057436625\n",
      "\n",
      "EPOCH 41 of 50\n",
      "\n",
      "Training loss: 0.005157147301360965\n",
      "Validation loss: 0.0033564132172614336\n",
      "\n",
      "EPOCH 42 of 50\n",
      "\n",
      "Training loss: 0.005387964865399731\n",
      "Validation loss: 0.0033715933871765933\n",
      "\n",
      "EPOCH 43 of 50\n",
      "\n",
      "Training loss: 0.0054039691264430685\n",
      "Validation loss: 0.003362692038839062\n",
      "\n",
      "EPOCH 44 of 50\n",
      "\n",
      "Training loss: 0.005072636866114206\n",
      "Validation loss: 0.003351454623043537\n",
      "\n",
      "EPOCH 45 of 50\n",
      "\n",
      "Training loss: 0.005270467605441809\n",
      "Validation loss: 0.0033562174066901207\n",
      "\n",
      "EPOCH 46 of 50\n",
      "\n",
      "Training loss: 0.005280050811254316\n",
      "Validation loss: 0.00335055124014616\n",
      "\n",
      "EPOCH 47 of 50\n",
      "\n",
      "Training loss: 0.0055262928104235064\n",
      "Validation loss: 0.003357807251935204\n",
      "\n",
      "EPOCH 48 of 50\n",
      "\n",
      "Training loss: 0.005125005807106693\n",
      "Validation loss: 0.003363215597346425\n",
      "\n",
      "EPOCH 49 of 50\n",
      "\n",
      "Training loss: 0.005232946740256416\n",
      "Validation loss: 0.0033532228941718736\n",
      "\n",
      "EPOCH 50 of 50\n",
      "\n",
      "Training loss: 0.005123235621593065\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-30 06:56:26,085] Trial 49 finished with value: 0.003347292309626937 and parameters: {'lr': 1.958415748336568e-05, 'weight_decay': 0.0002497017971682417, 'hidden_size': 388, 'dropout_rate': 0.45871954394896586}. Best is trial 16 with value: 0.0032828766076515117.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.003347292309626937\n",
      "{'lr': 0.00022213513827246681, 'weight_decay': 0.00011222525082498122, 'hidden_size': 305, 'dropout_rate': 0.4318767432591459}\n"
     ]
    }
   ],
   "source": [
    "# Optuna Hyperparameter Tuning\n",
    "import optuna\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=trial_count)\n",
    "print(study.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lr': 0.00022213513827246681, 'weight_decay': 0.00011222525082498122, 'hidden_size': 305, 'dropout_rate': 0.4318767432591459}\n",
      "\n",
      "EPOCH 1 of 50\n",
      "\n",
      "Training loss: 0.005767803561563293\n",
      "Validation loss: 0.0033256261764715114\n",
      "\n",
      "EPOCH 2 of 50\n",
      "\n",
      "Training loss: 0.005680902447137568\n",
      "Validation loss: 0.00336908937121431\n",
      "\n",
      "EPOCH 3 of 50\n",
      "\n",
      "Training loss: 0.005485808838986688\n",
      "Validation loss: 0.003386912246545156\n",
      "\n",
      "EPOCH 4 of 50\n",
      "\n",
      "Training loss: 0.005497760579196943\n",
      "Validation loss: 0.00340434528576831\n",
      "\n",
      "EPOCH 5 of 50\n",
      "\n",
      "Training loss: 0.005583293477280272\n",
      "Validation loss: 0.0034163009840995073\n",
      "\n",
      "EPOCH 6 of 50\n",
      "\n",
      "Training loss: 0.005202007479965687\n",
      "Validation loss: 0.0034341432619839907\n",
      "\n",
      "EPOCH 7 of 50\n",
      "\n",
      "Training loss: 0.005272911990889245\n",
      "Validation loss: 0.003410453675314784\n",
      "\n",
      "EPOCH 8 of 50\n",
      "\n",
      "Training loss: 0.005360884178015921\n",
      "Validation loss: 0.003407518845051527\n",
      "\n",
      "EPOCH 9 of 50\n",
      "\n",
      "Training loss: 0.005456977006461885\n",
      "Validation loss: 0.0034053342727323375\n",
      "\n",
      "EPOCH 10 of 50\n",
      "\n",
      "Training loss: 0.005169595281283061\n",
      "Validation loss: 0.0033883076005925736\n",
      "\n",
      "EPOCH 11 of 50\n",
      "\n",
      "Training loss: 0.005302900965842936\n",
      "Validation loss: 0.0034062946991374097\n",
      "\n",
      "EPOCH 12 of 50\n",
      "\n",
      "Training loss: 0.005150194771380888\n",
      "Validation loss: 0.0034173980044821897\n",
      "\n",
      "EPOCH 13 of 50\n",
      "\n",
      "Training loss: 0.0053169831323126955\n",
      "Validation loss: 0.003423373525341352\n",
      "\n",
      "EPOCH 14 of 50\n",
      "\n",
      "Training loss: 0.005594477005716827\n",
      "Validation loss: 0.003417837123076121\n",
      "\n",
      "EPOCH 15 of 50\n",
      "\n",
      "Training loss: 0.005248678072045247\n",
      "Validation loss: 0.0034119645909716687\n",
      "\n",
      "EPOCH 16 of 50\n",
      "\n",
      "Training loss: 0.005458089781718122\n",
      "Validation loss: 0.0034241591735432544\n",
      "\n",
      "EPOCH 17 of 50\n",
      "\n",
      "Training loss: 0.005505361645999882\n",
      "Validation loss: 0.0034202467650175095\n",
      "\n",
      "EPOCH 18 of 50\n",
      "\n",
      "Training loss: 0.005437537096440792\n",
      "Validation loss: 0.0034192956518381834\n",
      "\n",
      "EPOCH 19 of 50\n",
      "\n",
      "Training loss: 0.005353979766368866\n",
      "Validation loss: 0.003427567658945918\n",
      "\n",
      "EPOCH 20 of 50\n",
      "\n",
      "Training loss: 0.005563376083349188\n",
      "Validation loss: 0.0034214637707918882\n",
      "\n",
      "EPOCH 21 of 50\n",
      "\n",
      "Training loss: 0.005536353551886148\n",
      "Validation loss: 0.0034202671765039363\n",
      "\n",
      "EPOCH 22 of 50\n",
      "\n",
      "Training loss: 0.0053388745420508915\n",
      "Validation loss: 0.003413349467640122\n",
      "\n",
      "EPOCH 23 of 50\n",
      "\n",
      "Training loss: 0.005256472828073634\n",
      "Validation loss: 0.0034092559944838285\n",
      "\n",
      "EPOCH 24 of 50\n",
      "\n",
      "Training loss: 0.005271080907227265\n",
      "Validation loss: 0.0034090502498050532\n",
      "\n",
      "EPOCH 25 of 50\n",
      "\n",
      "Training loss: 0.005826812703162432\n",
      "Validation loss: 0.0034239637510230145\n",
      "\n",
      "EPOCH 26 of 50\n",
      "\n",
      "Training loss: 0.005286215157765482\n",
      "Validation loss: 0.0034128918002049127\n",
      "\n",
      "EPOCH 27 of 50\n",
      "\n",
      "Training loss: 0.005363109883748823\n",
      "Validation loss: 0.0034096463738630214\n",
      "\n",
      "EPOCH 28 of 50\n",
      "\n",
      "Training loss: 0.005323174487178524\n",
      "Validation loss: 0.0034080575375507274\n",
      "\n",
      "EPOCH 29 of 50\n",
      "\n",
      "Training loss: 0.005483940010890365\n",
      "Validation loss: 0.003424804968138536\n",
      "\n",
      "EPOCH 30 of 50\n",
      "\n",
      "Training loss: 0.005481586791574955\n",
      "Validation loss: 0.003410328800479571\n",
      "\n",
      "EPOCH 31 of 50\n",
      "\n",
      "Training loss: 0.0054494411322391694\n",
      "Validation loss: 0.003383771205941836\n",
      "\n",
      "EPOCH 32 of 50\n",
      "\n",
      "Training loss: 0.005398272407344646\n",
      "Validation loss: 0.0034030780661851168\n",
      "\n",
      "EPOCH 33 of 50\n",
      "\n",
      "Training loss: 0.005513623305079009\n",
      "Validation loss: 0.003420255690192183\n",
      "\n",
      "EPOCH 34 of 50\n",
      "\n",
      "Training loss: 0.005262140132900741\n",
      "Validation loss: 0.0034325724312414727\n",
      "\n",
      "EPOCH 35 of 50\n",
      "\n",
      "Training loss: 0.005431139893415902\n",
      "Validation loss: 0.0034332521415005126\n",
      "\n",
      "EPOCH 36 of 50\n",
      "\n",
      "Training loss: 0.005121866292837594\n",
      "Validation loss: 0.0034268361050635576\n",
      "\n",
      "EPOCH 37 of 50\n",
      "\n",
      "Training loss: 0.005435748491436243\n",
      "Validation loss: 0.0034177980851382017\n",
      "\n",
      "EPOCH 38 of 50\n",
      "\n",
      "Training loss: 0.0052716757895218\n",
      "Validation loss: 0.0033949377636114755\n",
      "\n",
      "EPOCH 39 of 50\n",
      "\n",
      "Training loss: 0.005339916303960813\n",
      "Validation loss: 0.0034186022045711675\n",
      "\n",
      "EPOCH 40 of 50\n",
      "\n",
      "Training loss: 0.0051589221434874665\n",
      "Validation loss: 0.0034229035954922438\n",
      "\n",
      "EPOCH 41 of 50\n",
      "\n",
      "Training loss: 0.005417038997014363\n",
      "Validation loss: 0.0034134770588328442\n",
      "\n",
      "EPOCH 42 of 50\n",
      "\n",
      "Training loss: 0.005544515844020579\n",
      "Validation loss: 0.003405713321020206\n",
      "\n",
      "EPOCH 43 of 50\n",
      "\n",
      "Training loss: 0.0053676413372159\n",
      "Validation loss: 0.0034229339410861335\n",
      "\n",
      "EPOCH 44 of 50\n",
      "\n",
      "Training loss: 0.0053427801467478275\n",
      "Validation loss: 0.003414541327704986\n",
      "\n",
      "EPOCH 45 of 50\n",
      "\n",
      "Training loss: 0.00529502984136343\n",
      "Validation loss: 0.0033852855364481607\n",
      "\n",
      "EPOCH 46 of 50\n",
      "\n",
      "Training loss: 0.005407905413044823\n",
      "Validation loss: 0.003401884421085318\n",
      "\n",
      "EPOCH 47 of 50\n",
      "\n",
      "Training loss: 0.004997649825074607\n",
      "Validation loss: 0.003411060431972146\n",
      "\n",
      "EPOCH 48 of 50\n",
      "\n",
      "Training loss: 0.005559942478107082\n",
      "Validation loss: 0.0034052434687813125\n",
      "\n",
      "EPOCH 49 of 50\n",
      "\n",
      "Training loss: 0.005175152845266793\n",
      "Validation loss: 0.003422162961214781\n",
      "\n",
      "EPOCH 50 of 50\n",
      "\n",
      "Training loss: 0.005291330886797773\n",
      "Validation loss: 0.0034154579819490514\n"
     ]
    }
   ],
   "source": [
    "best_params = study.best_trial.params\n",
    "print(best_params)\n",
    "model = VanillaNeuralNet(input_dim, best_params[\"hidden_size\"], best_params[\"dropout_rate\"]).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=best_params[\"lr\"], weight_decay=best_params[\"weight_decay\"])\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "loss_func = nn.MSELoss()\n",
    "\n",
    "num_epochs = 50 # training for more epochs on best model (which will be saved)\n",
    "\n",
    "train(model, train_loader, valid_loader, loss_func, optimizer, scheduler, num_epochs, device)\n",
    "\n",
    "# Train the model with the best hyperparameters on the entire dataset\n",
    "# for epoch in range(num_epochs):\n",
    "#     ########################### Training #####################################\n",
    "#     model.train(True)\n",
    "#     train_loss = 0\n",
    "#     count = 0\n",
    "#     for batch, (X, y) in enumerate(train_loader):\n",
    "#         X = X.float().to(device)\n",
    "#         y = y.float().to(device)\n",
    "#         prediction = model(X).to(device)\n",
    "#         optimizer.zero_grad()\n",
    "#         loss = loss_func(prediction, y.view(-1, 1).float().to(device))\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         train_loss += loss.float().item()\n",
    "#         count += 1\n",
    "#     print(\"Epoch\", epoch+1, \"Training loss:\", train_loss / count)\n",
    "#     scheduler.step()\n",
    "#     model.eval()\n",
    "#     val_loss = 0\n",
    "#     count = 0\n",
    "#     with torch.no_grad():\n",
    "#         for (X, y) in valid_loader:\n",
    "#             X = X.to(device)\n",
    "#             y = y.to(device)\n",
    "#             prediction = model(X).to(device)\n",
    "#             loss = loss_func(prediction, y.view(-1, 1))\n",
    "#             val_loss += loss.float().item()\n",
    "#             count += val_batch_size\n",
    "#         print(\"Validation loss:\", val_loss / count)\n",
    "\n",
    "# Save the best model\n",
    "torch.save(model.state_dict(), \"faceoff_mod_state_dict.pt\")\n",
    "# save the trained model\n",
    "torch.save(model, \"faceoff_mod.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.0074\n",
      "True labels: [-0.30201727, 1.2703346, 0.58898216, -0.82613456, -0.45925248, -0.354429, 0.22210003, -0.40684074, -0.30201727, -0.82613456, 0.3269235, -0.09237035, -0.30201727, -0.56407595, -0.5116642, -0.45925248, -0.6164877, -0.14478208, -0.82613456, -0.56407595, -0.82613456, -0.7737228, -0.66889936, -0.5116642, -0.45925248, -0.5116642, -0.82613456, -0.82613456, -0.7213111, -0.56407595, 0.27451175, 0.22210003, 1.6372167, -0.5116642, -0.45925248, -0.19719382, 0.5365704, 3.2095687, -0.30201727, -0.40684074, 0.22210003, -0.354429, -0.40684074, 0.58898216, 4.0481563, -0.7213111, -0.56407595, -0.14478208, -0.45925248, -0.19719382, 0.1696883, -0.5116642, -0.56407595, -0.82613456, -0.40684074, -0.039958622, -0.24960554, -0.14478208, -0.24960554, -0.82613456, 0.11727657, 1.6372167, -0.66889936, -0.7213111, -0.66889936, -0.19719382, -0.45925248, 1.3751581, -0.039958622, -0.6164877, 2.2137458, -0.56407595, -0.09237035, 0.11727657, 0.37933522, -0.82613456, 0.27451175, -0.19719382, -0.6164877, -0.039958622, -0.82613456, -0.354429, -0.14478208, -0.56407595, -0.45925248, 1.4275699, -0.66889936, -0.039958622, -0.7213111, -0.82613456, -0.45925248, 1.4799815, -0.66889936, -0.40684074, -0.40684074, -0.82613456, -0.56407595, -0.14478208, 1.6372167, 0.012453109, 1.6372167, 0.06486484, -0.7213111, -0.5116642, -0.7213111, 0.8510408, -0.354429, -0.6164877, 1.0606877, -0.66889936, -0.6164877, -0.56407595, -0.6164877, -0.09237035, 2.2661574, -0.354429, 0.06486484, -0.82613456, -0.6164877, -0.7737228, -0.354429, 1.6896285, -0.40684074, 0.06486484, -0.45925248, 0.5365704, 3.4192157, 0.06486484, -0.354429, -0.7737228, 0.58898216, -0.7213111, -0.5116642, -0.66889936, -0.45925248, -0.354429, -0.039958622, -0.039958622, 3.314392, -0.14478208, -0.24960554, 0.43174696, -0.66889936, 6.7211547, -0.40684074, -0.45925248, 0.6938056, -0.82613456, 3.157157, -0.66889936, 4.6770973, -0.40684074, -0.7213111, -0.7737228, 0.43174696, -0.30201727, -0.6164877, 0.1696883, -0.40684074, -0.5116642, -0.82613456, 2.737863, -0.45925248, 5.5156846, -0.82613456, 0.4841587, -0.24960554, -0.66889936, 0.22210003, 0.1696883, -0.56407595, 0.6413939, -0.7213111, 1.4275699, -0.56407595, -0.66889936, 0.7462173, 0.7462173, -0.6164877, -0.6164877, -0.19719382, -0.039958622, -0.6164877, -0.24960554, 0.79862905, -0.24960554, 0.11727657, 2.370981, 0.37933522, 0.11727657, 2.2661574, -0.039958622, -0.7213111, -0.40684074, -0.24960554, -0.82613456, 1.2703346, 0.79862905, 0.3269235, -0.7737228, -0.56407595, 2.6854513, -0.30201727, -0.7213111, -0.66889936, -0.30201727, -0.7737228, -0.66889936, -0.24960554, 2.2137458, -0.40684074, -0.6164877, -0.82613456, 0.43174696, -0.6164877, -0.7737228, 2.94751, 2.161334, -0.6164877, 0.3269235, -0.30201727, -0.56407595, -0.7737228, 2.9999218, -0.24960554, 0.012453109, -0.5116642, 1.2179229, -0.66889936, -0.7737228, -0.7213111, 1.008276, -0.14478208, -0.354429, 0.06486484, 0.11727657, -0.30201727, 0.6413939, -0.5116642, 1.2703346, -0.82613456, -0.82613456, -0.30201727, -0.6164877, 0.3269235, -0.24960554, -0.7213111, -0.82613456, -0.40684074, -0.40684074, -0.7737228, -0.5116642, 0.11727657, -0.30201727, 2.4233928, -0.40684074, -0.66889936, 1.1655111, 1.8468636, -0.82613456, 0.3269235, 0.3269235, -0.354429, -0.6164877, 0.27451175, -0.6164877, 0.6413939, -0.82613456, 0.37933522, 1.008276, 0.012453109, -0.66889936, -0.45925248, 0.6413939, -0.24960554, -0.56407595, -0.354429, -0.45925248, 0.58898216, -0.66889936, -0.66889936, 0.4841587, 2.4233928, -0.6164877, -0.30201727, -0.6164877, -0.039958622, -0.56407595, -0.6164877, -0.82613456, -0.5116642, -0.82613456, -0.039958622, 2.737863, 2.161334, -0.40684074, 0.58898216, 0.3269235, 0.22210003, 2.8426867, -0.7737228, -0.5116642, -0.82613456, -0.7213111, -0.66889936, 1.584805, -0.09237035, -0.40684074, -0.7737228, 0.95586425, -0.5116642, -0.56407595, -0.5116642, -0.6164877, -0.45925248, -0.56407595, -0.19719382, -0.09237035, -0.354429, -0.45925248, -0.45925248, -0.40684074, 2.004099, -0.66889936, -0.6164877, -0.66889936, -0.7737228, -0.30201727, -0.7737228, -0.6164877, -0.40684074, -0.6164877, -0.56407595, -0.30201727, -0.6164877, -0.6164877, 1.4275699, -0.354429, 0.8510408, -0.24960554, -0.039958622, 0.6413939, -0.24960554, -0.66889936, 0.11727657, -0.66889936, -0.82613456, -0.45925248, -0.14478208, -0.09237035, -0.09237035, 0.58898216, 0.3269235, -0.40684074, -0.66889936, 0.3269235, -0.7737228, -0.19719382, 1.4799815, -0.354429, 0.6413939, 2.2137458, -0.6164877, 0.012453109, -0.039958622, 0.1696883, 0.06486484, 0.43174696, -0.19719382, -0.56407595, -0.354429, 1.7420402, 0.22210003, -0.24960554, -0.7737228, -0.66889936, 0.6413939, 0.43174696, -0.7213111, -0.7213111, 0.11727657, -0.56407595, 0.7462173, -0.82613456, -0.66889936, 1.0606877, -0.24960554, -0.6164877, 0.012453109, -0.82613456, 1.1130995, -0.66889936, -0.14478208, -0.09237035, -0.5116642, -0.14478208, -0.039958622, 0.1696883, -0.40684074, 0.11727657, -0.354429, -0.30201727, -0.7737228, -0.30201727, 0.4841587, -0.354429, 0.27451175, -0.66889936, -0.30201727, -0.45925248, 0.43174696, 0.37933522, -0.7213111, 0.11727657, 0.8510408, -0.82613456, 0.012453109, -0.5116642, -0.14478208, -0.5116642, 0.3269235, 1.2179229, -0.30201727, 2.6854513, -0.039958622, -0.82613456, -0.5116642, -0.7737228, -0.7737228, 0.3269235, 0.8510408, 0.06486484, -0.7737228, -0.7737228, 0.012453109, 0.6938056, -0.7213111, -0.24960554, -0.56407595, -0.40684074, 0.79862905, -0.7213111, 0.4841587, -0.09237035, 0.012453109, 0.22210003, -0.40684074, -0.09237035, -0.09237035, -0.66889936, 0.1696883, 2.2661574, -0.30201727, -0.45925248, 0.11727657, 0.8510408, 0.012453109, 1.1655111, -0.7737228, -0.24960554, 0.95586425, 0.11727657, 0.06486484, -0.40684074, 1.2703346, 3.4192157, -0.24960554, -0.6164877, -0.56407595, 0.012453109, -0.7213111, -0.56407595, -0.7213111, 0.1696883, -0.354429, -0.7213111, -0.56407595, -0.82613456, -0.6164877, -0.039958622, 0.9034525, -0.45925248, -0.56407595, -0.82613456, 0.1696883, -0.19719382, -0.82613456, -0.24960554, -0.30201727, -0.7213111, -0.354429, -0.14478208, 0.1696883, -0.5116642, -0.66889936, -0.354429, -0.66889936, -0.6164877, -0.7737228, 0.06486484, -0.7213111, -0.82613456, 0.06486484, -0.7213111, 0.58898216, -0.82613456, -0.5116642, -0.40684074, -0.30201727, -0.82613456, 0.58898216, 0.3269235, 1.794452, -0.56407595, -0.354429, 0.11727657, -0.6164877, 1.4275699, 0.58898216, -0.40684074, -0.40684074, -0.56407595, -0.40684074, -0.82613456, -0.14478208, 2.5282161, -0.66889936, 0.9034525, -0.66889936, -0.7737228, -0.039958622, -0.7213111, -0.45925248, 0.43174696, -0.66889936, -0.7737228, 0.3269235, -0.82613456, 0.22210003, -0.30201727, 2.004099, 0.95586425, 2.580628, -0.30201727, -0.7737228, -0.354429, 2.8426867, -0.5116642, 2.370981, 0.9034525, 1.7420402, -0.66889936, -0.6164877, -0.30201727, -0.56407595, 1.1655111, -0.14478208, -0.6164877, 0.012453109, -0.24960554, -0.7737228, -0.56407595, 2.0565107, 0.4841587, -0.7213111, -0.14478208, -0.7213111, -0.7213111, 0.3269235, -0.14478208, -0.82613456, -0.30201727, -0.30201727, -0.5116642, 3.6288624, -0.039958622, -0.66889936, -0.82613456, -0.82613456, -0.5116642, 0.06486484, -0.19719382, -0.354429, 1.3227464, -0.354429, -0.7737228, -0.19719382, -0.039958622, 1.3227464, 0.4841587, 5.725332, -0.6164877, -0.6164877, 0.11727657, -0.7213111, -0.7737228, -0.30201727, 0.5365704, 0.22210003, -0.82613456, -0.14478208, -0.45925248, -0.6164877, 2.370981, -0.7213111, 2.2137458, 0.37933522, 0.1696883, -0.7737228, 0.012453109, 1.1655111, -0.14478208, -0.45925248, -0.354429, 0.1696883, -0.19719382, -0.24960554, -0.039958622, -0.66889936, -0.5116642, -0.354429, -0.7737228, -0.45925248, 0.1696883, 0.9034525, -0.7213111, -0.039958622, -0.19719382, -0.66889936, -0.19719382, -0.6164877, 1.6896285, 0.37933522, -0.45925248, -0.56407595, 0.11727657, -0.30201727, -0.6164877, -0.039958622, 1.3751581, -0.45925248, 2.2661574, 0.1696883, -0.56407595, 2.8950982, -0.6164877, 2.2661574, -0.7737228, -0.09237035, -0.09237035, -0.354429, -0.30201727, -0.66889936, 0.58898216, 0.79862905, 4.310215, -0.6164877, 0.012453109, -0.354429, -0.14478208, -0.354429, -0.5116642, -0.82613456, 0.22210003, -0.30201727, -0.56407595, 3.733686, -0.039958622, -0.354429, 2.580628, 0.8510408, -0.45925248, -0.45925248, 0.22210003, -0.56407595, 1.8468636, -0.56407595, 4.2053914, -0.45925248, -0.56407595, -0.66889936, -0.7213111, 0.43174696, -0.6164877, -0.14478208, -0.40684074, -0.66889936, -0.7737228, 0.1696883, -0.5116642, -0.56407595, 0.4841587, 0.11727657, 0.5365704, -0.5116642, -0.7213111, -0.039958622, -0.7737228, -0.6164877, 0.37933522, -0.6164877, 0.37933522, -0.30201727, -0.6164877, -0.40684074, 0.012453109, -0.30201727, -0.6164877, -0.6164877, -0.5116642, -0.5116642, -0.24960554, -0.45925248, -0.039958622]\n",
      "Predictions: [0.13836151, 0.09972043, 0.09788899, 0.2005986, 0.1779904, 0.14129801, 0.12034872, 0.10955031, 0.10679331, 0.15001114, 0.08844128, 0.16270654, 0.103110954, 0.13826178, 0.15106298, 0.060464203, 0.15024309, 0.14690939, 0.1255054, 0.2179428, 0.14593996, 0.06458267, 0.06530461, 0.13371868, 0.082718596, 0.14435816, 0.15540804, 0.14040956, 0.10852498, 0.16102363, 0.11023897, 0.10088241, 0.17938305, 0.09120968, 0.14048839, 0.18753229, 0.101306394, 0.21491234, 0.15697138, 0.11619222, 0.08771904, 0.13065852, 0.16320987, 0.16615827, 0.2001765, 0.056697622, 0.1836956, 0.13523006, 0.17330891, 0.16778055, 0.1462192, 0.105658844, 0.14954056, 0.16145097, 0.19108254, 0.081093505, 0.22192112, 0.11950536, 0.07885046, 0.13953473, 0.15883836, 0.19170757, 0.14273857, 0.14435816, 0.113838494, 0.10781826, 0.16403851, 0.10546583, 0.18185268, 0.12798189, 0.19641544, 0.1006598, 0.13890111, 0.1439389, 0.156668, 0.109801866, 0.20737275, 0.20260012, 0.14898294, 0.17826042, 0.16865194, 0.20952101, 0.14562228, 0.1468622, 0.13139443, 0.1554841, 0.15679798, 0.18079568, 0.15784433, 0.16956045, 0.18763852, 0.10849805, 0.12775306, 0.12307204, 0.10916361, 0.1375625, 0.16390964, 0.123855874, 0.14489783, 0.11421068, 0.22086354, 0.14966784, 0.14021145, 0.10250595, 0.10191576, 0.16431032, 0.15800725, 0.09292649, 0.22918622, 0.21035068, 0.1680811, 0.13208652, 0.18930572, 0.08140309, 0.12305711, 0.11226642, 0.12864952, 0.012003729, 0.18026611, 0.16307633, 0.13383023, 0.12170254, 0.1528476, 0.118195504, 0.11473669, 0.1316094, 0.110535845, 0.15160032, 0.15176615, 0.23242858, 0.18340321, 0.14048839, 0.1502785, 0.07777722, 0.09512405, 0.120899156, 0.094846025, 0.106429994, 0.13724627, 0.06642787, 0.09075348, 0.11884199, 0.12825078, 0.1394823, 0.096961156, 0.10812335, 0.13538529, 0.084568605, 0.10143544, -0.026152669, 0.059548095, 0.13330504, 0.1366548, 0.120997846, 0.1580629, 0.075877935, 0.14087062, 0.14867982, 0.13318436, 0.13612507, 0.105091065, 0.08180493, 0.081257656, 0.15917097, 0.06289524, 0.1650517, 0.15161183, 0.13330504, 0.1022497, 0.023037972, 0.20367232, 0.09454943, 0.11119467, 0.0809242, 0.14962833, 0.17278633, 0.08559836, 0.11028843, 0.08739728, 0.07975092, 0.15326928, 0.19305828, 0.12976697, 0.16326319, 0.1274896, 0.13877913, 0.106213436, 0.11509578, 0.11885035, 0.1658568, 0.1507916, 0.16909547, 0.1381834, 0.139533, 0.10179748, 0.16827922, 0.14754333, 0.09450991, 0.12436418, 0.114586405, 0.14809789, 0.13010009, 0.085840054, 0.10750005, 0.15240376, 0.13619016, 0.13245781, 0.13761237, 0.17318158, 0.11173163, 0.13349019, 0.11583057, 0.15167868, 0.10015771, 0.1274896, 0.0661702, 0.15369041, 0.23337306, 0.12645973, 0.13172027, 0.14794573, 0.12885089, 0.1216057, 0.12206456, 0.11826325, 0.09086101, 0.120899156, 0.15038411, 0.1062458, 0.098929584, 0.14445357, 0.10449974, 0.08874877, 0.12816928, 0.10151155, 0.19161408, 0.07890167, 0.06322243, 0.17984633, 0.19360913, 0.08212188, 0.17141546, 0.18365067, 0.1766137, 0.11933261, 0.08418517, 0.10044782, 0.1722127, 0.101142645, 0.117973134, 0.012692658, 0.14665242, 0.18979053, 0.097568154, 0.17050497, 0.14703085, 0.20124243, 0.14078964, 0.15356684, 0.07333727, 0.19823821, 0.18272482, 0.13706109, 0.1286534, 0.15634264, 0.07790269, 0.09863196, 0.18426362, 0.05748941, 0.14854361, 0.14756657, 0.14156944, 0.14667591, 0.14679094, 0.22644249, 0.18058044, 0.12223102, 0.19148193, 0.17217852, 0.13194922, 0.18010929, 0.11555605, 0.15921168, 0.07333032, 0.10841046, 0.09566878, 0.1558056, 0.17189537, 0.098853156, 0.11537306, 0.10691577, 0.14803809, 0.12576872, 0.1139919, 0.13675986, 0.18031965, 0.13309869, 0.09606676, 0.1642057, 0.19176584, 0.11200686, 0.12388768, 0.11194269, 0.07152949, 0.1466627, 0.17232688, 0.17102958, 0.14999029, 0.10551898, 0.15425915, 0.14627668, 0.13917902, 0.111172505, 0.10557407, 0.16242865, 0.18010929, 0.12193753, 0.13817592, 0.19291602, 0.15212362, 0.12709947, 0.18722545, 0.10456517, 0.1915756, 0.115214325, 0.16355292, 0.07288565, 0.13369763, 0.13419977, 0.142376, 0.1330658, 0.21012308, 0.1517386, 0.18049529, 0.09807657, 0.15737216, 0.07338118, 0.07302062, 0.17603351, 0.08379863, 0.17171194, 0.17260812, 0.13642913, 0.12309463, 0.16090523, 0.12280989, 0.17369404, 0.07634629, 0.09903282, 0.18965276, 0.15822303, 0.09450991, 0.11694725, 0.17210539, 0.1458245, 0.16851272, 0.12166725, 0.14498901, 0.14253266, 0.09369621, 0.12490755, 0.18244107, 0.12085591, 0.17893887, 0.12239124, 0.1773181, 0.1608452, 0.13816532, 0.11687733, 0.12282245, 0.1469487, 0.16706234, 0.13536862, 0.098198265, 0.13708973, 0.13573235, 0.19059876, 0.14592928, 0.18136333, 0.21043965, 0.16723804, 0.11881952, 0.14181955, 0.13284974, 0.051975697, 0.0817685, 0.12641811, 0.045833744, 0.09256751, 0.1396843, 0.16069378, 0.12553562, 0.14399515, 0.106504396, 0.11854862, 0.08030383, 0.08484915, 0.15787977, 0.19653498, 0.11992669, 0.120720804, 0.13665292, 0.10627568, 0.043945834, 0.095470734, 0.12005825, 0.061890185, 0.19998963, 0.1506012, 0.15832748, 0.044894636, 0.16050658, 0.09341785, 0.14522861, 0.09465255, 0.11938599, 0.1343632, 0.14432968, 0.14381716, 0.20212719, 0.17171194, 0.1685106, 0.08625543, 0.13161291, 0.14281209, 0.09282942, 0.1062458, 0.14952852, 0.13778017, 0.12844968, 0.15124328, 0.13395706, 0.07931273, 0.1681201, 0.18934758, 0.114331424, 0.07345342, 0.19912134, 0.122911915, 0.1776267, 0.16600901, 0.2629272, 0.09033388, 0.19705723, 0.13089238, 0.10657337, 0.1899361, 0.1308741, 0.11214118, 0.14986263, 0.10982451, 0.10873753, 0.18837535, 0.09809145, 0.13519324, 0.09031744, 0.14273839, 0.10923393, 0.08919932, 0.19875808, 0.19063728, 0.1372818, 0.112364575, 0.08180493, 0.18423282, 0.13738741, 0.12734315, 0.18891795, 0.13683511, 0.16424097, 0.13983132, 0.095276445, 0.17308466, 0.13092618, 0.13390183, 0.06923559, 0.15611729, 0.122140884, 0.10524243, 0.14286433, 0.1084743, 0.12650299, 0.16335961, 0.16493085, 0.19682832, 0.1538073, 0.14039998, 0.1188498, 0.11560993, 0.108453095, 0.055428818, 0.121731535, 0.21373756, 0.14995316, 0.10534988, 0.1219791, 0.11092518, 0.0651962, 0.105073586, 0.20245403, 0.1592801, 0.12519205, 0.11170435, 0.13583346, 0.14104787, 0.18104817, 0.13361172, 0.1332519, 0.011882471, 0.12657247, 0.14645128, 0.11511515, 0.14605163, 0.10683553, 0.19361533, 0.084695615, 0.14868908, 0.124017484, 0.13124844, 0.15228897, 0.19679666, 0.1276814, 0.17575185, 0.10229795, 0.06779226, 0.120876715, 0.13159287, 0.15977663, 0.15128796, 0.1450487, 0.09312175, 0.10132237, 0.14243458, 0.15042052, 0.18407439, 0.035171337, 0.12366043, 0.050984606, 0.06702081, 0.08695638, 0.21113549, 0.16959406, 0.07794009, 0.15873887, 0.068561375, 0.15057193, 0.12397063, 0.13953473, 0.13311592, 0.13237755, 0.13850273, 0.15862678, 0.14331746, 0.17159188, 0.14405565, 0.16125523, 0.08667634, 0.18651761, 0.16590707, 0.15328074, 0.13269083, 0.084865555, 0.10571829, 0.13556255, 0.1735584, 0.075792596, 0.15149044, 0.151053, 0.16058424, 0.049742542, 0.16678296, 0.0915965, 0.09386152, 0.18138151, 0.13579479, 0.14846304, 0.084256575, 0.21215297, 0.068705656, 0.119935334, 0.120734304, 0.16514541, 0.0563704, 0.121101335, 0.117182896, 0.13521536, 0.097893655, 0.08887993, 0.08602648, 0.14411597, 0.16663277, 0.15677804, 0.16496228, 0.076079205, 0.11273123, 0.14620751, 0.14418985, 0.10806851, 0.12879308, 0.099883735, 0.12664987, 0.14384924, 0.080013126, 0.12621315, 0.15870942, 0.16645573, 0.13514857, 0.15657894, 0.12868258, 0.109610364, 0.11133818, 0.1972403, 0.10301873, 0.092558146, 0.12296734, 0.10619974, 0.16766101, 0.09549628, 0.1298021, 0.1292456, 0.009275721, 0.08760248, 0.1263983, 0.12604086, 0.07287662, 0.1276814, 0.03456212, 0.1986763, 0.19305606, 0.10384978, 0.17315269, 0.20268013, 0.12424057, 0.17893545, 0.1354032, 0.17963143, 0.06616847, 0.10341807, 0.15246266, 0.18631445, 0.20912185, 0.13450953, 0.18217, 0.10662112, 0.080766246, 0.13176256, 0.17143108, 0.0396896, 0.17569523, 0.16502902, 0.1292456, 0.1281293, 0.0671622, 0.10045172, 0.09894668, 0.12111707, 0.17015181, 0.07825165, 0.13262694, 0.22009303, 0.057564363, 0.13818143, 0.1083671, 0.15167329, 0.1231183, 0.20829314, 0.19556482, 0.17760436, 0.21203212, 0.18868588, 0.14464879, 0.18868949, 0.18130589, 0.12467982, 0.11280827, 0.14803673, 0.14255701, 0.14510274, 0.1722127, 0.12445479, 0.1365948, 0.16736491, 0.12985083, 0.059173517, 0.15723781, 0.13895954, 0.19015142, 0.15895577, 0.09916042, 0.14049524, 0.1349107, 0.1034915, 0.11100693, 0.1537389, 0.14476536, 0.07347442, 0.14522573, 0.22103219, 0.1399705, 0.121023074, 0.21096422, 0.14373614, 0.13139288, 0.18161331, 0.11265242, 0.11423053, 0.19462119, 0.07068373, 0.10921538, 0.07656683, 0.19802396, 0.10880493, 0.043610357, 0.0993302]\n",
      "Loss: 1.007414956887563\n",
      "R-squared Coefficient: -0.0137\n",
      "16.820802377414562\n",
      "19.332568368241123\n",
      "True labels (unnormalized): [ 10.982033    41.379635    28.20734      0.84949875   7.942272\n",
      "   9.96878     21.114567     8.955526    10.982033     0.84949875\n",
      "  23.141073    15.035047    10.982033     5.915766     6.929019\n",
      "   7.942272     4.9025116   14.021793     0.84949875   5.915766\n",
      "   0.84949875   1.8627529    3.8892593    6.929019     7.942272\n",
      "   6.929019     0.84949875   0.84949875   2.8760061    5.915766\n",
      "  22.12782     21.114567    48.47241      6.929019     7.942272\n",
      "  13.008539    27.194088    78.87001     10.982033     8.955526\n",
      "  21.114567     9.96878      8.955526    28.20734     95.08206\n",
      "   2.8760061    5.915766    14.021793     7.942272    13.008539\n",
      "  20.101313     6.929019     5.915766     0.84949875   8.955526\n",
      "  16.0483      11.995287    14.021793    11.995287     0.84949875\n",
      "  19.08806     48.47241      3.8892593    2.8760061    3.8892593\n",
      "  13.008539     7.942272    43.406143    16.0483       4.9025116\n",
      "  59.618195     5.915766    15.035047    19.08806     24.154327\n",
      "   0.84949875  22.12782     13.008539     4.9025116   16.0483\n",
      "   0.84949875   9.96878     14.021793     5.915766     7.942272\n",
      "  44.419395     3.8892593   16.0483       2.8760061    0.84949875\n",
      "   7.942272    45.432648     3.8892593    8.955526     8.955526\n",
      "   0.84949875   5.915766    14.021793    48.47241     17.061554\n",
      "  48.47241     18.074806     2.8760061    6.929019     2.8760061\n",
      "  33.273605     9.96878      4.9025116   37.326622     3.8892593\n",
      "   4.9025116    5.915766     4.9025116   15.035047    60.631447\n",
      "   9.96878     18.074806     0.84949875   4.9025116    1.8627529\n",
      "   9.96878     49.485664     8.955526    18.074806     7.942272\n",
      "  27.194088    82.92303     18.074806     9.96878      1.8627529\n",
      "  28.20734      2.8760061    6.929019     3.8892593    7.942272\n",
      "   9.96878     16.0483      16.0483      80.896515    14.021793\n",
      "  11.995287    25.16758      3.8892593  146.758        8.955526\n",
      "   7.942272    30.233847     0.84949875  77.85676      3.8892593\n",
      " 107.24111      8.955526     2.8760061    1.8627529   25.16758\n",
      "  10.982033     4.9025116   20.101313     8.955526     6.929019\n",
      "   0.84949875  69.75073      7.942272   123.453156     0.84949875\n",
      "  26.180834    11.995287     3.8892593   21.114567    20.101313\n",
      "   5.915766    29.220594     2.8760061   44.419395     5.915766\n",
      "   3.8892593   31.2471      31.2471       4.9025116    4.9025116\n",
      "  13.008539    16.0483       4.9025116   11.995287    32.260353\n",
      "  11.995287    19.08806     62.65796     24.154327    19.08806\n",
      "  60.631447    16.0483       2.8760061    8.955526    11.995287\n",
      "   0.84949875  41.379635    32.260353    23.141073     1.8627529\n",
      "   5.915766    68.73747     10.982033     2.8760061    3.8892593\n",
      "  10.982033     1.8627529    3.8892593   11.995287    59.618195\n",
      "   8.955526     4.9025116    0.84949875  25.16758      4.9025116\n",
      "   1.8627529   73.80374     58.604942     4.9025116   23.141073\n",
      "  10.982033     5.915766     1.8627529   74.817       11.995287\n",
      "  17.061554     6.929019    40.366383     3.8892593    1.8627529\n",
      "   2.8760061   36.31337     14.021793     9.96878     18.074806\n",
      "  19.08806     10.982033    29.220594     6.929019    41.379635\n",
      "   0.84949875   0.84949875  10.982033     4.9025116   23.141073\n",
      "  11.995287     2.8760061    0.84949875   8.955526     8.955526\n",
      "   1.8627529    6.929019    19.08806     10.982033    63.67121\n",
      "   8.955526     3.8892593   39.353127    52.52542      0.84949875\n",
      "  23.141073    23.141073     9.96878      4.9025116   22.12782\n",
      "   4.9025116   29.220594     0.84949875  24.154327    36.31337\n",
      "  17.061554     3.8892593    7.942272    29.220594    11.995287\n",
      "   5.915766     9.96878      7.942272    28.20734      3.8892593\n",
      "   3.8892593   26.180834    63.67121      4.9025116   10.982033\n",
      "   4.9025116   16.0483       5.915766     4.9025116    0.84949875\n",
      "   6.929019     0.84949875  16.0483      69.75073     58.604942\n",
      "   8.955526    28.20734     23.141073    21.114567    71.77724\n",
      "   1.8627529    6.929019     0.84949875   2.8760061    3.8892593\n",
      "  47.459156    15.035047     8.955526     1.8627529   35.300114\n",
      "   6.929019     5.915766     6.929019     4.9025116    7.942272\n",
      "   5.915766    13.008539    15.035047     9.96878      7.942272\n",
      "   7.942272     8.955526    55.565186     3.8892593    4.9025116\n",
      "   3.8892593    1.8627529   10.982033     1.8627529    4.9025116\n",
      "   8.955526     4.9025116    5.915766    10.982033     4.9025116\n",
      "   4.9025116   44.419395     9.96878     33.273605    11.995287\n",
      "  16.0483      29.220594    11.995287     3.8892593   19.08806\n",
      "   3.8892593    0.84949875   7.942272    14.021793    15.035047\n",
      "  15.035047    28.20734     23.141073     8.955526     3.8892593\n",
      "  23.141073     1.8627529   13.008539    45.432648     9.96878\n",
      "  29.220594    59.618195     4.9025116   17.061554    16.0483\n",
      "  20.101313    18.074806    25.16758     13.008539     5.915766\n",
      "   9.96878     50.498917    21.114567    11.995287     1.8627529\n",
      "   3.8892593   29.220594    25.16758      2.8760061    2.8760061\n",
      "  19.08806      5.915766    31.2471       0.84949875   3.8892593\n",
      "  37.326622    11.995287     4.9025116   17.061554     0.84949875\n",
      "  38.339874     3.8892593   14.021793    15.035047     6.929019\n",
      "  14.021793    16.0483      20.101313     8.955526    19.08806\n",
      "   9.96878     10.982033     1.8627529   10.982033    26.180834\n",
      "   9.96878     22.12782      3.8892593   10.982033     7.942272\n",
      "  25.16758     24.154327     2.8760061   19.08806     33.273605\n",
      "   0.84949875  17.061554     6.929019    14.021793     6.929019\n",
      "  23.141073    40.366383    10.982033    68.73747     16.0483\n",
      "   0.84949875   6.929019     1.8627529    1.8627529   23.141073\n",
      "  33.273605    18.074806     1.8627529    1.8627529   17.061554\n",
      "  30.233847     2.8760061   11.995287     5.915766     8.955526\n",
      "  32.260353     2.8760061   26.180834    15.035047    17.061554\n",
      "  21.114567     8.955526    15.035047    15.035047     3.8892593\n",
      "  20.101313    60.631447    10.982033     7.942272    19.08806\n",
      "  33.273605    17.061554    39.353127     1.8627529   11.995287\n",
      "  35.300114    19.08806     18.074806     8.955526    41.379635\n",
      "  82.92303     11.995287     4.9025116    5.915766    17.061554\n",
      "   2.8760061    5.915766     2.8760061   20.101313     9.96878\n",
      "   2.8760061    5.915766     0.84949875   4.9025116   16.0483\n",
      "  34.28686      7.942272     5.915766     0.84949875  20.101313\n",
      "  13.008539     0.84949875  11.995287    10.982033     2.8760061\n",
      "   9.96878     14.021793    20.101313     6.929019     3.8892593\n",
      "   9.96878      3.8892593    4.9025116    1.8627529   18.074806\n",
      "   2.8760061    0.84949875  18.074806     2.8760061   28.20734\n",
      "   0.84949875   6.929019     8.955526    10.982033     0.84949875\n",
      "  28.20734     23.141073    51.51217      5.915766     9.96878\n",
      "  19.08806      4.9025116   44.419395    28.20734      8.955526\n",
      "   8.955526     5.915766     8.955526     0.84949875  14.021793\n",
      "  65.697716     3.8892593   34.28686      3.8892593    1.8627529\n",
      "  16.0483       2.8760061    7.942272    25.16758      3.8892593\n",
      "   1.8627529   23.141073     0.84949875  21.114567    10.982033\n",
      "  55.565186    35.300114    66.71097     10.982033     1.8627529\n",
      "   9.96878     71.77724      6.929019    62.65796     34.28686\n",
      "  50.498917     3.8892593    4.9025116   10.982033     5.915766\n",
      "  39.353127    14.021793     4.9025116   17.061554    11.995287\n",
      "   1.8627529    5.915766    56.578438    26.180834     2.8760061\n",
      "  14.021793     2.8760061    2.8760061   23.141073    14.021793\n",
      "   0.84949875  10.982033    10.982033     6.929019    86.976036\n",
      "  16.0483       3.8892593    0.84949875   0.84949875   6.929019\n",
      "  18.074806    13.008539     9.96878     42.39289      9.96878\n",
      "   1.8627529   13.008539    16.0483      42.39289     26.180834\n",
      " 127.50617      4.9025116    4.9025116   19.08806      2.8760061\n",
      "   1.8627529   10.982033    27.194088    21.114567     0.84949875\n",
      "  14.021793     7.942272     4.9025116   62.65796      2.8760061\n",
      "  59.618195    24.154327    20.101313     1.8627529   17.061554\n",
      "  39.353127    14.021793     7.942272     9.96878     20.101313\n",
      "  13.008539    11.995287    16.0483       3.8892593    6.929019\n",
      "   9.96878      1.8627529    7.942272    20.101313    34.28686\n",
      "   2.8760061   16.0483      13.008539     3.8892593   13.008539\n",
      "   4.9025116   49.485664    24.154327     7.942272     5.915766\n",
      "  19.08806     10.982033     4.9025116   16.0483      43.406143\n",
      "   7.942272    60.631447    20.101313     5.915766    72.79049\n",
      "   4.9025116   60.631447     1.8627529   15.035047    15.035047\n",
      "   9.96878     10.982033     3.8892593   28.20734     32.260353\n",
      " 100.14833      4.9025116   17.061554     9.96878     14.021793\n",
      "   9.96878      6.929019     0.84949875  21.114567    10.982033\n",
      "   5.915766    89.00254     16.0483       9.96878     66.71097\n",
      "  33.273605     7.942272     7.942272    21.114567     5.915766\n",
      "  52.52542      5.915766    98.12182      7.942272     5.915766\n",
      "   3.8892593    2.8760061   25.16758      4.9025116   14.021793\n",
      "   8.955526     3.8892593    1.8627529   20.101313     6.929019\n",
      "   5.915766    26.180834    19.08806     27.194088     6.929019\n",
      "   2.8760061   16.0483       1.8627529    4.9025116   24.154327\n",
      "   4.9025116   24.154327    10.982033     4.9025116    8.955526\n",
      "  17.061554    10.982033     4.9025116    4.9025116    6.929019\n",
      "   6.929019    11.995287     7.942272    16.0483    ]\n",
      "Predictions (unnormalized): [19.495686 18.748655 18.71325  20.698889 20.261814 19.552456 19.147453\n",
      " 18.938692 18.885391 19.720903 18.5306   19.96634  18.814201 19.49376\n",
      " 19.741238 17.98973  19.725388 19.660938 19.247145 21.034197 19.642197\n",
      " 18.069351 18.08331  19.405928 18.419966 19.611616 19.82524  19.53528\n",
      " 18.918869 19.933804 18.952005 18.771118 20.288738 18.58412  19.536804\n",
      " 20.446283 18.779316 20.97561  19.855463 19.067097 18.516638 19.346767\n",
      " 19.976068 20.03307  20.69073  17.916914 20.37211  19.435146 20.171309\n",
      " 20.064432 19.647594 18.86346  19.711805 19.942064 20.51492  18.388548\n",
      " 21.111107 19.13115  18.345184 19.518368 19.891556 20.527002 19.580305\n",
      " 19.611616 19.021593 18.905207 19.992088 18.859728 20.336483 19.295021\n",
      " 20.618017 18.766815 19.506119 19.603512 19.849598 18.943554 20.829851\n",
      " 20.737583 19.701025 20.267035 20.081278 20.871382 19.636055 19.660027\n",
      " 19.360994 19.82671  19.85211  20.316048 19.87234  20.098843 20.448338\n",
      " 18.918348 19.290598 19.200102 18.931215 19.48024  19.989597 19.215256\n",
      " 19.62205  19.02879  21.090662 19.714266 19.53145  18.802505 18.791096\n",
      " 19.997343 19.875488 18.61731  21.25156  20.88742  20.070242 19.374374\n",
      " 20.480568 18.394533 19.199814 18.991201 19.307928 17.052866 20.305809\n",
      " 19.973488 19.408085 19.173626 19.77574  19.105825 19.038958 19.36515\n",
      " 18.957745 19.751627 19.754833 21.314243 20.366459 19.536804 19.726072\n",
      " 18.324436 18.659796 19.158094 18.65442  18.878368 19.474125 18.105024\n",
      " 18.5753   19.118324 19.30022  19.517353 18.69531  18.911104 19.438148\n",
      " 18.45573  18.78181  16.315205 17.972021 19.397932 19.46269  19.160002\n",
      " 19.876564 18.287718 19.544193 19.695166 19.3956   19.45245  18.852484\n",
      " 18.402302 18.391722 19.897987 18.03673  20.011677 19.751848 19.397932\n",
      " 18.797552 17.266186 20.758312 18.648685 18.970482 18.385275 19.713503\n",
      " 20.161205 18.47564  18.952961 18.510416 18.362593 19.783892 20.553116\n",
      " 19.329533 19.9771   19.285503 19.50376  18.874182 19.0459   19.118484\n",
      " 20.02724  19.735992 20.089853 19.492243 19.518333 18.788809 20.074072\n",
      " 19.673195 18.647923 19.225082 19.036053 19.683916 19.335972 18.480312\n",
      " 18.899055 19.767159 19.453709 19.381552 19.481203 20.168848 18.980862\n",
      " 19.40151  19.060104 19.753141 18.757109 19.285503 18.100042 19.792034\n",
      " 21.332504 19.265594 19.367294 19.680973 19.311821 19.171753 19.180624\n",
      " 19.107136 18.57738  19.158094 19.728113 18.874807 18.733366 19.61346\n",
      " 18.841051 18.536545 19.298645 18.783281 20.525196 18.346174 18.043055\n",
      " 20.297695 20.563765 18.40843  20.134705 20.371243 20.235199 19.127808\n",
      " 18.448318 18.762716 20.150116 18.77615  19.101526 17.066185 19.655972\n",
      " 20.48994  18.707047 20.117102 19.663286 20.711336 19.54263  19.789644\n",
      " 18.2386   20.653257 20.353342 19.470547 19.308004 19.843307 18.326862\n",
      " 18.727612 20.383091 17.93222  19.692532 19.673643 19.557703 19.656425\n",
      " 19.65865  21.198517 20.311886 19.183842 20.52264  20.149456 19.37172\n",
      " 20.302778 19.054798 19.898773 18.238466 18.916656 18.670326 19.832926\n",
      " 20.143982 18.731888 19.05126  18.88776  19.682758 19.252235 19.024559\n",
      " 19.464722 20.306845 19.393942 18.67802  19.995321 20.52813  18.986183\n",
      " 19.21587  18.984943 18.203651 19.65617  20.152325 20.127243 19.7205\n",
      " 18.860756 19.803028 19.648706 19.51149  18.970053 18.86182  19.960966\n",
      " 20.302778 19.17817  19.492098 20.550365 19.761744 19.277962 20.440351\n",
      " 18.842316 20.524452 19.048191 19.9827   18.22987  19.405521 19.41523\n",
      " 19.573296 19.393307 20.883022 19.7543   20.31024  18.716875 19.86321\n",
      " 18.239449 18.23248  20.223984 18.440845 20.140436 20.15776  19.458328\n",
      " 19.200539 19.931515 19.195034 20.178755 18.296772 18.735361 20.487278\n",
      " 19.87966  18.647923 19.081694 20.148043 19.639965 20.078587 19.172943\n",
      " 19.623814 19.576324 18.63219  19.235586 20.347857 19.157257 20.280151\n",
      " 19.18694  20.248817 19.930353 19.491894 19.080341 19.195276 19.6617\n",
      " 20.050547 19.437826 18.719227 19.4711   19.444859 20.505566 19.64199\n",
      " 20.327023 20.889141 20.053944 19.11789  19.562538 19.38913  17.825626\n",
      " 18.401598 19.26479  17.706886 18.61037  19.52126  19.927425 19.247728\n",
      " 19.604599 18.879807 19.112652 18.373281 18.461155 19.873024 20.620329\n",
      " 19.139294 19.154646 19.462654 18.875385 17.67039  18.666496 19.141838\n",
      " 18.0173   20.687117 19.732311 19.88168  17.688732 19.923807 18.62681\n",
      " 19.628445 18.65068  19.128841 19.418388 19.611067 19.601158 20.728441\n",
      " 20.140436 20.078545 18.488342 19.36522  19.581728 18.615435 18.874807\n",
      " 19.711573 19.484447 19.304066 19.744724 19.410538 18.354122 20.070995\n",
      " 20.481379 19.031122 18.240847 20.67033  19.197006 20.254784 20.030184\n",
      " 21.903862 18.567188 20.630424 19.351288 18.88114  20.492756 19.350935\n",
      " 18.98878  19.718033 18.943993 18.922977 20.462582 18.717163 19.434435\n",
      " 18.566872 19.580303 18.932575 18.545254 20.663307 20.506311 19.474813\n",
      " 18.9931   18.402302 20.382496 19.476854 19.282673 20.473072 19.466177\n",
      " 19.996002 19.524101 18.66274  20.166973 19.351942 19.40947  18.159306\n",
      " 19.838951 19.1821   18.85541  19.582737 18.917889 19.26643  19.978964\n",
      " 20.00934  20.626    19.794292 19.535095 19.118475 19.05584  18.91748\n",
      " 17.892384 19.174187 20.9529   19.719782 18.857487 19.178972 18.965271\n",
      " 18.081213 18.852144 20.73476  19.900097 19.241087 18.980335 19.446812\n",
      " 19.54762  20.320929 19.40386  19.396904 17.050522 19.267775 19.652082\n",
      " 19.046274 19.644356 18.886208 20.563885 18.458187 19.695345 19.21838\n",
      " 19.358173 19.76494  20.625387 19.289211 20.218538 18.798485 18.131401\n",
      " 19.15766  19.36483  19.909695 19.745588 19.624966 18.621086 18.779625\n",
      " 19.574429 19.728817 20.379433 17.500755 19.211477 17.806465 18.116488\n",
      " 18.501892 20.902594 20.099491 18.327585 19.889633 18.14627  19.731745\n",
      " 19.217474 19.518368 19.394276 19.380001 19.498417 19.887466 19.591497\n",
      " 20.138115 19.605768 19.93828  18.496479 20.426666 20.028214 19.784113\n",
      " 19.386057 18.461472 18.864609 19.441574 20.176132 18.286068 19.749502\n",
      " 19.741045 19.925308 17.782454 20.045145 18.591599 18.635387 20.327374\n",
      " 19.446064 19.690975 18.4497   20.922264 18.14906  19.139462 19.154907\n",
      " 20.013489 17.910587 19.162003 19.086248 19.434862 18.713339 18.53908\n",
      " 18.483915 19.606934 20.042242 19.851725 20.009947 18.291609 19.000187\n",
      " 19.64737  19.608362 18.910046 19.310703 18.751812 19.26927  19.601778\n",
      " 18.367662 19.260826 19.889063 20.03882  19.43357  19.847876 19.308567\n",
      " 18.939854 18.973255 20.633965 18.81242  18.61019  19.198078 18.873917\n",
      " 20.06212  18.66699  19.330212 19.319452 17.000126 18.514383 19.264406\n",
      " 19.257496 18.229694 19.289211 17.488977 20.661726 20.553072 18.828485\n",
      " 20.16829  20.73913  19.222692 20.280085 19.438494 20.29354  18.10001\n",
      " 18.820139 19.768297 20.422739 20.863667 19.421217 20.342617 18.882063\n",
      " 18.382221 19.36811  20.135006 17.588104 20.217443 20.011238 19.319452\n",
      " 19.29787  18.11922  18.762793 18.733696 19.162308 20.110275 18.333609\n",
      " 19.384823 21.075768 17.93367  19.492205 18.915817 19.753036 19.200996\n",
      " 20.847645 20.601574 20.25435  20.91993  20.468586 19.617235 20.468655\n",
      " 20.32591  19.231184 19.001677 19.682734 19.576796 19.62601  20.150116\n",
      " 19.226833 19.46153  20.056396 19.331154 17.964779 19.860613 19.507248\n",
      " 20.496918 19.893826 18.73783  19.536936 19.428974 18.82156  18.966852\n",
      " 19.79297  19.61949  18.241253 19.62839  21.093922 19.526793 19.16049\n",
      " 20.899282 19.599592 19.360964 20.331854 18.998663 19.029173 20.58333\n",
      " 18.187302 18.932217 18.301037 20.649115 18.924282 17.663902 18.74111 ]\n",
      "Loss: 1.007414956887563\n",
      "R-squared Coefficient: -0.0137\n",
      "              player_id  opposing_player_id    season    y_pred  \\\n",
      "1425  ALEKSANDER.BARKOV       RYAN.O'REILLY  20172018  0.138362   \n",
      "3557      ALEX.WENNBERG         TYLER.BOZAK  20212022  0.099720   \n",
      "907       RYAN.O'REILLY        NICK.FOLIGNO  20172018  0.097889   \n",
      "1846     TOMAS.PLEKANEC  FREDERICK.GAUDREAU  20172018  0.200599   \n",
      "2417        SEAN.KURALY        DYLAN.LARKIN  20172018  0.177990   \n",
      "...                 ...                 ...       ...       ...   \n",
      "3244       JARED.MCCANN        MARK.LETESTU  20172018  0.076567   \n",
      "162       CHARLIE.COYLE        JORDAN.STAAL  20172018  0.198024   \n",
      "57          NICK.BONINO        DAVID.KREJCI  20172018  0.108805   \n",
      "184        TYLER.SEGUIN        PAUL.STASTNY  20172018  0.043610   \n",
      "2793      EVGENI.MALKIN   HENRIK.ZETTERBERG  20172018  0.099330   \n",
      "\n",
      "      y_pred_unnorm    y_true  y_true_unnorm  \n",
      "1425      19.495686 -0.302017      10.982033  \n",
      "3557      18.748655  1.270335      41.379635  \n",
      "907       18.713249  0.588982      28.207340  \n",
      "1846      20.698889 -0.826135       0.849499  \n",
      "2417      20.261814 -0.459252       7.942272  \n",
      "...             ...       ...            ...  \n",
      "3244      18.301037 -0.511664       6.929019  \n",
      "162       20.649115 -0.511664       6.929019  \n",
      "57        18.924282 -0.249606      11.995287  \n",
      "184       17.663902 -0.459252       7.942272  \n",
      "2793      18.741110 -0.039959      16.048300  \n",
      "\n",
      "[714 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "test_loss = 0\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch, (X, y) in enumerate(test_loader):\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "        #print(\"\" + str(X) + \"\" + str(y))\n",
    "        prediction = model(X).to(device)\n",
    "        # Remove extra dimensions from y\n",
    "        y = y.unsqueeze(1).squeeze()\n",
    "        loss = loss_func(prediction.squeeze(), y)\n",
    "        test_loss += loss.float().item()\n",
    "        y_true.extend(y.cpu().numpy())\n",
    "        y_pred.extend(prediction.cpu().numpy().squeeze())\n",
    "\n",
    "test_loss /= len(test_loader)\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "r2 = r2_score(y_true, y_pred)\n",
    "print(\"True labels:\", y_true)\n",
    "print(\"Predictions:\", y_pred)\n",
    "print(\"Loss:\", test_loss)\n",
    "print(f\"R-squared Coefficient: {r2:.4f}\")\n",
    "\n",
    "y_pred = np.array(y_pred)\n",
    "y_true = np.array(y_true)\n",
    "\n",
    "mean = data_unnorm['FA_zone_time'].mean()\n",
    "sd = data_unnorm['FA_zone_time'].std()\n",
    "print(mean)\n",
    "print(sd)\n",
    "\n",
    "# Calculate the unnormalized predictions and true labels\n",
    "y_pred_unnorm = mean + sd * y_pred\n",
    "y_true_unnorm = mean + sd * y_true\n",
    "\n",
    "print(\"True labels (unnormalized):\", y_true_unnorm)\n",
    "print(\"Predictions (unnormalized):\", y_pred_unnorm)\n",
    "print(\"Loss:\", test_loss)\n",
    "print(f\"R-squared Coefficient: {r2:.4f}\")\n",
    "\n",
    "data = {\n",
    "    'player_id': player_ids_test,\n",
    "    'opposing_player_id': opposing_player_ids_test,\n",
    "    'season': seasons_test,\n",
    "    'y_pred': y_pred,\n",
    "    'y_pred_unnorm': y_pred_unnorm,\n",
    "    'y_true': y_true,\n",
    "    'y_true_unnorm': y_true_unnorm\n",
    "}\n",
    "\n",
    "# Create a DataFrame from the dictionary\n",
    "summary_df = pd.DataFrame(data)\n",
    "\n",
    "# Print the summary DataFrame\n",
    "print(summary_df)\n",
    "summary_df.to_csv(\"neural_net_faceoff_preds.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lr': 0.05079528480724134, 'weight_decay': 0.0002664512798888168, 'hidden_size': 371, 'dropout_rate': 0.30519095473457875}\n",
      "\n",
      "EPOCH 1 of 50\n",
      "\n",
      "Training loss: 0.6805602010091146\n",
      "Validation loss: 721.4608125\n",
      "\n",
      "EPOCH 2 of 50\n",
      "\n",
      "Training loss: 0.5989303792317708\n",
      "Validation loss: 19.4476015625\n",
      "\n",
      "EPOCH 3 of 50\n",
      "\n",
      "Training loss: 0.4945974324544271\n",
      "Validation loss: 1.871621826171875\n",
      "\n",
      "EPOCH 4 of 50\n",
      "\n",
      "Training loss: 0.3958716735839844\n",
      "Validation loss: 1.329352294921875\n",
      "\n",
      "EPOCH 5 of 50\n",
      "\n",
      "Training loss: 0.44238685099283853\n",
      "Validation loss: 0.656499755859375\n",
      "\n",
      "EPOCH 6 of 50\n",
      "\n",
      "Training loss: 0.4221099853515625\n",
      "Validation loss: 0.40468789672851563\n",
      "\n",
      "EPOCH 7 of 50\n",
      "\n",
      "Training loss: 0.34276007588704427\n",
      "Validation loss: 0.3281842041015625\n",
      "\n",
      "EPOCH 8 of 50\n",
      "\n",
      "Training loss: 0.41156744384765626\n",
      "Validation loss: 0.3112650451660156\n",
      "\n",
      "EPOCH 9 of 50\n",
      "\n",
      "Training loss: 0.4296044413248698\n",
      "Validation loss: 0.3059061279296875\n",
      "\n",
      "EPOCH 10 of 50\n",
      "\n",
      "Training loss: 0.39637403361002604\n",
      "Validation loss: 0.30223544311523437\n",
      "\n",
      "EPOCH 11 of 50\n",
      "\n",
      "Training loss: 0.40227322387695313\n",
      "Validation loss: 0.3002938232421875\n",
      "\n",
      "EPOCH 12 of 50\n",
      "\n",
      "Training loss: 0.47181070963541666\n",
      "Validation loss: 0.29971142578125\n",
      "\n",
      "EPOCH 13 of 50\n",
      "\n",
      "Training loss: 0.40132481892903643\n",
      "Validation loss: 0.2994045104980469\n",
      "\n",
      "EPOCH 14 of 50\n",
      "\n",
      "Training loss: 0.37923162841796876\n",
      "Validation loss: 0.2989622802734375\n",
      "\n",
      "EPOCH 15 of 50\n",
      "\n",
      "Training loss: 0.3679151509602865\n",
      "Validation loss: 0.2987954406738281\n",
      "\n",
      "EPOCH 16 of 50\n",
      "\n",
      "Training loss: 0.37831889851888023\n",
      "Validation loss: 0.2988221435546875\n",
      "\n",
      "EPOCH 17 of 50\n",
      "\n",
      "Training loss: 0.3866609903971354\n",
      "Validation loss: 0.298796630859375\n",
      "\n",
      "EPOCH 18 of 50\n",
      "\n",
      "Training loss: 0.3507939961751302\n",
      "Validation loss: 0.29883981323242187\n",
      "\n",
      "EPOCH 19 of 50\n",
      "\n",
      "Training loss: 0.4024333292643229\n",
      "Validation loss: 0.2989914855957031\n",
      "\n",
      "EPOCH 20 of 50\n",
      "\n",
      "Training loss: 0.368602294921875\n",
      "Validation loss: 0.2988533935546875\n",
      "\n",
      "EPOCH 21 of 50\n",
      "\n",
      "Training loss: 0.42070962524414063\n",
      "Validation loss: 0.29879183959960937\n",
      "\n",
      "EPOCH 22 of 50\n",
      "\n",
      "Training loss: 0.4044859415690104\n",
      "Validation loss: 0.29893304443359375\n",
      "\n",
      "EPOCH 23 of 50\n",
      "\n",
      "Training loss: 0.398800526936849\n",
      "Validation loss: 0.29929302978515626\n",
      "\n",
      "EPOCH 24 of 50\n",
      "\n",
      "Training loss: 0.45229090372721353\n",
      "Validation loss: 0.29954852294921874\n",
      "\n",
      "EPOCH 25 of 50\n",
      "\n",
      "Training loss: 0.3452628580729167\n",
      "Validation loss: 0.29940313720703127\n",
      "\n",
      "EPOCH 26 of 50\n",
      "\n",
      "Training loss: 0.4285741984049479\n",
      "Validation loss: 0.29967572021484373\n",
      "\n",
      "EPOCH 27 of 50\n",
      "\n",
      "Training loss: 0.34404463704427085\n",
      "Validation loss: 0.29990176391601564\n",
      "\n",
      "EPOCH 28 of 50\n",
      "\n",
      "Training loss: 0.4416959737141927\n",
      "Validation loss: 0.3000442810058594\n",
      "\n",
      "EPOCH 29 of 50\n",
      "\n",
      "Training loss: 0.3732965596516927\n",
      "Validation loss: 0.3008600769042969\n",
      "\n",
      "EPOCH 30 of 50\n",
      "\n",
      "Training loss: 0.3762234802246094\n",
      "Validation loss: 0.3012192077636719\n",
      "\n",
      "EPOCH 31 of 50\n",
      "\n",
      "Training loss: 0.40760389200846353\n",
      "Validation loss: 0.3014153747558594\n",
      "\n",
      "EPOCH 32 of 50\n",
      "\n",
      "Training loss: 0.4535986938476563\n",
      "Validation loss: 0.3010049743652344\n",
      "\n",
      "EPOCH 33 of 50\n",
      "\n",
      "Training loss: 0.4188819071451823\n",
      "Validation loss: 0.300384521484375\n",
      "\n",
      "EPOCH 34 of 50\n",
      "\n",
      "Training loss: 0.3651580810546875\n",
      "Validation loss: 0.30015957641601565\n",
      "\n",
      "EPOCH 35 of 50\n",
      "\n",
      "Training loss: 0.34004566446940104\n",
      "Validation loss: 0.3006828918457031\n",
      "\n",
      "EPOCH 36 of 50\n",
      "\n",
      "Training loss: 0.4079169209798177\n",
      "Validation loss: 0.30041949462890627\n",
      "\n",
      "EPOCH 37 of 50\n",
      "\n",
      "Training loss: 0.39012762451171873\n",
      "Validation loss: 0.30029296875\n",
      "\n",
      "EPOCH 38 of 50\n",
      "\n",
      "Training loss: 0.382911376953125\n",
      "Validation loss: 0.30081155395507814\n",
      "\n",
      "EPOCH 39 of 50\n",
      "\n",
      "Training loss: 0.45986991373697916\n",
      "Validation loss: 0.30113796997070313\n",
      "\n",
      "EPOCH 40 of 50\n",
      "\n",
      "Training loss: 0.36808561197916667\n",
      "Validation loss: 0.30156472778320315\n",
      "\n",
      "EPOCH 41 of 50\n",
      "\n",
      "Training loss: 0.39533298746744794\n",
      "Validation loss: 0.3020265808105469\n",
      "\n",
      "EPOCH 42 of 50\n",
      "\n",
      "Training loss: 0.4432085774739583\n",
      "Validation loss: 0.30163485717773436\n",
      "\n",
      "EPOCH 43 of 50\n",
      "\n",
      "Training loss: 0.38912552897135416\n",
      "Validation loss: 0.3017039489746094\n",
      "\n",
      "EPOCH 44 of 50\n",
      "\n",
      "Training loss: 0.441428944905599\n",
      "Validation loss: 0.3012891845703125\n",
      "\n",
      "EPOCH 45 of 50\n",
      "\n",
      "Training loss: 0.3948177388509115\n",
      "Validation loss: 0.30150460815429686\n",
      "\n",
      "EPOCH 46 of 50\n",
      "\n",
      "Training loss: 0.3956632283528646\n",
      "Validation loss: 0.3010899658203125\n",
      "\n",
      "EPOCH 47 of 50\n",
      "\n",
      "Training loss: 0.40416892496744794\n",
      "Validation loss: 0.30134765625\n",
      "\n",
      "EPOCH 48 of 50\n",
      "\n",
      "Training loss: 0.35978090413411457\n",
      "Validation loss: 0.30190850830078125\n",
      "\n",
      "EPOCH 49 of 50\n",
      "\n",
      "Training loss: 0.34459294128417967\n",
      "Validation loss: 0.30142803955078123\n",
      "\n",
      "EPOCH 50 of 50\n",
      "\n",
      "Training loss: 0.4319450174967448\n",
      "Validation loss: 0.30015359497070315\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.30015359497070315"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(best_params)\n",
    "model = VanillaNeuralNet(input_dim, best_params[\"hidden_size\"], best_params[\"dropout_rate\"]).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=best_params[\"lr\"], weight_decay=best_params[\"weight_decay\"])\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "loss_func = nn.MSELoss()\n",
    "\n",
    "num_epochs = 50 # training for more epochs on best model (which will be saved)\n",
    "\n",
    "train(model, train_loader, valid_loader, loss_func, optimizer, scheduler, num_epochs, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(833, 694)\n",
      "(833, 310)\n",
      "Test Loss: 497308586.6667\n",
      "True labels: [40.0, 32.0, 7.0, 26.0, 9.0, 23.0, 14.0, 60.0, 5.0, 38.0, 30.0, 4.0, 28.0, 33.0, 4.0, 10.0, 28.0, 5.0, 20.0, 10.0, 16.0, 32.0, 7.0, 3.0, 9.0, 15.0, 8.0, 3.0, 10.0, 7.0, 19.0, 13.0, 10.0, 16.0, 7.0, 28.0, 5.0, 9.0, 3.0, 9.0, 5.0, 10.0, 4.0, 7.0, 3.0, 11.0, 13.0, 48.0, 22.0, 23.0, 13.0, 19.0, 8.0, 8.0, 19.0, 5.0, 12.0, 15.0, 65.0, 14.0, 18.0, 2.0, 44.0, 22.0, 3.0, 15.0, 9.0, 94.0, 8.0, 82.0, 11.0, 4.0, 30.0, 4.0, 9.0, 3.0, 19.0, 16.0, 7.0, 22.0, 53.0, 7.0, 5.0, 27.0, 11.0, 7.0, 18.0, 14.0, 26.0, 19.0, 12.0, 12.0, 22.0, 7.0, 15.0, 35.0, 12.0, 1.0, 3.0, 3.0, 13.0, 17.0, 18.0, 3.0, 7.0, 6.0, 5.0, 2.0, 10.0, 4.0, 8.0, 4.0, 14.0, 22.0, 29.0, 3.0, 5.0, 2.0, 7.0, 23.0, 12.0, 40.0, 9.0, 3.0, 30.0, 19.0, 19.0, 7.0, 19.0, 7.0, 67.0, 63.0, 6.0, 1.0, 9.0, 51.0, 22.0, 18.0, 22.0, 4.0, 12.0, 21.0, 7.0, 3.0, 12.0, 5.0, 18.0, 17.0, 12.0, 5.0, 4.0, 1.0, 4.0, 22.0, 5.0, 9.0, 16.0, 5.0, 61.0, 25.0, 14.0, 12.0, 15.0, 15.0, 22.0, 4.0, 1.0, 4.0, 40.0, 4.0, 5.0, 5.0, 15.0, 24.0, 7.0, 39.0, 3.0, 4.0, 38.0, 24.0, 10.0, 32.0, 51.0, 25.0, 10.0, 62.0, 22.0, 20.0, 6.0, 11.0, 9.0, 2.0, 40.0, 83.0, 41.0, 37.0, 23.0, 17.0, 8.0, 107.0, 12.0, 12.0, 56.0, 22.0, 6.0, 52.0, 45.0, 10.0, 5.0, 3.0, 41.0, 38.0, 5.0, 4.0, 11.0, 18.0, 1.0, 4.0, 5.0, 7.0, 4.0, 2.0, 11.0, 19.0, 3.0, 10.0, 16.0, 15.0, 12.0, 9.0, 12.0, 16.0, 19.0, 7.0, 75.0, 7.0, 50.0, 8.0, 110.0, 21.0, 6.0, 5.0, 7.0, 26.0, 6.0, 30.0, 15.0, 81.0, 21.0, 16.0, 4.0, 3.0, 4.0, 82.0, 7.0, 5.0, 18.0, 3.0, 4.0, 4.0, 1.0, 8.0, 3.0, 6.0, 4.0, 1.0, 6.0, 1.0, 1.0, 2.0, 14.0, 9.0, 31.0, 15.0, 12.0, 8.0, 22.0, 21.0, 3.0, 48.0, 3.0, 19.0, 42.0, 83.0, 6.0, 8.0, 16.0, 9.0, 5.0, 4.0, 10.0, 9.0, 26.0, 7.0, 20.0, 31.0, 48.0, 8.0, 26.0, 4.0, 27.0, 9.0, 34.0, 26.0, 15.0, 9.0, 22.0, 22.0, 17.0, 73.0, 18.0, 9.0, 4.0, 42.0, 4.0, 14.0, 19.0, 29.0, 16.0, 8.0, 43.0, 6.0, 15.0, 16.0, 109.0, 12.0, 33.0, 14.0, 31.0, 13.0, 1.0, 5.0, 25.0, 12.0, 6.0, 85.0, 2.0, 17.0, 27.0, 7.0, 6.0, 9.0, 1.0, 25.0, 37.0, 8.0, 23.0, 2.0, 23.0, 3.0, 27.0, 41.0, 15.0, 20.0, 26.0, 29.0, 1.0, 15.0, 3.0, 26.0, 3.0, 31.0, 12.0, 16.0, 78.0, 10.0, 45.0, 1.0, 6.0, 12.0, 6.0, 13.0, 14.0, 4.0, 22.0, 13.0, 9.0, 14.0, 19.0, 4.0, 4.0, 110.0, 1.0, 7.0, 16.0, 5.0, 4.0, 4.0, 4.0, 39.0, 4.0, 15.0, 5.0, 3.0, 6.0, 7.0, 12.0, 8.0, 2.0, 3.0, 68.0, 3.0, 10.0, 11.0, 9.0, 8.0, 37.0, 48.0, 2.0, 13.0, 17.0, 48.0, 85.0, 1.0, 5.0, 8.0, 13.0, 1.0, 47.0, 16.0, 5.0, 22.0, 4.0, 46.0, 4.0, 10.0, 4.0, 19.0, 5.0, 7.0, 13.0, 5.0, 116.0, 101.0, 27.0, 6.0, 4.0, 12.0, 2.0, 2.0, 10.0, 9.0, 8.0, 9.0, 9.0, 11.0, 8.0, 4.0, 49.0, 18.0, 18.0, 6.0, 1.0, 31.0, 1.0, 8.0, 29.0, 19.0, 5.0, 13.0, 84.0, 28.0, 34.0, 25.0, 20.0, 2.0, 3.0, 13.0, 2.0, 23.0, 69.0, 3.0, 3.0, 15.0, 13.0, 18.0, 10.0, 8.0, 7.0, 3.0, 71.0, 16.0, 18.0, 34.0, 8.0, 2.0, 19.0, 10.0, 5.0, 20.0, 6.0, 4.0, 8.0, 12.0, 126.0, 15.0, 44.0, 8.0, 25.0, 1.0, 5.0, 32.0, 5.0, 1.0, 7.0, 20.0, 1.0, 21.0, 16.0, 4.0, 50.0, 87.0, 45.0, 4.0, 130.0, 11.0, 52.0, 22.0, 36.0, 8.0, 8.0, 14.0, 13.0, 4.0, 10.0, 10.0, 5.0, 11.0, 65.0, 20.0, 17.0, 15.0, 8.0, 35.0, 7.0, 22.0, 6.0, 31.0, 2.0, 4.0, 20.0, 8.0, 7.0, 6.0, 8.0, 21.0, 13.0, 11.0, 21.0, 8.0, 14.0, 4.0, 24.0, 17.0, 6.0, 23.0, 43.0, 22.0, 20.0, 3.0, 20.0, 16.0, 54.0, 53.0, 24.0, 5.0, 13.0, 64.0, 30.0, 19.0, 69.0, 2.0, 13.0, 18.0, 4.0, 70.0, 2.0, 22.0, 5.0, 33.0, 6.0, 21.0, 2.0, 11.0, 7.0, 5.0, 3.0, 4.0, 7.0, 38.0, 21.0, 50.0, 29.0, 31.0, 9.0, 6.0, 7.0, 63.0, 1.0, 1.0, 17.0, 4.0, 6.0, 12.0, 13.0, 11.0, 2.0, 27.0, 30.0, 17.0, 21.0, 34.0, 68.0, 3.0, 7.0, 8.0, 35.0, 7.0, 12.0, 10.0, 8.0, 5.0, 14.0, 3.0, 23.0, 5.0, 6.0, 5.0, 8.0, 17.0, 35.0, 34.0, 18.0, 6.0, 2.0, 34.0, 11.0, 5.0, 42.0, 22.0, 8.0, 9.0, 11.0, 5.0, 8.0, 5.0, 24.0, 99.0, 21.0, 1.0, 8.0, 8.0, 21.0, 3.0, 17.0, 13.0, 9.0, 4.0, 5.0, 34.0, 126.0, 96.0, 68.0, 3.0, 36.0, 10.0, 17.0, 9.0, 3.0, 12.0, 5.0, 6.0, 3.0, 6.0, 9.0, 40.0, 11.0, 1.0, 11.0, 3.0, 8.0, 9.0, 14.0, 20.0, 4.0, 7.0, 17.0, 65.0, 38.0, 5.0, 22.0, 18.0, 42.0, 5.0, 4.0, 20.0, 3.0, 11.0, 4.0, 62.0, 31.0, 11.0, 5.0, 17.0, 5.0, 12.0, 23.0, 16.0, 5.0, 11.0, 31.0, 19.0, 18.0, 31.0, 5.0, 6.0, 5.0, 49.0, 10.0, 2.0, 40.0, 62.0, 13.0, 10.0, 9.0, 16.0, 1.0, 2.0, 81.0, 6.0, 33.0, 27.0, 4.0, 21.0, 14.0, 2.0, 14.0, 38.0, 2.0, 5.0, 3.0, 22.0, 6.0, 2.0, 8.0, 17.0, 16.0, 11.0, 6.0, 9.0, 16.0, 4.0, 1.0, 18.0, 33.0, 49.0, 27.0, 6.0, 4.0, 18.0, 2.0, 5.0, 79.0, 9.0, 3.0, 22.0, 1.0, 20.0, 3.0, 12.0, 19.0, 3.0, 7.0, 26.0, 20.0, 11.0, 11.0, 103.0, 69.0, 7.0, 3.0, 25.0, 9.0, 11.0, 10.0, 3.0, 1.0, 9.0, 6.0, 2.0, 5.0, 2.0, 32.0, 4.0, 15.0, 7.0, 26.0, 5.0, 24.0, 20.0, 2.0, 8.0, 14.0, 14.0, 4.0, 11.0, 18.0, 39.0, 25.0, 13.0, 15.0, 12.0, 21.0, 33.0, 6.0, 26.0, 12.0, 17.0, 25.0, 23.0, 4.0, 27.0, 10.0, 5.0, 23.0, 32.0, 20.0, 22.0, 21.0, 62.0, 14.0, 18.0, 6.0]\n",
      "Predictions: [19330.635, 19330.688, 19330.705, 19330.824, 19330.803, 19330.537, 19330.865, 19330.703, 19330.74, 19330.674, 19330.781, 19330.842, 19330.57, 19330.705, 19330.783, 19330.93, 19330.557, 19330.795, 19331.04, 19330.646, 19330.521, 19330.725, 19330.729, 19330.691, 19330.662, 19330.635, 19330.738, 19330.707, 19330.8, 19330.605, 19330.719, 19330.674, 19330.768, 19330.662, 19330.764, 19330.73, 19330.693, 19330.693, 19330.709, 19330.82, 19330.693, 19330.627, 19330.96, 19330.658, 19330.727, 19330.67, 19325.887, 19330.793, 19330.729, 19330.629, 19330.807, 19330.96, 19330.676, 19330.912, 19330.635, 19330.734, 19330.63, 19330.686, 19330.695, 19330.594, 19330.771, 19330.736, 19330.807, 19330.742, 19330.654, 19330.654, 19330.807, 19330.713, 19330.584, 19330.578, 19330.86, 19330.564, 19330.797, 19330.748, 19330.785, 19330.676, 19330.71, 19330.895, 19330.861, 19330.64, 19330.688, 19330.795, 19330.734, 19330.734, 19330.752, 19330.662, 19330.635, 19330.652, 19330.57, 19330.562, 19330.791, 19330.969, 19330.71, 19330.877, 19325.799, 19330.605, 19330.836, 19330.88, 19330.945, 19330.773, 19330.658, 19330.764, 19330.58, 19330.94, 19330.477, 19330.703, 19330.742, 19330.56, 19330.654, 19330.768, 19330.715, 19330.729, 19330.736, 19330.887, 19331.158, 19330.783, 19330.521, 19330.744, 19330.78, 19330.521, 19330.646, 19330.727, 19330.688, 19330.887, 19330.928, 19330.646, 19325.783, 19330.744, 19330.629, 19330.604, 19330.58, 19330.846, 19330.709, 19330.732, 19330.896, 19330.7, 19330.781, 19330.73, 19330.805, 19330.842, 19330.68, 19325.904, 19330.572, 19330.545, 19330.545, 19330.69, 19330.754, 19330.701, 19330.686, 19330.754, 19330.627, 19330.662, 19330.764, 19330.71, 19330.623, 19330.623, 19330.732, 19330.732, 19330.764, 19330.65, 19330.826, 19330.803, 19330.752, 19330.932, 19330.78, 19330.756, 19330.662, 19330.812, 19330.627, 19330.752, 19330.762, 19330.682, 19330.79, 19330.63, 19330.61, 19330.793, 19330.648, 19330.56, 19330.84, 19330.697, 19330.717, 19330.613, 19330.455, 19330.748, 19330.74, 19330.795, 19330.662, 19330.639, 19330.678, 19330.717, 19330.652, 19330.752, 19330.652, 19330.867, 19330.611, 19330.678, 19330.713, 19330.717, 19330.752, 19330.666, 19330.78, 19330.787, 19330.768, 19330.764, 19330.715, 19330.766, 19330.719, 19330.701, 19330.959, 19330.705, 19330.57, 19330.535, 19330.566, 19330.584, 19330.693, 19330.791, 19330.766, 19330.828, 19330.818, 19330.766, 19330.752, 19330.646, 19330.646, 19330.65, 19330.783, 19330.744, 19330.688, 19330.69, 19330.666, 19330.77, 19330.523, 19330.645, 19330.713, 19330.959, 19330.871, 19330.67, 19330.654, 19330.67, 19330.736, 19330.775, 19330.736, 19330.85, 19330.764, 19330.74, 19331.096, 19330.75, 19330.908, 19330.92, 19330.771, 19330.83, 19330.932, 19330.76, 19330.697, 19330.627, 19330.732, 19330.785, 19330.785, 19330.533, 19330.658, 19330.686, 19330.678, 19330.678, 19330.752, 19330.715, 19330.672, 19330.672, 19330.697, 19330.871, 19330.709, 19330.877, 19330.883, 19330.826, 19330.846, 19330.572, 19330.877, 19330.898, 19330.697, 19330.705, 19330.854, 19330.752, 19330.537, 19330.549, 19330.611, 19330.625, 19330.682, 19330.78, 19330.572, 19330.713, 19330.621, 19330.69, 19330.654, 19330.879, 19331.03, 19330.83, 19330.863, 19330.527, 19330.709, 19330.674, 19330.781, 19330.686, 19330.686, 19330.74, 19330.775, 19330.615, 19330.59, 19330.662, 19330.748, 19330.766, 19330.863, 19330.777, 19330.855, 19330.59, 19330.824, 19330.598, 19330.701, 19330.756, 19330.77, 19330.734, 19330.664, 19330.783, 19330.861, 19330.678, 19330.768, 19330.78, 19330.633, 19330.797, 19325.947, 19330.734, 19330.76, 19330.752, 19330.688, 19330.865, 19330.799, 19330.775, 19330.57, 19330.812, 19330.713, 19330.764, 19330.842, 19330.59, 19330.59, 19330.734, 19330.697, 19330.648, 19330.828, 19330.572, 19330.676, 19330.72, 19330.715, 19330.71, 19330.723, 19330.709, 19330.78, 19330.795, 19330.709, 19330.678, 19330.621, 19330.697, 19330.678, 19330.736, 19330.812, 19330.7, 19330.732, 19330.701, 19330.688, 19330.838, 19330.762, 19330.564, 19330.736, 19330.701, 19330.658, 19330.838, 19330.686, 19330.729, 19330.807, 19330.607, 19330.691, 19330.56, 19330.664, 19330.83, 19330.88, 19330.643, 19325.807, 19330.58, 19330.58, 19330.58, 19330.602, 19330.693, 19330.572, 19330.578, 19330.576, 19330.713, 19330.713, 19330.63, 19330.63, 19330.615, 19330.662, 19330.973, 19330.955, 19330.701, 19330.771, 19330.785, 19330.969, 19330.725, 19330.654, 19330.877, 19330.701, 19330.775, 19330.64, 19330.662, 19330.797, 19326.092, 19330.941, 19330.678, 19330.467, 19330.633, 19330.66, 19330.613, 19330.766, 19330.725, 19330.701, 19330.697, 19330.656, 19330.629, 19330.654, 19330.521, 19330.857, 19330.646, 19330.668, 19330.588, 19330.625, 19330.727, 19330.744, 19330.715, 19330.748, 19330.752, 19330.63, 19330.854, 19330.69, 19330.72, 19330.8, 19330.752, 19330.943, 19330.828, 19330.826, 19330.727, 19330.656, 19330.674, 19330.744, 19330.787, 19330.73, 19330.754, 19330.748, 19330.752, 19330.893, 19330.73, 19330.604, 19330.734, 19330.678, 19330.61, 19330.678, 19330.729, 19330.734, 19330.648, 19330.775, 19330.688, 19330.688, 19330.654, 19330.752, 19330.639, 19330.682, 19330.605, 19330.695, 19330.838, 19330.725, 19330.627, 19330.848, 19330.783, 19330.748, 19330.562, 19330.768, 19330.768, 19330.662, 19330.885, 19330.53, 19330.83, 19330.645, 19330.723, 19330.834, 19330.656, 19330.572, 19330.818, 19330.748, 19330.607, 19330.877, 19330.83, 19330.744, 19330.746, 19330.793, 19330.635, 19330.783, 19330.576, 19330.65, 19330.672, 19330.502, 19330.842, 19330.682, 19330.666, 19330.725, 19330.682, 19330.707, 19330.533, 19330.672, 19330.45, 19330.676, 19330.63, 19330.66, 19330.59, 19330.734, 19330.785, 19330.834, 19330.658, 19330.725, 19330.74, 19330.725, 19330.656, 19330.742, 19330.748, 19330.764, 19330.752, 19330.72, 19330.893, 19330.564, 19330.72, 19330.652, 19330.652, 19330.693, 19330.717, 19330.832, 19330.611, 19330.611, 19330.797, 19330.725, 19330.742, 19330.848, 19330.686, 19330.775, 19330.912, 19330.81, 19330.771, 19330.785, 19330.78, 19330.787, 19330.639, 19330.746, 19330.758, 19330.709, 19330.908, 19330.674, 19330.62, 19330.822, 19330.729, 19330.824, 19330.662, 19330.668, 19330.666, 19330.729, 19330.607, 19330.521, 19330.805, 19330.623, 19330.88, 19330.791, 19330.639, 19330.62, 19330.82, 19330.816, 19330.688, 19330.652, 19330.652, 19330.6, 19330.82, 19330.82, 19330.705, 19330.754, 19330.643, 19330.85, 19330.74, 19330.736, 19330.762, 19330.668, 19330.662, 19330.695, 19330.768, 19330.557, 19330.705, 19330.746, 19330.79, 19330.746, 19330.885, 19330.826, 19330.72, 19330.861, 19330.771, 19330.639, 19330.791, 19330.684, 19330.824, 19330.76, 19330.646, 19330.703, 19330.703, 19330.729, 19325.803, 19330.654, 19330.652, 19330.525, 19330.83, 19330.7, 19330.717, 19331.045, 19330.889, 19330.75, 19330.686, 19330.912, 19330.889, 19330.623, 19330.729, 19330.656, 19331.05, 19330.57, 19330.611, 19330.664, 19330.588, 19330.768, 19330.635, 19330.814, 19330.926, 19330.807, 19330.61, 19330.648, 19330.611, 19330.6, 19330.75, 19330.762, 19330.623, 19330.754, 19330.701, 19330.76, 19330.73, 19330.857, 19330.611, 19330.627, 19330.787, 19330.707, 19330.768, 19330.682, 19330.682, 19330.807, 19330.578, 19330.666, 19330.896, 19330.742, 19330.682, 19330.697, 19330.662, 19330.557, 19330.658, 19330.564, 19330.67, 19330.523, 19330.725, 19330.705, 19330.773, 19330.816, 19330.816, 19330.803, 19330.611, 19330.674, 19330.611, 19330.691, 19330.828, 19331.072, 19330.78, 19330.674, 19330.627, 19330.65, 19330.635, 19330.75, 19330.6, 19330.635, 19330.824, 19330.584, 19330.676, 19330.816, 19330.803, 19330.81, 19330.693, 19330.676, 19330.732, 19330.58, 19330.637, 19330.74, 19330.654, 19330.703, 19330.85, 19330.775, 19330.637, 19330.854, 19330.719, 19330.643, 19330.838, 19330.666, 19331.12, 19330.705, 19330.666, 19330.71, 19330.703, 19330.67, 19330.822, 19330.729, 19330.559, 19330.805, 19330.715, 19330.67, 19330.781, 19330.613, 19330.72, 19330.795, 19330.648, 19330.709, 19330.625, 19330.713, 19330.688, 19330.729, 19330.592, 19330.72, 19330.873, 19325.562, 19330.73, 19330.957, 19330.732, 19330.883, 19330.664, 19330.666, 19330.74, 19330.742, 19330.756, 19330.889, 19330.875, 19330.783, 19330.65, 19330.643, 19330.732, 19330.705, 19330.799, 19330.783, 19330.877, 19330.963, 19330.658, 19330.914, 19330.766, 19330.74, 19330.666, 19330.773, 19330.666, 19330.709, 19330.752, 19330.768, 19330.775, 19330.78, 19330.725, 19330.744, 19330.621, 19330.7, 19330.707, 19331.12, 19331.064, 19330.668, 19330.701, 19330.6, 19330.814, 19330.666, 19330.678, 19330.908, 19330.787, 19330.713, 19330.842, 19330.764, 19330.791, 19330.793, 19330.732, 19330.727, 19330.709, 19330.826, 19330.975, 19330.64, 19330.674, 19330.654, 19330.668, 19330.7, 19330.787, 19330.82, 19330.684, 19330.746, 19330.52, 19330.75, 19330.8, 19330.713, 19325.785, 19330.79, 19330.824, 19330.834, 19330.7, 19330.541, 19330.82, 19330.686, 19330.654, 19330.713, 19330.686, 19330.715, 19330.715, 19330.725, 19330.7, 19330.596, 19330.686, 19330.613, 19330.734, 19330.805, 19330.709, 19330.873, 19330.822, 19330.842]\n",
      "Loss: 497308586.6666667\n",
      "R-squared Coefficient: -930598.2209\n",
      "16.820802377414562\n",
      "19.332568368241123\n",
      "True labels (unnormalized): [ 790.12354   635.463     152.14879   519.4676    190.81392   461.46988\n",
      "  287.47678  1176.7749    113.48364   751.45844   596.79785    94.15108\n",
      "  558.13275   654.7956     94.15108   210.14648   558.13275   113.48364\n",
      "  403.47217   210.14648   326.1419    635.463     152.14879    74.81851\n",
      "  190.81392   306.80933   171.48135    74.81851   210.14648   152.14879\n",
      "  384.13962   268.1442    210.14648   326.1419    152.14879   558.13275\n",
      "  113.48364   190.81392    74.81851   190.81392   113.48364   210.14648\n",
      "   94.15108   152.14879    74.81851   229.47906   268.1442    944.7841\n",
      "  442.13733   461.46988   268.1442    384.13962   171.48135   171.48135\n",
      "  384.13962   113.48364   248.81163   306.80933  1273.4377    287.47678\n",
      "  364.80704    55.48594   867.45386   442.13733    74.81851   306.80933\n",
      "  190.81392  1834.0823    171.48135  1602.0914    229.47906    94.15108\n",
      "  596.79785    94.15108   190.81392    74.81851   384.13962   326.1419\n",
      "  152.14879   442.13733  1041.447     152.14879   113.48364   538.8002\n",
      "  229.47906   152.14879   364.80704   287.47678   519.4676    384.13962\n",
      "  248.81163   248.81163   442.13733   152.14879   306.80933   693.4607\n",
      "  248.81163    36.153374   74.81851    74.81851   268.1442    345.4745\n",
      "  364.80704    74.81851   152.14879   132.81622   113.48364    55.48594\n",
      "  210.14648    94.15108   171.48135    94.15108   287.47678   442.13733\n",
      "  577.46533    74.81851   113.48364    55.48594   152.14879   461.46988\n",
      "  248.81163   790.12354   190.81392    74.81851   596.79785   384.13962\n",
      "  384.13962   152.14879   384.13962   152.14879  1312.1029   1234.7727\n",
      "  132.81622    36.153374  190.81392  1002.7818    442.13733   364.80704\n",
      "  442.13733    94.15108   248.81163   422.80475   152.14879    74.81851\n",
      "  248.81163   113.48364   364.80704   345.4745    248.81163   113.48364\n",
      "   94.15108    36.153374   94.15108   442.13733   113.48364   190.81392\n",
      "  326.1419    113.48364  1196.1075    500.13504   287.47678   248.81163\n",
      "  306.80933   306.80933   442.13733    94.15108    36.153374   94.15108\n",
      "  790.12354    94.15108   113.48364   113.48364   306.80933   480.80246\n",
      "  152.14879   770.791      74.81851    94.15108   751.45844   480.80246\n",
      "  210.14648   635.463    1002.7818    500.13504   210.14648  1215.4401\n",
      "  442.13733   403.47217   132.81622   229.47906   190.81392    55.48594\n",
      "  790.12354  1621.4241    809.4561    732.12585   461.46988   345.4745\n",
      "  171.48135  2085.4058    248.81163   248.81163  1099.4447    442.13733\n",
      "  132.81622  1022.1144    886.78644   210.14648   113.48364    74.81851\n",
      "  809.4561    751.45844   113.48364    94.15108   229.47906   364.80704\n",
      "   36.153374   94.15108   113.48364   152.14879    94.15108    55.48594\n",
      "  229.47906   384.13962    74.81851   210.14648   326.1419    306.80933\n",
      "  248.81163   190.81392   248.81163   326.1419    384.13962   152.14879\n",
      " 1466.7634    152.14879   983.4493    171.48135  2143.4033    422.80475\n",
      "  132.81622   113.48364   152.14879   519.4676    132.81622   596.79785\n",
      "  306.80933  1582.7589    422.80475   326.1419     94.15108    74.81851\n",
      "   94.15108  1602.0914    152.14879   113.48364   364.80704    74.81851\n",
      "   94.15108    94.15108    36.153374  171.48135    74.81851   132.81622\n",
      "   94.15108    36.153374  132.81622    36.153374   36.153374   55.48594\n",
      "  287.47678   190.81392   616.13043   306.80933   248.81163   171.48135\n",
      "  442.13733   422.80475    74.81851   944.7841     74.81851   384.13962\n",
      "  828.7887   1621.4241    132.81622   171.48135   326.1419    190.81392\n",
      "  113.48364    94.15108   210.14648   190.81392   519.4676    152.14879\n",
      "  403.47217   616.13043   944.7841    171.48135   519.4676     94.15108\n",
      "  538.8002    190.81392   674.1282    519.4676    306.80933   190.81392\n",
      "  442.13733   442.13733   345.4745   1428.0984    364.80704   190.81392\n",
      "   94.15108   828.7887     94.15108   287.47678   384.13962   577.46533\n",
      "  326.1419    171.48135   848.1213    132.81622   306.80933   326.1419\n",
      " 2124.0708    248.81163   654.7956    287.47678   616.13043   268.1442\n",
      "   36.153374  113.48364   500.13504   248.81163   132.81622  1660.0892\n",
      "   55.48594   345.4745    538.8002    152.14879   132.81622   190.81392\n",
      "   36.153374  500.13504   732.12585   171.48135   461.46988    55.48594\n",
      "  461.46988    74.81851   538.8002    809.4561    306.80933   403.47217\n",
      "  519.4676    577.46533    36.153374  306.80933    74.81851   519.4676\n",
      "   74.81851   616.13043   248.81163   326.1419   1524.7612    210.14648\n",
      "  886.78644    36.153374  132.81622   248.81163   132.81622   268.1442\n",
      "  287.47678    94.15108   442.13733   268.1442    190.81392   287.47678\n",
      "  384.13962    94.15108    94.15108  2143.4033     36.153374  152.14879\n",
      "  326.1419    113.48364    94.15108    94.15108    94.15108   770.791\n",
      "   94.15108   306.80933   113.48364    74.81851   132.81622   152.14879\n",
      "  248.81163   171.48135    55.48594    74.81851  1331.4355     74.81851\n",
      "  210.14648   229.47906   190.81392   171.48135   732.12585   944.7841\n",
      "   55.48594   268.1442    345.4745    944.7841   1660.0892     36.153374\n",
      "  113.48364   171.48135   268.1442     36.153374  925.45154   326.1419\n",
      "  113.48364   442.13733    94.15108   906.11896    94.15108   210.14648\n",
      "   94.15108   384.13962   113.48364   152.14879   268.1442    113.48364\n",
      " 2259.399    1969.4103    538.8002    132.81622    94.15108   248.81163\n",
      "   55.48594    55.48594   210.14648   190.81392   171.48135   190.81392\n",
      "  190.81392   229.47906   171.48135    94.15108   964.1167    364.80704\n",
      "  364.80704   132.81622    36.153374  616.13043    36.153374  171.48135\n",
      "  577.46533   384.13962   113.48364   268.1442   1640.7566    558.13275\n",
      "  674.1282    500.13504   403.47217    55.48594    74.81851   268.1442\n",
      "   55.48594   461.46988  1350.7681     74.81851    74.81851   306.80933\n",
      "  268.1442    364.80704   210.14648   171.48135   152.14879    74.81851\n",
      " 1389.4332    326.1419    364.80704   674.1282    171.48135    55.48594\n",
      "  384.13962   210.14648   113.48364   403.47217   132.81622    94.15108\n",
      "  171.48135   248.81163  2452.7246    306.80933   867.45386   171.48135\n",
      "  500.13504    36.153374  113.48364   635.463     113.48364    36.153374\n",
      "  152.14879   403.47217    36.153374  422.80475   326.1419     94.15108\n",
      "  983.4493   1698.7543    886.78644    94.15108  2530.0547    229.47906\n",
      " 1022.1144    442.13733   712.7933    171.48135   171.48135   287.47678\n",
      "  268.1442     94.15108   210.14648   210.14648   113.48364   229.47906\n",
      " 1273.4377    403.47217   345.4745    306.80933   171.48135   693.4607\n",
      "  152.14879   442.13733   132.81622   616.13043    55.48594    94.15108\n",
      "  403.47217   171.48135   152.14879   132.81622   171.48135   422.80475\n",
      "  268.1442    229.47906   422.80475   171.48135   287.47678    94.15108\n",
      "  480.80246   345.4745    132.81622   461.46988   848.1213    442.13733\n",
      "  403.47217    74.81851   403.47217   326.1419   1060.7795   1041.447\n",
      "  480.80246   113.48364   268.1442   1254.1052    596.79785   384.13962\n",
      " 1350.7681     55.48594   268.1442    364.80704    94.15108  1370.1006\n",
      "   55.48594   442.13733   113.48364   654.7956    132.81622   422.80475\n",
      "   55.48594   229.47906   152.14879   113.48364    74.81851    94.15108\n",
      "  152.14879   751.45844   422.80475   983.4493    577.46533   616.13043\n",
      "  190.81392   132.81622   152.14879  1234.7727     36.153374   36.153374\n",
      "  345.4745     94.15108   132.81622   248.81163   268.1442    229.47906\n",
      "   55.48594   538.8002    596.79785   345.4745    422.80475   674.1282\n",
      " 1331.4355     74.81851   152.14879   171.48135   693.4607    152.14879\n",
      "  248.81163   210.14648   171.48135   113.48364   287.47678    74.81851\n",
      "  461.46988   113.48364   132.81622   113.48364   171.48135   345.4745\n",
      "  693.4607    674.1282    364.80704   132.81622    55.48594   674.1282\n",
      "  229.47906   113.48364   828.7887    442.13733   171.48135   190.81392\n",
      "  229.47906   113.48364   171.48135   113.48364   480.80246  1930.7451\n",
      "  422.80475    36.153374  171.48135   171.48135   422.80475    74.81851\n",
      "  345.4745    268.1442    190.81392    94.15108   113.48364   674.1282\n",
      " 2452.7246   1872.7474   1331.4355     74.81851   712.7933    210.14648\n",
      "  345.4745    190.81392    74.81851   248.81163   113.48364   132.81622\n",
      "   74.81851   132.81622   190.81392   790.12354   229.47906    36.153374\n",
      "  229.47906    74.81851   171.48135   190.81392   287.47678   403.47217\n",
      "   94.15108   152.14879   345.4745   1273.4377    751.45844   113.48364\n",
      "  442.13733   364.80704   828.7887    113.48364    94.15108   403.47217\n",
      "   74.81851   229.47906    94.15108  1215.4401    616.13043   229.47906\n",
      "  113.48364   345.4745    113.48364   248.81163   461.46988   326.1419\n",
      "  113.48364   229.47906   616.13043   384.13962   364.80704   616.13043\n",
      "  113.48364   132.81622   113.48364   964.1167    210.14648    55.48594\n",
      "  790.12354  1215.4401    268.1442    210.14648   190.81392   326.1419\n",
      "   36.153374   55.48594  1582.7589    132.81622   654.7956    538.8002\n",
      "   94.15108   422.80475   287.47678    55.48594   287.47678   751.45844\n",
      "   55.48594   113.48364    74.81851   442.13733   132.81622    55.48594\n",
      "  171.48135   345.4745    326.1419    229.47906   132.81622   190.81392\n",
      "  326.1419     94.15108    36.153374  364.80704   654.7956    964.1167\n",
      "  538.8002    132.81622    94.15108   364.80704    55.48594   113.48364\n",
      " 1544.0938    190.81392    74.81851   442.13733    36.153374  403.47217\n",
      "   74.81851   248.81163   384.13962    74.81851   152.14879   519.4676\n",
      "  403.47217   229.47906   229.47906  2008.0754   1350.7681    152.14879\n",
      "   74.81851   500.13504   190.81392   229.47906   210.14648    74.81851\n",
      "   36.153374  190.81392   132.81622    55.48594   113.48364    55.48594\n",
      "  635.463      94.15108   306.80933   152.14879   519.4676    113.48364\n",
      "  480.80246   403.47217    55.48594   171.48135   287.47678   287.47678\n",
      "   94.15108   229.47906   364.80704   770.791     500.13504   268.1442\n",
      "  306.80933   248.81163   422.80475   654.7956    132.81622   519.4676\n",
      "  248.81163   345.4745    500.13504   461.46988    94.15108   538.8002\n",
      "  210.14648   113.48364   461.46988   635.463     403.47217   442.13733\n",
      "  422.80475  1215.4401    287.47678   364.80704   132.81622 ]\n",
      "Predictions (unnormalized): [373727.66 373728.66 373729.   373731.3  373730.9  373725.75 373732.1\n",
      " 373728.97 373729.7  373728.4  373730.47 373731.66 373726.4  373729.\n",
      " 373730.5  373733.34 373726.12 373730.75 373735.47 373727.88 373725.47\n",
      " 373729.38 373729.47 373728.75 373728.2  373727.66 373729.66 373729.03\n",
      " 373730.84 373727.1  373729.28 373728.4  373730.22 373728.2  373730.12\n",
      " 373729.5  373728.78 373728.78 373729.1  373731.22 373728.78 373727.5\n",
      " 373733.94 373728.1  373729.4  373728.3  373635.84 373730.72 373729.47\n",
      " 373727.53 373730.97 373733.94 373728.44 373733.   373727.66 373729.56\n",
      " 373727.56 373728.62 373728.8  373726.84 373730.28 373729.6  373730.97\n",
      " 373729.72 373728.03 373728.03 373730.97 373729.16 373726.66 373726.56\n",
      " 373732.   373726.28 373730.78 373729.84 373730.56 373728.44 373729.12\n",
      " 373732.66 373732.03 373727.75 373728.66 373730.75 373729.56 373729.56\n",
      " 373729.9  373728.2  373727.66 373728.   373726.4  373726.25 373730.66\n",
      " 373734.1  373729.12 373732.3  373634.16 373727.1  373731.53 373732.4\n",
      " 373733.66 373730.3  373728.1  373730.12 373726.6  373733.53 373724.6\n",
      " 373728.97 373729.72 373726.22 373728.03 373730.22 373729.2  373729.47\n",
      " 373729.6  373732.53 373737.75 373730.5  373725.47 373729.75 373730.44\n",
      " 373725.47 373727.88 373729.4  373728.66 373732.53 373733.3  373727.88\n",
      " 373633.84 373729.75 373727.53 373727.03 373726.6  373731.72 373729.1\n",
      " 373729.53 373732.72 373728.9  373730.47 373729.5  373730.94 373731.66\n",
      " 373728.5  373636.2  373726.44 373725.9  373725.9  373728.7  373729.94\n",
      " 373728.94 373728.62 373729.94 373727.5  373728.2  373730.12 373729.12\n",
      " 373727.4  373727.4  373729.53 373729.53 373730.12 373727.94 373731.34\n",
      " 373730.9  373729.9  373733.38 373730.44 373730.   373728.2  373731.1\n",
      " 373727.5  373729.9  373730.1  373728.56 373730.62 373727.56 373727.16\n",
      " 373730.72 373727.9  373726.22 373731.62 373728.84 373729.22 373727.22\n",
      " 373724.16 373729.84 373729.7  373730.75 373728.2  373727.72 373728.47\n",
      " 373729.22 373728.   373729.9  373728.   373732.12 373727.2  373728.47\n",
      " 373729.16 373729.22 373729.9  373728.25 373730.44 373730.6  373730.22\n",
      " 373730.12 373729.2  373730.2  373729.28 373728.94 373733.9  373729.\n",
      " 373726.4  373725.72 373726.3  373726.66 373728.78 373730.66 373730.2\n",
      " 373731.38 373731.2  373730.2  373729.9  373727.88 373727.88 373727.94\n",
      " 373730.5  373729.75 373728.66 373728.7  373728.25 373730.25 373725.5\n",
      " 373727.84 373729.16 373733.9  373732.22 373728.3  373728.03 373728.3\n",
      " 373729.6  373730.38 373729.6  373731.8  373730.12 373729.7  373736.56\n",
      " 373729.88 373732.94 373733.16 373730.28 373731.4  373733.38 373730.06\n",
      " 373728.84 373727.5  373729.53 373730.56 373730.56 373725.7  373728.1\n",
      " 373728.62 373728.47 373728.47 373729.9  373729.2  373728.38 373728.38\n",
      " 373728.84 373732.22 373729.1  373732.3  373732.44 373731.34 373731.72\n",
      " 373726.44 373732.3  373732.75 373728.84 373729.   373731.88 373729.9\n",
      " 373725.75 373725.97 373727.2  373727.47 373728.56 373730.44 373726.44\n",
      " 373729.16 373727.38 373728.7  373728.03 373732.38 373735.28 373731.4\n",
      " 373732.06 373725.56 373729.1  373728.4  373730.47 373728.62 373728.62\n",
      " 373729.7  373730.38 373727.28 373726.78 373728.2  373729.84 373730.2\n",
      " 373732.06 373730.4  373731.9  373726.78 373731.3  373726.94 373728.94\n",
      " 373730.   373730.25 373729.56 373728.22 373730.5  373732.03 373728.47\n",
      " 373730.22 373730.44 373727.6  373730.78 373637.03 373729.56 373730.06\n",
      " 373729.9  373728.66 373732.1  373730.8  373730.38 373726.4  373731.1\n",
      " 373729.16 373730.12 373731.66 373726.78 373726.78 373729.56 373728.84\n",
      " 373727.9  373731.38 373726.44 373728.44 373729.3  373729.2  373729.12\n",
      " 373729.34 373729.1  373730.44 373730.75 373729.1  373728.47 373727.38\n",
      " 373728.84 373728.47 373729.6  373731.1  373728.9  373729.53 373728.94\n",
      " 373728.66 373731.56 373730.1  373726.28 373729.6  373728.94 373728.1\n",
      " 373731.56 373728.62 373729.47 373730.97 373727.12 373728.75 373726.22\n",
      " 373728.22 373731.4  373732.4  373727.78 373634.3  373726.6  373726.6\n",
      " 373726.6  373727.   373728.78 373726.44 373726.56 373726.5  373729.16\n",
      " 373729.16 373727.56 373727.56 373727.28 373728.2  373734.2  373733.84\n",
      " 373728.94 373730.28 373730.56 373734.1  373729.38 373728.03 373732.3\n",
      " 373728.94 373730.38 373727.75 373728.2  373730.78 373639.8  373733.56\n",
      " 373728.47 373724.4  373727.6  373728.12 373727.22 373730.2  373729.38\n",
      " 373728.94 373728.84 373728.06 373727.53 373728.03 373725.47 373731.94\n",
      " 373727.88 373728.28 373726.75 373727.47 373729.4  373729.75 373729.2\n",
      " 373729.84 373729.9  373727.56 373731.88 373728.7  373729.3  373730.84\n",
      " 373729.9  373733.62 373731.38 373731.34 373729.4  373728.06 373728.4\n",
      " 373729.75 373730.6  373729.5  373729.94 373729.84 373729.9  373732.62\n",
      " 373729.5  373727.03 373729.56 373728.47 373727.16 373728.47 373729.47\n",
      " 373729.56 373727.9  373730.38 373728.66 373728.66 373728.03 373729.9\n",
      " 373727.72 373728.56 373727.1  373728.8  373731.56 373729.38 373727.5\n",
      " 373731.75 373730.5  373729.84 373726.25 373730.22 373730.22 373728.2\n",
      " 373732.47 373725.6  373731.4  373727.84 373729.34 373731.5  373728.06\n",
      " 373726.44 373731.2  373729.84 373727.12 373732.3  373731.4  373729.75\n",
      " 373729.8  373730.72 373727.66 373730.5  373726.5  373727.94 373728.38\n",
      " 373725.06 373731.66 373728.56 373728.25 373729.38 373728.56 373729.03\n",
      " 373725.7  373728.38 373724.06 373728.44 373727.56 373728.12 373726.78\n",
      " 373729.56 373730.56 373731.5  373728.1  373729.38 373729.7  373729.38\n",
      " 373728.06 373729.72 373729.84 373730.12 373729.9  373729.3  373732.62\n",
      " 373726.28 373729.3  373728.   373728.   373728.78 373729.22 373731.47\n",
      " 373727.2  373727.2  373730.78 373729.38 373729.72 373731.75 373728.62\n",
      " 373730.38 373733.   373731.03 373730.28 373730.56 373730.44 373730.6\n",
      " 373727.72 373729.8  373730.03 373729.1  373732.94 373728.4  373727.34\n",
      " 373731.28 373729.47 373731.3  373728.2  373728.28 373728.25 373729.47\n",
      " 373727.12 373725.47 373730.94 373727.4  373732.4  373730.66 373727.72\n",
      " 373727.34 373731.22 373731.16 373728.66 373728.   373728.   373726.97\n",
      " 373731.22 373731.22 373729.   373729.94 373727.78 373731.8  373729.7\n",
      " 373729.6  373730.1  373728.28 373728.2  373728.8  373730.22 373726.12\n",
      " 373729.   373729.8  373730.62 373729.8  373732.47 373731.34 373729.3\n",
      " 373732.03 373730.28 373727.72 373730.66 373728.6  373731.3  373730.06\n",
      " 373727.88 373728.97 373728.97 373729.47 373634.22 373728.03 373728.\n",
      " 373725.53 373731.4  373728.9  373729.22 373735.56 373732.56 373729.88\n",
      " 373728.62 373733.   373732.56 373727.4  373729.47 373728.06 373735.7\n",
      " 373726.4  373727.2  373728.22 373726.75 373730.22 373727.66 373731.12\n",
      " 373733.28 373730.97 373727.16 373727.9  373727.2  373726.97 373729.88\n",
      " 373730.1  373727.4  373729.94 373728.94 373730.06 373729.5  373731.94\n",
      " 373727.2  373727.5  373730.6  373729.03 373730.22 373728.56 373728.56\n",
      " 373730.97 373726.56 373728.25 373732.72 373729.72 373728.56 373728.84\n",
      " 373728.2  373726.12 373728.1  373726.28 373728.3  373725.5  373729.38\n",
      " 373729.   373730.3  373731.16 373731.16 373730.9  373727.2  373728.4\n",
      " 373727.2  373728.75 373731.38 373736.1  373730.44 373728.4  373727.5\n",
      " 373727.94 373727.66 373729.88 373726.97 373727.66 373731.3  373726.66\n",
      " 373728.44 373731.16 373730.9  373731.03 373728.78 373728.44 373729.53\n",
      " 373726.6  373727.7  373729.7  373728.03 373728.97 373731.8  373730.38\n",
      " 373727.7  373731.88 373729.28 373727.78 373731.56 373728.25 373737.\n",
      " 373729.   373728.25 373729.12 373728.97 373728.3  373731.28 373729.47\n",
      " 373726.2  373730.94 373729.2  373728.3  373730.47 373727.22 373729.3\n",
      " 373730.75 373727.9  373729.1  373727.47 373729.16 373728.66 373729.47\n",
      " 373726.8  373729.3  373732.25 373629.6  373729.5  373733.88 373729.53\n",
      " 373732.44 373728.22 373728.25 373729.7  373729.72 373730.   373732.56\n",
      " 373732.28 373730.5  373727.94 373727.78 373729.53 373729.   373730.8\n",
      " 373730.5  373732.3  373734.   373728.1  373733.03 373730.2  373729.7\n",
      " 373728.25 373730.3  373728.25 373729.1  373729.9  373730.22 373730.38\n",
      " 373730.44 373729.38 373729.75 373727.38 373728.9  373729.03 373737.\n",
      " 373735.94 373728.28 373728.94 373726.97 373731.12 373728.25 373728.47\n",
      " 373732.94 373730.6  373729.16 373731.66 373730.12 373730.66 373730.72\n",
      " 373729.53 373729.4  373729.1  373731.34 373734.22 373727.75 373728.4\n",
      " 373728.03 373728.28 373728.9  373730.6  373731.22 373728.6  373729.8\n",
      " 373725.4  373729.88 373730.84 373729.16 373633.88 373730.62 373731.3\n",
      " 373731.5  373728.9  373725.84 373731.22 373728.62 373728.03 373729.16\n",
      " 373728.62 373729.2  373729.2  373729.38 373728.9  373726.88 373728.62\n",
      " 373727.22 373729.56 373730.94 373729.1  373732.25 373731.28 373731.66]\n",
      "Loss: 497308586.6666667\n",
      "R-squared Coefficient: -930598.2209\n",
      "               player_id   opposing_player_id    season        y_pred  \\\n",
      "0           BRETT.HOWDEN      GABRIEL.VILARDI  20222023  19330.634766   \n",
      "1    CHANDLER.STEPHENSON      PHILLIP.DANAULT  20222023  19330.687500   \n",
      "2       WILLIAM.KARLSSON      QUINTON.BYFIELD  20222023  19330.705078   \n",
      "3            NICOLAS.ROY      GABRIEL.VILARDI  20222023  19330.824219   \n",
      "4            JACK.EICHEL         ANZE.KOPITAR  20222023  19330.802734   \n",
      "..                   ...                  ...       ...           ...   \n",
      "828       MARK.SCHEIFELE       CONNOR.MCDAVID  20222023  19330.804688   \n",
      "829           ADAM.LOWRY       CONNOR.MCDAVID  20222023  19330.708984   \n",
      "830       MARK.SCHEIFELE  RYAN.NUGENT-HOPKINS  20222023  19330.873047   \n",
      "831           ADAM.LOWRY  RYAN.NUGENT-HOPKINS  20222023  19330.822266   \n",
      "832     DAVID.GUSTAFSSON           DEREK.RYAN  20222023  19330.841797   \n",
      "\n",
      "     y_pred_unnorm  y_true  y_true_unnorm  \n",
      "0     373727.65625    40.0     790.123535  \n",
      "1     373728.65625    32.0     635.463013  \n",
      "2     373729.00000     7.0     152.148788  \n",
      "3     373731.31250    26.0     519.467590  \n",
      "4     373730.90625     9.0     190.813919  \n",
      "..             ...     ...            ...  \n",
      "828   373730.93750    21.0     422.804749  \n",
      "829   373729.09375    62.0    1215.440063  \n",
      "830   373732.25000    14.0     287.476776  \n",
      "831   373731.28125    18.0     364.807037  \n",
      "832   373731.65625     6.0     132.816223  \n",
      "\n",
      "[833 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"/Users/Tad/Documents/faceoffs\"\n",
    "data_new = pd.read_csv(\"final_dataset_2022-2023_imputed.csv\")\n",
    "\n",
    "print(data_new.shape)\n",
    "\n",
    "subset_data = data_new.iloc[:, include_col_indices].fillna(0)\n",
    "df_subset = pd.DataFrame(subset_data)\n",
    "\n",
    "player_ids_projections = data_new['event_player_1']\n",
    "opposing_player_ids_projections = data_new['event_player_2']\n",
    "seasons_projections = data_new['season']\n",
    "\n",
    "data_new = df_subset.select_dtypes(include=['number'])\n",
    "\n",
    "print(data_new.shape)\n",
    "data_new_scaled = pd.DataFrame(scaler_fit.transform(data_new.values), columns = data_new.columns)\n",
    "\n",
    "y = data_new['FA_zone_time']\n",
    "x = data_new.loc[:, data_new.columns != 'FA_zone_time']\n",
    "x = x.loc[:, x.columns != 'FA_zone_time']\n",
    "\n",
    "projections_df = pd.concat([x, y], axis = 1)\n",
    "projections_dataset = CustomDataset(projections_df)\n",
    "projections_loader = DataLoader(projections_dataset, batch_size=test_batch_size, shuffle=False)\n",
    "\n",
    "test_loss = 0\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch, (X, y) in enumerate(projections_loader):\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "        #print(\"\" + str(X) + \"\" + str(y))\n",
    "        prediction = model(X).to(device)\n",
    "        # Remove extra dimensions from y\n",
    "        y = y.unsqueeze(1).squeeze()\n",
    "        loss = loss_func(prediction.squeeze(), y)\n",
    "        test_loss += loss.float().item()\n",
    "        y_true.extend(y.cpu().numpy())\n",
    "        y_pred.extend(prediction.cpu().numpy().squeeze())\n",
    "\n",
    "test_loss /= len(test_loader)\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "r2 = r2_score(y_true, y_pred)\n",
    "print(\"True labels:\", y_true)\n",
    "print(\"Predictions:\", y_pred)\n",
    "print(\"Loss:\", test_loss)\n",
    "print(f\"R-squared Coefficient: {r2:.4f}\")\n",
    "\n",
    "y_pred = np.array(y_pred)\n",
    "y_true = np.array(y_true)\n",
    "\n",
    "mean = data_unnorm['FA_zone_time'].mean()\n",
    "sd = data_unnorm['FA_zone_time'].std()\n",
    "print(mean)\n",
    "print(sd)\n",
    "\n",
    "# Calculate the unnormalized predictions and true labels\n",
    "y_pred_unnorm = mean + sd * y_pred\n",
    "y_true_unnorm = mean + sd * y_true\n",
    "\n",
    "print(\"True labels (unnormalized):\", y_true_unnorm)\n",
    "print(\"Predictions (unnormalized):\", y_pred_unnorm)\n",
    "print(\"Loss:\", test_loss)\n",
    "print(f\"R-squared Coefficient: {r2:.4f}\")\n",
    "\n",
    "new_df = {\n",
    "    'player_id': player_ids_projections,\n",
    "    'opposing_player_id': opposing_player_ids_projections,\n",
    "    'season': seasons_projections,\n",
    "    'y_pred': y_pred,\n",
    "    'y_pred_unnorm': y_pred_unnorm,\n",
    "    'y_true': y_true,\n",
    "    'y_true_unnorm': y_true_unnorm\n",
    "}\n",
    "\n",
    "# Create a DataFrame from the dictionary\n",
    "projections_summary_df = pd.DataFrame(new_df)\n",
    "\n",
    "# Print the summary DataFrame\n",
    "print(projections_summary_df)\n",
    "projections_summary_df.to_csv(\"neural_net_faceoff_projections.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
