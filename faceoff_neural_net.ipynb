{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as functional\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from skimage import io\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaceoffsDataset(Dataset):\n",
    "\n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.landmarks_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        data = self.data.iloc[idx, 1:]\n",
    "        data = np.array([data])\n",
    "        data = data.reshape(-1, 2)\n",
    "\n",
    "        if self.transform:\n",
    "            data = self.transform(data)\n",
    "\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"/Users/Tad/Documents/faceoffs\"\n",
    "data_initial = pd.read_csv(\"training_data_all_offensive_offensive.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    game_id_x  season_x  game_seconds  event_index  game_period  coords_x  \\\n",
      "0   -1.798257 -1.797913     -0.715929    -0.712210    -0.606279 -0.531820   \n",
      "1   -1.798257 -1.797913     -0.697547    -0.677103    -0.606279 -0.531820   \n",
      "2   -1.798257 -1.797913      0.039564     0.060154    -0.606279 -0.531820   \n",
      "3   -1.798257 -1.797913      0.210515     0.255753    -0.606279  1.880391   \n",
      "4   -1.798257 -1.797913      0.445803     0.471414     0.764344 -0.531820   \n",
      "..        ...       ...           ...          ...          ...       ...   \n",
      "95  -1.798252 -1.797913     -0.357483    -0.326028    -0.606279  1.880391   \n",
      "96  -1.798252 -1.797913      0.458670     0.476429     0.764344  1.880391   \n",
      "97  -1.798252 -1.797913      0.554256     0.556675     0.764344  1.880391   \n",
      "98  -1.798252 -1.797913      0.604806     0.596797     0.764344 -0.531820   \n",
      "99  -1.798252 -1.797913      0.816197     0.772335     0.764344  1.880391   \n",
      "\n",
      "    coords_y  home_score  away_score  event_distance  ...  \\\n",
      "0  -0.532003   -0.525466   -0.518676        -0.00558  ...   \n",
      "1   1.879738   -0.525466   -0.518676        -0.00558  ...   \n",
      "2   1.879738   -0.525466   -0.518676        -0.00558  ...   \n",
      "3   1.879738   -0.525466    0.371798        -0.00558  ...   \n",
      "4  -0.532003   -0.525466    2.152746        -0.00558  ...   \n",
      "..       ...         ...         ...             ...  ...   \n",
      "95 -0.532003    0.294989   -0.518676        -0.00558  ...   \n",
      "96 -0.532003    0.294989    0.371798        -0.00558  ...   \n",
      "97  1.879738    0.294989    0.371798        -0.00558  ...   \n",
      "98 -0.532003    0.294989    0.371798        -0.00558  ...   \n",
      "99  1.879738    1.115444    0.371798        -0.00558  ...   \n",
      "\n",
      "    High.Danger_Win_All  High.Danger_Win_F  High.Danger_Win_D  \\\n",
      "0             -0.019301           0.002882          -0.218090   \n",
      "1              0.262404           0.296602          -0.218090   \n",
      "2             -1.065635          -1.088080          -0.218090   \n",
      "3              0.423378           0.464442          -0.218090   \n",
      "4             -0.019301           0.002882          -0.218090   \n",
      "..                  ...                ...                ...   \n",
      "95            -1.186366          -1.130040          -1.013622   \n",
      "96            -0.824173          -0.752399          -1.013622   \n",
      "97             0.222160           0.170722           0.577443   \n",
      "98            -1.105878          -1.130040          -0.218090   \n",
      "99             0.866058           0.842083           0.577443   \n",
      "\n",
      "    One-timer_Win_All  One-timer_Win_F  One-timer_Win_D  Deflection_Win_All  \\\n",
      "0           -0.412478        -0.377287        -0.345184            0.250386   \n",
      "1            0.477429         0.583752        -0.215521           -0.974937   \n",
      "2           -1.135528        -1.183320        -0.345184           -0.484808   \n",
      "3            0.171524         0.490748        -1.252826            0.863047   \n",
      "4           -0.162192         0.118733        -1.252826            0.495450   \n",
      "..                ...              ...              ...                 ...   \n",
      "95          -1.385814        -1.276323        -1.123163           -1.832663   \n",
      "96          -1.079909        -0.904308        -1.252826           -2.200260   \n",
      "97           0.088095         0.056730         0.173469           -0.239743   \n",
      "98          -1.413624        -1.276323        -1.252826           -2.077728   \n",
      "99           0.171524         0.149734         0.173469            0.985579   \n",
      "\n",
      "    Deflection_Win_F  Deflection_Win_D    net_xg  \n",
      "0          -0.024390          0.505048  0.089291  \n",
      "1          -0.379765         -1.347750  0.073043  \n",
      "2          -1.090514          0.505048  0.112887  \n",
      "3           1.041734          0.273448  0.051333  \n",
      "4           0.508672          0.273448  0.073019  \n",
      "..               ...               ...       ...  \n",
      "95         -1.090514         -2.042549  0.015949  \n",
      "96         -1.268202         -2.505748  0.020040  \n",
      "97         -0.379765          0.041848  0.022755  \n",
      "98         -1.090514         -2.505748  0.024117  \n",
      "99          1.397108          0.041848  0.082255  \n",
      "\n",
      "[100 rows x 530 columns]\n",
      "torch.Size([60387, 529])\n"
     ]
    }
   ],
   "source": [
    "data_no_na = data_initial.dropna() # data should already have no NAs due to numerical imputation\n",
    "data = data_no_na.select_dtypes(['number'])\n",
    "print(data.head(100))\n",
    "x = data.loc[:, data.columns != 'net_xg']\n",
    "x = x.loc[:, x.columns != 'net_xg']\n",
    "y = data['net_xg']\n",
    "\n",
    "X_train, X_intermediate, y_train, y_intermediate = train_test_split(x, y, train_size = 0.65, test_size=0.35, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_intermediate, y_intermediate, test_size=0.571, random_state=42)\n",
    "# Above two lines in unison accomplish a 65-15-20 split for train-test-validation\n",
    "\n",
    "X_train = torch.from_numpy(X_train.to_numpy())\n",
    "X_val = torch.from_numpy(X_val.to_numpy())\n",
    "X_test = torch.from_numpy(X_test.to_numpy())\n",
    "\n",
    "y_train = torch.from_numpy(y_train.to_numpy())\n",
    "y_val = torch.from_numpy(y_val.to_numpy())\n",
    "y_test = torch.from_numpy(y_test.to_numpy())\n",
    "\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VanillaNeuralNet(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(VanillaNeuralNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(1500, 1000)\n",
    "        self.fc2 = nn.Linear(1000, 750)\n",
    "        self.fc3 = nn.Linear(750, 500)\n",
    "        self.fc4 = nn.Linear(500, 500)\n",
    "        self.fc5 = nn.Linear(500, 500)\n",
    "        self.fc6 = nn.Linear(500, 500)\n",
    "        self.fc7 = nn.Linear(500, 250)\n",
    "        self.fc8 = nn.Linear(250, 100)\n",
    "        self.fc9 = nn.Linear(100, 50)\n",
    "        self.fc10 = nn.Linear(50, 1) # output singular prediction\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x_1 = self.fc1(torch.nn.functional.relu(x))\n",
    "        x_2 = self.fc2(torch.nn.functional.relu(x_1))\n",
    "        x_3 = self.fc3(torch.nn.functional.relu(x_2))\n",
    "        x_4 = self.fc4(torch.nn.functional.relu(x_3))\n",
    "        x_5 = self.fc5(torch.nn.functional.relu(x_4))\n",
    "        x_6 = self.fc6(torch.nn.functional.relu(x_5))\n",
    "        x_7 = self.fc7(torch.nn.functional.relu(x_6))\n",
    "        x_8 = self.fc8(torch.nn.functional.relu(x_7))\n",
    "        x_9 = self.fc9(torch.nn.functional.relu(x_8))\n",
    "        x_10 = self.fc10(torch.nn.functional.relu(x_9))\n",
    "        return x_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CreateDataset(Dataset):\n",
    "\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        sample = {'feature': torch.tensor(self.x[index], dtype=torch.float32), \n",
    "                    'label': torch.tensor(self.y[index], dtype=torch.float32)}\n",
    "\n",
    "        return sample\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60387, 529])\n",
      "torch.Size([60387])\n",
      "<__main__.CreateDataset object at 0x000001D581BDAF50>\n"
     ]
    }
   ],
   "source": [
    "training_dataset = CreateDataset(X_train, y_train)\n",
    "validation_dataset = CreateDataset(X_val, y_val)\n",
    "print(training_dataset.x.shape)\n",
    "print(training_dataset.y.shape)\n",
    "print(validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training...\n",
      "\n",
      "EPOCH 1 of 200\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tad\\AppData\\Local\\Temp\\ipykernel_13644\\4281265504.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  sample = {'feature': torch.tensor(self.x[index], dtype=torch.float32),\n",
      "C:\\Users\\Tad\\AppData\\Local\\Temp\\ipykernel_13644\\4281265504.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'label': torch.tensor(self.y[index], dtype=torch.float32)}\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'to'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[81], line 31\u001b[0m\n\u001b[0;32m     29\u001b[0m count \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m     30\u001b[0m \u001b[39mfor\u001b[39;00m batch, (X, y) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_dataloader):\n\u001b[1;32m---> 31\u001b[0m   X \u001b[39m=\u001b[39m X\u001b[39m.\u001b[39;49mto(device)\n\u001b[0;32m     32\u001b[0m   y \u001b[39m=\u001b[39m y\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     33\u001b[0m   prediction \u001b[39m=\u001b[39m model(X)\u001b[39m.\u001b[39mto(device)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'to'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "model = VanillaNeuralNet().to(device)\n",
    "learning_rate = 0.01\n",
    "train_batch_size = 300\n",
    "validation_batch_size = 300\n",
    "train_dataloader = DataLoader(training_dataset, batch_size=train_batch_size, shuffle=True)\n",
    "validation_dataloader = DataLoader(validation_dataset, batch_size=validation_batch_size, shuffle=False)\n",
    "## Initialize Optimizer and Learning Rate Scheduler\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "loss_func = nn.MSELoss()\n",
    "print(\"Start Training...\")\n",
    "num_epochs = 200\n",
    "training_losses = np.array([])\n",
    "validation_losses = np.array([])\n",
    "for epoch in range(num_epochs):\n",
    "    ########################### Training #####################################\n",
    "    model.train(True)\n",
    "    print(\"\\nEPOCH \" +str(epoch+1)+\" of \"+str(num_epochs)+\"\\n\")\n",
    "    # TODO: Design your own training section\n",
    "    train_loss = 0\n",
    "    count = 0\n",
    "    for batch, (X, y) in enumerate(train_dataloader):\n",
    "      X = X.to(device)\n",
    "      y = y.to(device)\n",
    "      prediction = model(X).to(device)\n",
    "      optimizer.zero_grad()\n",
    "      loss = loss_func(prediction, y)\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "      train_loss += loss.float().item()\n",
    "      count += 1\n",
    "    scheduler.step()\n",
    "    training_losses = np.append(training_losses, train_loss / count)\n",
    "    print(\"Training loss:\", train_loss / count)\n",
    "    model.train(False)\n",
    "\n",
    "    ########################### Validation #####################################\n",
    "    # TODO: Design your own validation section\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    count = 0\n",
    "    with torch.no_grad():\n",
    "      for X, y in validation_dataloader:\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "        prediction = model(X).to(device)\n",
    "        loss = loss_func(prediction, y)\n",
    "        val_loss += loss.float().item()\n",
    "        count += 1\n",
    "    print(\"Validation loss:\", val_loss / count)\n",
    "    validation_losses = np.append(validation_losses, val_loss / count)\n",
    "\n",
    "torch.save(model, \"colorization_model_1.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "model = VanillaNeuralNet().to(device)\n",
    "learning_rate = 0.01\n",
    "train_batch_size = 300\n",
    "validation_batch_size = 300\n",
    "train_dataloader = DataLoader(training_dataset, batch_size=train_batch_size, shuffle=True)\n",
    "validation_dataloader = DataLoader(validation_dataset, batch_size=validation_batch_size, shuffle=False)\n",
    "## Initialize Optimizer and Learning Rate Scheduler\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "loss_func = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training...\n",
      "\n",
      "EPOCH 1 of 200\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tad\\AppData\\Local\\Temp\\ipykernel_13644\\4281265504.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  sample = {'feature': torch.tensor(self.x[index], dtype=torch.float32),\n",
      "C:\\Users\\Tad\\AppData\\Local\\Temp\\ipykernel_13644\\4281265504.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'label': torch.tensor(self.y[index], dtype=torch.float32)}\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'to'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[64], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m count \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m     12\u001b[0m \u001b[39mfor\u001b[39;00m batch, (X, y) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_dataloader):\n\u001b[1;32m---> 13\u001b[0m   X \u001b[39m=\u001b[39m X\u001b[39m.\u001b[39;49mto(device)\n\u001b[0;32m     14\u001b[0m   y \u001b[39m=\u001b[39m y\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     15\u001b[0m   prediction \u001b[39m=\u001b[39m model(X)\u001b[39m.\u001b[39mto(device)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'to'"
     ]
    }
   ],
   "source": [
    "print(\"Start Training...\")\n",
    "num_epochs = 200\n",
    "training_losses = np.array([])\n",
    "validation_losses = np.array([])\n",
    "for epoch in range(num_epochs):\n",
    "    ########################### Training #####################################\n",
    "    model.train(True)\n",
    "    print(\"\\nEPOCH \" +str(epoch+1)+\" of \"+str(num_epochs)+\"\\n\")\n",
    "    # TODO: Design your own training section\n",
    "    train_loss = 0\n",
    "    count = 0\n",
    "    for batch, (X, y) in enumerate(train_dataloader):\n",
    "      X = X.to(device)\n",
    "      y = y.to(device)\n",
    "      prediction = model(X).to(device)\n",
    "      optimizer.zero_grad()\n",
    "      loss = loss_func(prediction, y)\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "      train_loss += loss.float().item()\n",
    "      count += 1\n",
    "    scheduler.step()\n",
    "    training_losses = np.append(training_losses, train_loss / count)\n",
    "    print(\"Training loss:\", train_loss / count)\n",
    "    model.train(False)\n",
    "\n",
    "    ########################### Validation #####################################\n",
    "    # TODO: Design your own validation section\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    count = 0\n",
    "    with torch.no_grad():\n",
    "      for X, y in validation_dataloader:\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "        prediction = model(X).to(device)\n",
    "        loss = loss_func(prediction, y)\n",
    "        val_loss += loss.float().item()\n",
    "        count += 1\n",
    "    print(\"Validation loss:\", val_loss / count)\n",
    "    validation_losses = np.append(validation_losses, val_loss / count)\n",
    "\n",
    "torch.save(model, \"colorization_model_1.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
